Running
Module loaded
Command Line Args: Namespace(config_file='configs/danio2d/video_maskformer2_R50_bs16_8ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:51339', opts=['SOLVER.IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'output_3_train_1_fish_low_class_loss'])
[32m[06/02 13:44:41 detectron2]: [0mRank of current process: 0. World size: 1
[32m[06/02 13:44:43 detectron2]: [0mEnvironment info:
----------------------  ----------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.12 (main, Jun  1 2022, 11:51:15) [GCC 7.3.0]
numpy                   1.22.4
detectron2              0.6 @/home/asravan2/.local/lib/python3.9/site-packages/detectron2
Compiler                GCC 8.5
CUDA compiler           CUDA 11.2
detectron2 arch flags   3.7, 6.0, 7.0, 7.5, 8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.2 @/opt/miniconda3/envs/opence-v1.6.1/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   Tesla V100-SXM2-16GB (arch=7.0)
Driver version          550.54.15
CUDA_HOME               /usr/local/cuda-11.2
TORCH_CUDA_ARCH_LIST    3.7;6.0;7.0;7.5;8.0
Pillow                  8.4.0
torchvision             0.11.3 @/opt/miniconda3/envs/opence-v1.6.1/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.7, 6.0, 7.0, 7.5, 8.0
fvcore                  0.1.5.post20220512
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: VSX
  - CUDA Runtime 11.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80
  - CuDNN 8.1.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CUDA_VERSION=11.2, CUDNN_VERSION=8.1.1, CXX_COMPILER=/opt/conda/conda-bld/pytorch-base_1653429086405/_build_env/bin/powerpc64le-conda_cos7-linux-gnu-c++, CXX_FLAGS=-fvisibility-inlines-hidden -fmessage-length=0 -mcpu=power8 -mtune=power8 -mpower8-fusion -mpower8-vector -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O3 -pipe -I/opt/conda/conda-bld/pytorch-base_1653429086405/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/include -fdebug-prefix-map=/opt/conda/conda-bld/pytorch-base_1653429086405/work=/usr/local/src/conda/pytorch-base-1.10.2 -fdebug-prefix-map=/opt/conda/conda-bld/pytorch-base_1653429086405/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_=/usr/local/src/conda-prefix -D__STDC_FORMAT_MACROS -I/opt/conda/conda-bld/pytorch-base_1653429086405/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/include -I/usr/local/cuda/include -I/opt/conda/conda-bld/pytorch-base_1653429086405/_build_env/include -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=open, TORCH_VERSION=1.10.2, USE_CUDA=1, USE_CUDNN=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKLDNN=OFF, USE_MPI=0, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=1, 

[32m[06/02 13:44:43 detectron2]: [0mCommand line arguments: Namespace(config_file='configs/danio2d/video_maskformer2_R50_bs16_8ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:51339', opts=['SOLVER.IMS_PER_BATCH', '1', 'OUTPUT_DIR', 'output_3_train_1_fish_low_class_loss'])
[32m[06/02 13:44:43 detectron2]: [0mContents of args.config_file=configs/danio2d/video_maskformer2_R50_bs16_8ep.yaml:
[38;5;204m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-YouTubeVIS-VideoInstanceSegmentation.yaml[39m
[38;5;204mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmodel_final_3c8ec9.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;204mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mVideoMaskFormer[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;204mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;204mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;245m# pixel decoder[39m
[38;5;15m    [39m[38;5;204mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;204mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mVideoMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;204mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;204mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.2[39m
[38;5;15m    [39m[38;5;204mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mPOSE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;204mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;204mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;204mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;204mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;245m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;204mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;204mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;204mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;204mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;204mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;204mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;204mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;204mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[32m[06/02 13:44:43 detectron2]: [0mRunning with full config:
[38;5;204mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;204mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;204mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;204mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;204mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mdanio2d_val[39m
[38;5;15m  [39m[38;5;204mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mdanio2d_train[39m
[38;5;204mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;204mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAUGMENTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m720[39m
[38;5;15m    [39m[38;5;204mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute_range[39m
[38;5;15m  [39m[38;5;204mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_semantic[39m
[38;5;15m  [39m[38;5;204mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;204mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;204mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;204mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;204mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;204mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1333[39m
[38;5;15m  [39m[38;5;204mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice_by_clip[39m
[38;5;15m  [39m[38;5;204mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnone[39m
[38;5;15m  [39m[38;5;204mSAMPLING_FRAME_NUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m  [39m[38;5;204mSAMPLING_FRAME_RANGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;204mSAMPLING_FRAME_SHUFFLE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;204mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;204mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;204mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;204mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;204mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;204mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;204mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.2[39m
[38;5;15m    [39m[38;5;204mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;204mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;204mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;204mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;204mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;204mPOSE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;204mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;204mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;204mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;204mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;204mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mVideoMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;204mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mVideoMaskFormer[39m
[38;5;15m  [39m[38;5;204mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;204mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;204mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;204mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;204mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;204mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;204mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;204mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;204mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;204mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;204mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;204mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;204mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;204mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;204mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;204mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;204mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;204mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;204mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;204mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;204mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;204mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;204mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;204mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;204mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;204mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;204mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;204mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;204mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;204mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;204mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;204mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;204mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;204mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;204mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;204mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;204mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;204mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;204mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmodel_final_3c8ec9.pkl[39m
[38;5;204mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141moutput_3_train_1_fish_low_class_loss[39m
[38;5;204mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;204mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0005[39m
[38;5;15m  [39m[38;5;204mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;204mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;204mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;204mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;204mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;204mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;204mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;204mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;204mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;204mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m  [39m[38;5;204mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;204mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;204mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;204mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;204mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m  [39m[38;5;204mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;204mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;204mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[32m[06/02 13:44:43 detectron2]: [0mFull config saved to output_3_train_1_fish_low_class_loss/config.yaml
[32m[06/02 13:44:43 d2.utils.env]: [0mUsing a generated random seed 44942485
[32m[06/02 13:44:48 d2.engine.defaults]: [0mModel:
VideoMaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): VideoMultiScaleMaskedTransformerDecoder(
      (pe_layer): PositionEmbeddingSine3D()
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=2, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pose_embed): MLP_pose(
        (layers): ModuleList(
          (0): Linear(in_features=25600, out_features=6400, bias=True)
          (1): Linear(in_features=6400, out_features=1600, bias=True)
          (2): Linear(in_features=1600, out_features=400, bias=True)
          (3): Linear(in_features=400, out_features=100, bias=True)
          (4): Linear(in_features=100, out_features=24, bias=True)
        )
        (bn): ModuleList(
          (0): BatchNorm1d(6400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm1d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ModuleList(
          (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))
          (1): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2))
          (2): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2))
          (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
          (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))
        )
        (bn_conv): ModuleList(
          (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)
        (linear_layers): ModuleList(
          (0): Linear(in_features=36992, out_features=2312, bias=True)
          (1): Linear(in_features=2312, out_features=578, bias=True)
          (2): Linear(in_features=578, out_features=144, bias=True)
          (3): Linear(in_features=144, out_features=24, bias=True)
        )
      )
      (resize): Resize(size=[160, 160], interpolation=bilinear, max_size=None, antialias=None)
    )
  )
  (criterion): Criterion VideoSetCriterion
      matcher: Matcher VideoHungarianMatcher
          cost_class: 0.2
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 0.2, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_pose': 0.0, 'loss_ce_0': 0.2, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_pose_0': 0.0, 'loss_ce_1': 0.2, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_pose_1': 0.0, 'loss_ce_2': 0.2, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_pose_2': 0.0, 'loss_ce_3': 0.2, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_pose_3': 0.0, 'loss_ce_4': 0.2, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_pose_4': 0.0, 'loss_ce_5': 0.2, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_pose_5': 0.0, 'loss_ce_6': 0.2, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_pose_6': 0.0, 'loss_ce_7': 0.2, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_pose_7': 0.0, 'loss_ce_8': 0.2, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_pose_8': 0.0}
      num_classes: 1
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[32m[06/02 13:44:48 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice_by_clip', clip_frame_cnt=2)]
[32m[06/02 13:44:48 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/train.json
[32m[06/02 13:44:48 mask2former_video.data_video.build]: [0mUsing training sampler TrainingSampler
[32m[06/02 13:44:48 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:44:48 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[32m[06/02 13:44:51 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[06/02 13:45:02 d2.utils.events]: [0m eta: 1:08:35  iter: 19  total_loss: 103.6  loss_ce: 0.1275  loss_mask: 0.543  loss_dice: 9.848  loss_ce_0: 0.1488  loss_mask_0: 0.3139  loss_dice_0: 9.754  loss_ce_1: 0.1097  loss_mask_1: 0.4315  loss_dice_1: 9.783  loss_ce_2: 0.136  loss_mask_2: 0.6164  loss_dice_2: 9.779  loss_ce_3: 0.1225  loss_mask_3: 0.6076  loss_dice_3: 9.774  loss_ce_4: 0.1194  loss_mask_4: 0.4498  loss_dice_4: 9.709  loss_ce_5: 0.1077  loss_mask_5: 0.5319  loss_dice_5: 9.695  loss_ce_6: 0.1094  loss_mask_6: 0.6633  loss_dice_6: 9.756  loss_ce_7: 0.0999  loss_mask_7: 0.5367  loss_dice_7: 9.753  loss_ce_8: 0.08922  loss_mask_8: 0.539  loss_dice_8: 9.755  time: 0.3971  data_time: 0.0206  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:10 d2.utils.events]: [0m eta: 1:08:50  iter: 39  total_loss: 41.19  loss_ce: 0.06204  loss_mask: 0.1977  loss_dice: 4.014  loss_ce_0: 0.1436  loss_mask_0: 0.1331  loss_dice_0: 3.377  loss_ce_1: 0.06496  loss_mask_1: 0.1868  loss_dice_1: 3.71  loss_ce_2: 0.08185  loss_mask_2: 0.2458  loss_dice_2: 3.96  loss_ce_3: 0.07211  loss_mask_3: 0.2527  loss_dice_3: 3.248  loss_ce_4: 0.06727  loss_mask_4: 0.2184  loss_dice_4: 3.907  loss_ce_5: 0.06296  loss_mask_5: 0.2252  loss_dice_5: 3.491  loss_ce_6: 0.06173  loss_mask_6: 0.2049  loss_dice_6: 3.672  loss_ce_7: 0.06625  loss_mask_7: 0.2008  loss_dice_7: 3.896  loss_ce_8: 0.06272  loss_mask_8: 0.2033  loss_dice_8: 3.598  time: 0.4051  data_time: 0.0065  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:15 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:45:15 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:45:15 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:45:15 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:45:15 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:45:15 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:45:15 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259874 (0.259874 s / iter per device, on 1 devices)
[32m[06/02 13:45:15 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.196382 s / iter per device, on 1 devices)
[32m[06/02 13:45:15 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:45:15 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.362
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.362
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:45:15 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|:------:|:------:|
| 36.157 | 59.496 | 59.496 | 36.157 |  nan  |  nan  | 65.000 | 65.000 |
[32m[06/02 13:45:15 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:45:15 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:45:15 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:45:15 d2.evaluation.testing]: [0mcopypaste: 36.1566,59.4959,59.4959,36.1566,nan,nan,65.0000,65.0000
[32m[06/02 13:45:20 d2.utils.events]: [0m eta: 1:08:40  iter: 59  total_loss: 22.01  loss_ce: 0.06148  loss_mask: 0.06041  loss_dice: 2.3  loss_ce_0: 0.1404  loss_mask_0: 0.05314  loss_dice_0: 2.187  loss_ce_1: 0.05523  loss_mask_1: 0.06379  loss_dice_1: 2.046  loss_ce_2: 0.06207  loss_mask_2: 0.06839  loss_dice_2: 2.108  loss_ce_3: 0.06179  loss_mask_3: 0.06609  loss_dice_3: 2.049  loss_ce_4: 0.0614  loss_mask_4: 0.06731  loss_dice_4: 2.232  loss_ce_5: 0.06155  loss_mask_5: 0.06842  loss_dice_5: 1.985  loss_ce_6: 0.06135  loss_mask_6: 0.06015  loss_dice_6: 2.036  loss_ce_7: 0.06161  loss_mask_7: 0.05995  loss_dice_7: 1.976  loss_ce_8: 0.06131  loss_mask_8: 0.06815  loss_dice_8: 1.994  time: 0.4075  data_time: 0.0065  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:28 d2.utils.events]: [0m eta: 1:08:35  iter: 79  total_loss: 18.53  loss_ce: 0.06132  loss_mask: 0.06202  loss_dice: 1.727  loss_ce_0: 0.1354  loss_mask_0: 0.05322  loss_dice_0: 1.745  loss_ce_1: 0.05024  loss_mask_1: 0.05716  loss_dice_1: 1.518  loss_ce_2: 0.05981  loss_mask_2: 0.06327  loss_dice_2: 1.601  loss_ce_3: 0.06156  loss_mask_3: 0.06267  loss_dice_3: 1.853  loss_ce_4: 0.0613  loss_mask_4: 0.06392  loss_dice_4: 1.723  loss_ce_5: 0.06163  loss_mask_5: 0.06811  loss_dice_5: 1.907  loss_ce_6: 0.06136  loss_mask_6: 0.06332  loss_dice_6: 1.947  loss_ce_7: 0.0619  loss_mask_7: 0.05538  loss_dice_7: 1.55  loss_ce_8: 0.06133  loss_mask_8: 0.05934  loss_dice_8: 1.867  time: 0.4098  data_time: 0.0070  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:36 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:45:36 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:45:36 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:45:36 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:45:36 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:45:36 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:45:37 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282561 (0.282561 s / iter per device, on 1 devices)
[32m[06/02 13:45:37 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.222850 s / iter per device, on 1 devices)
[32m[06/02 13:45:37 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:45:37 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:45:37 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|:------:|:------:|
| 47.597 | 59.496 | 59.496 | 47.597 |  nan  |  nan  | 80.000 | 80.000 |
[32m[06/02 13:45:37 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:45:37 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:45:37 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:45:37 d2.evaluation.testing]: [0mcopypaste: 47.5968,59.4959,59.4959,47.5968,nan,nan,80.0000,80.0000
[32m[06/02 13:45:37 d2.utils.events]: [0m eta: 1:08:26  iter: 99  total_loss: 17.9  loss_ce: 0.0613  loss_mask: 0.06729  loss_dice: 1.737  loss_ce_0: 0.1346  loss_mask_0: 0.04709  loss_dice_0: 1.64  loss_ce_1: 0.0455  loss_mask_1: 0.06436  loss_dice_1: 1.63  loss_ce_2: 0.05825  loss_mask_2: 0.07763  loss_dice_2: 1.638  loss_ce_3: 0.06121  loss_mask_3: 0.06403  loss_dice_3: 1.684  loss_ce_4: 0.0613  loss_mask_4: 0.06676  loss_dice_4: 1.915  loss_ce_5: 0.06134  loss_mask_5: 0.06466  loss_dice_5: 1.782  loss_ce_6: 0.06131  loss_mask_6: 0.0657  loss_dice_6: 1.726  loss_ce_7: 0.06131  loss_mask_7: 0.05972  loss_dice_7: 1.694  loss_ce_8: 0.0613  loss_mask_8: 0.05919  loss_dice_8: 1.672  time: 0.4109  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:45 d2.utils.events]: [0m eta: 1:08:22  iter: 119  total_loss: 18.35  loss_ce: 0.06131  loss_mask: 0.05658  loss_dice: 1.756  loss_ce_0: 0.1315  loss_mask_0: 0.0427  loss_dice_0: 1.891  loss_ce_1: 0.0273  loss_mask_1: 0.05553  loss_dice_1: 1.862  loss_ce_2: 0.05454  loss_mask_2: 0.04818  loss_dice_2: 1.723  loss_ce_3: 0.0609  loss_mask_3: 0.05098  loss_dice_3: 1.579  loss_ce_4: 0.06132  loss_mask_4: 0.05747  loss_dice_4: 1.674  loss_ce_5: 0.06132  loss_mask_5: 0.052  loss_dice_5: 1.64  loss_ce_6: 0.06131  loss_mask_6: 0.05113  loss_dice_6: 1.533  loss_ce_7: 0.06131  loss_mask_7: 0.0565  loss_dice_7: 1.605  loss_ce_8: 0.06133  loss_mask_8: 0.06281  loss_dice_8: 2.052  time: 0.4119  data_time: 0.0063  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:54 d2.utils.events]: [0m eta: 1:08:12  iter: 139  total_loss: 17.22  loss_ce: 0.06131  loss_mask: 0.06612  loss_dice: 1.705  loss_ce_0: 0.1293  loss_mask_0: 0.04925  loss_dice_0: 1.568  loss_ce_1: 0.01174  loss_mask_1: 0.06619  loss_dice_1: 1.542  loss_ce_2: 0.04844  loss_mask_2: 0.06915  loss_dice_2: 1.546  loss_ce_3: 0.06035  loss_mask_3: 0.06866  loss_dice_3: 1.614  loss_ce_4: 0.06129  loss_mask_4: 0.06794  loss_dice_4: 1.687  loss_ce_5: 0.06131  loss_mask_5: 0.06262  loss_dice_5: 1.597  loss_ce_6: 0.06131  loss_mask_6: 0.06428  loss_dice_6: 1.484  loss_ce_7: 0.0613  loss_mask_7: 0.0698  loss_dice_7: 1.6  loss_ce_8: 0.0613  loss_mask_8: 0.0665  loss_dice_8: 1.542  time: 0.4129  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:45:58 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:45:58 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:45:58 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:45:58 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:45:58 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:45:58 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:45:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274067 (0.274067 s / iter per device, on 1 devices)
[32m[06/02 13:45:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213670 s / iter per device, on 1 devices)
[32m[06/02 13:45:59 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:45:59 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:45:59 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|:------:|:------:|
| 47.597 | 59.496 | 59.496 | 47.597 |  nan  |  nan  | 80.000 | 80.000 |
[32m[06/02 13:45:59 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:45:59 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:45:59 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:45:59 d2.evaluation.testing]: [0mcopypaste: 47.5968,59.4959,59.4959,47.5968,nan,nan,80.0000,80.0000
[32m[06/02 13:46:03 d2.utils.events]: [0m eta: 1:07:59  iter: 159  total_loss: 17.71  loss_ce: 0.06131  loss_mask: 0.05935  loss_dice: 1.627  loss_ce_0: 0.1265  loss_mask_0: 0.06046  loss_dice_0: 1.742  loss_ce_1: 0.003186  loss_mask_1: 0.06835  loss_dice_1: 1.59  loss_ce_2: 0.03754  loss_mask_2: 0.06427  loss_dice_2: 1.561  loss_ce_3: 0.05927  loss_mask_3: 0.06448  loss_dice_3: 1.612  loss_ce_4: 0.0612  loss_mask_4: 0.06375  loss_dice_4: 1.622  loss_ce_5: 0.06131  loss_mask_5: 0.06311  loss_dice_5: 1.6  loss_ce_6: 0.06131  loss_mask_6: 0.06998  loss_dice_6: 1.669  loss_ce_7: 0.0613  loss_mask_7: 0.06119  loss_dice_7: 1.625  loss_ce_8: 0.06131  loss_mask_8: 0.06269  loss_dice_8: 1.844  time: 0.4126  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:46:11 d2.utils.events]: [0m eta: 1:07:51  iter: 179  total_loss: 18.87  loss_ce: 0.06131  loss_mask: 0.06429  loss_dice: 1.642  loss_ce_0: 0.1251  loss_mask_0: 0.04153  loss_dice_0: 1.692  loss_ce_1: 0.0008724  loss_mask_1: 0.05944  loss_dice_1: 1.834  loss_ce_2: 0.02029  loss_mask_2: 0.06359  loss_dice_2: 1.827  loss_ce_3: 0.05673  loss_mask_3: 0.06032  loss_dice_3: 1.644  loss_ce_4: 0.06105  loss_mask_4: 0.06216  loss_dice_4: 1.661  loss_ce_5: 0.0613  loss_mask_5: 0.06625  loss_dice_5: 1.926  loss_ce_6: 0.06132  loss_mask_6: 0.06825  loss_dice_6: 1.691  loss_ce_7: 0.0613  loss_mask_7: 0.06534  loss_dice_7: 1.773  loss_ce_8: 0.06131  loss_mask_8: 0.05973  loss_dice_8: 1.456  time: 0.4129  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:46:20 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:46:20 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:46:20 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:46:20 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:46:20 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:46:20 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:46:20 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277440 (0.277440 s / iter per device, on 1 devices)
[32m[06/02 13:46:20 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.218551 s / iter per device, on 1 devices)
[32m[06/02 13:46:20 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:46:20 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.526
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:46:20 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|:------:|:------:|
| 52.646 | 59.496 | 59.496 | 52.646 |  nan  |  nan  | 85.000 | 85.000 |
[32m[06/02 13:46:20 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:46:20 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:46:20 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:46:20 d2.evaluation.testing]: [0mcopypaste: 52.6463,59.4959,59.4959,52.6463,nan,nan,85.0000,85.0000
[32m[06/02 13:46:20 d2.utils.events]: [0m eta: 1:07:43  iter: 199  total_loss: 18.23  loss_ce: 0.0613  loss_mask: 0.05888  loss_dice: 1.73  loss_ce_0: 0.1195  loss_mask_0: 0.04953  loss_dice_0: 1.632  loss_ce_1: 0.0002845  loss_mask_1: 0.06157  loss_dice_1: 1.726  loss_ce_2: 0.005385  loss_mask_2: 0.06792  loss_dice_2: 1.816  loss_ce_3: 0.05093  loss_mask_3: 0.06441  loss_dice_3: 1.817  loss_ce_4: 0.06066  loss_mask_4: 0.07045  loss_dice_4: 1.782  loss_ce_5: 0.06125  loss_mask_5: 0.06176  loss_dice_5: 1.793  loss_ce_6: 0.06131  loss_mask_6: 0.05958  loss_dice_6: 1.537  loss_ce_7: 0.0613  loss_mask_7: 0.05529  loss_dice_7: 1.495  loss_ce_8: 0.06131  loss_mask_8: 0.06538  loss_dice_8: 1.67  time: 0.4132  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:46:29 d2.utils.events]: [0m eta: 1:07:34  iter: 219  total_loss: 16.2  loss_ce: 0.06131  loss_mask: 0.05956  loss_dice: 1.484  loss_ce_0: 0.1154  loss_mask_0: 0.06071  loss_dice_0: 1.475  loss_ce_1: 0.0002128  loss_mask_1: 0.05437  loss_dice_1: 1.48  loss_ce_2: 0.001302  loss_mask_2: 0.0537  loss_dice_2: 1.531  loss_ce_3: 0.04057  loss_mask_3: 0.05714  loss_dice_3: 1.495  loss_ce_4: 0.05995  loss_mask_4: 0.06552  loss_dice_4: 1.579  loss_ce_5: 0.06121  loss_mask_5: 0.05956  loss_dice_5: 1.591  loss_ce_6: 0.06131  loss_mask_6: 0.0566  loss_dice_6: 1.378  loss_ce_7: 0.06131  loss_mask_7: 0.05877  loss_dice_7: 1.585  loss_ce_8: 0.0613  loss_mask_8: 0.06031  loss_dice_8: 1.62  time: 0.4134  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:46:37 d2.utils.events]: [0m eta: 1:07:26  iter: 239  total_loss: 15.45  loss_ce: 0.06131  loss_mask: 0.05751  loss_dice: 1.451  loss_ce_0: 0.1108  loss_mask_0: 0.05875  loss_dice_0: 1.429  loss_ce_1: 0.0001522  loss_mask_1: 0.05603  loss_dice_1: 1.423  loss_ce_2: 0.000194  loss_mask_2: 0.05391  loss_dice_2: 1.377  loss_ce_3: 0.01845  loss_mask_3: 0.05035  loss_dice_3: 1.452  loss_ce_4: 0.05751  loss_mask_4: 0.05707  loss_dice_4: 1.501  loss_ce_5: 0.06098  loss_mask_5: 0.06065  loss_dice_5: 1.416  loss_ce_6: 0.0613  loss_mask_6: 0.0593  loss_dice_6: 1.519  loss_ce_7: 0.06131  loss_mask_7: 0.06222  loss_dice_7: 1.507  loss_ce_8: 0.06131  loss_mask_8: 0.05483  loss_dice_8: 1.279  time: 0.4136  data_time: 0.0086  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:46:41 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:46:41 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:46:41 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:46:41 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:46:41 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:46:41 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:46:42 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.276553 (0.276553 s / iter per device, on 1 devices)
[32m[06/02 13:46:42 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215349 s / iter per device, on 1 devices)
[32m[06/02 13:46:42 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:46:42 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.476
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:46:42 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|:------:|:------:|
| 47.597 | 59.496 | 59.496 | 47.597 |  nan  |  nan  | 80.000 | 80.000 |
[32m[06/02 13:46:42 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:46:42 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:46:42 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:46:42 d2.evaluation.testing]: [0mcopypaste: 47.5968,59.4959,59.4959,47.5968,nan,nan,80.0000,80.0000
[32m[06/02 13:46:46 d2.utils.events]: [0m eta: 1:07:18  iter: 259  total_loss: 15.02  loss_ce: 0.06132  loss_mask: 0.05704  loss_dice: 1.558  loss_ce_0: 0.1061  loss_mask_0: 0.05423  loss_dice_0: 1.494  loss_ce_1: 0.0001466  loss_mask_1: 0.05856  loss_dice_1: 1.307  loss_ce_2: 6.533e-05  loss_mask_2: 0.05593  loss_dice_2: 1.615  loss_ce_3: 0.001828  loss_mask_3: 0.05196  loss_dice_3: 1.422  loss_ce_4: 0.04965  loss_mask_4: 0.05929  loss_dice_4: 1.472  loss_ce_5: 0.0602  loss_mask_5: 0.05317  loss_dice_5: 1.339  loss_ce_6: 0.06122  loss_mask_6: 0.0633  loss_dice_6: 1.417  loss_ce_7: 0.06131  loss_mask_7: 0.0553  loss_dice_7: 1.335  loss_ce_8: 0.06131  loss_mask_8: 0.06642  loss_dice_8: 1.433  time: 0.4137  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:46:54 d2.utils.events]: [0m eta: 1:07:09  iter: 279  total_loss: 14.54  loss_ce: 0.06131  loss_mask: 0.05708  loss_dice: 1.293  loss_ce_0: 0.1008  loss_mask_0: 0.05645  loss_dice_0: 1.437  loss_ce_1: 8.635e-05  loss_mask_1: 0.04954  loss_dice_1: 1.329  loss_ce_2: 8.576e-05  loss_mask_2: 0.05907  loss_dice_2: 1.452  loss_ce_3: 0.0003331  loss_mask_3: 0.04907  loss_dice_3: 1.467  loss_ce_4: 0.0368  loss_mask_4: 0.05703  loss_dice_4: 1.334  loss_ce_5: 0.05869  loss_mask_5: 0.05113  loss_dice_5: 1.243  loss_ce_6: 0.0611  loss_mask_6: 0.0564  loss_dice_6: 1.335  loss_ce_7: 0.06131  loss_mask_7: 0.05036  loss_dice_7: 1.383  loss_ce_8: 0.06132  loss_mask_8: 0.0579  loss_dice_8: 1.26  time: 0.4131  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:03 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:47:03 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:47:03 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:47:03 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:47:03 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:47:03 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:47:03 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.262657 (0.262657 s / iter per device, on 1 devices)
[32m[06/02 13:47:03 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.203255 s / iter per device, on 1 devices)
[32m[06/02 13:47:03 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:47:03 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.535
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.595
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.535
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:47:03 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:------:|:------:|:------:|:-----:|:-----:|:------:|:------:|
| 53.546 | 59.496 | 59.496 | 53.546 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:47:03 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:47:03 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:47:03 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:47:03 d2.evaluation.testing]: [0mcopypaste: 53.5464,59.4959,59.4959,53.5464,nan,nan,90.0000,90.0000
[32m[06/02 13:47:03 d2.utils.events]: [0m eta: 1:07:00  iter: 299  total_loss: 14.62  loss_ce: 0.06131  loss_mask: 0.05075  loss_dice: 1.332  loss_ce_0: 0.0954  loss_mask_0: 0.05166  loss_dice_0: 1.313  loss_ce_1: 6.223e-05  loss_mask_1: 0.04856  loss_dice_1: 1.387  loss_ce_2: 3.905e-05  loss_mask_2: 0.06103  loss_dice_2: 1.324  loss_ce_3: 5.111e-05  loss_mask_3: 0.05019  loss_dice_3: 1.407  loss_ce_4: 0.005967  loss_mask_4: 0.05244  loss_dice_4: 1.414  loss_ce_5: 0.05064  loss_mask_5: 0.0453  loss_dice_5: 1.268  loss_ce_6: 0.06041  loss_mask_6: 0.04781  loss_dice_6: 1.365  loss_ce_7: 0.06126  loss_mask_7: 0.04485  loss_dice_7: 1.274  loss_ce_8: 0.06131  loss_mask_8: 0.04607  loss_dice_8: 1.347  time: 0.4131  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:12 d2.utils.events]: [0m eta: 1:06:49  iter: 319  total_loss: 12.78  loss_ce: 0.06132  loss_mask: 0.05137  loss_dice: 1.336  loss_ce_0: 0.08929  loss_mask_0: 0.05405  loss_dice_0: 1.175  loss_ce_1: 4.215e-05  loss_mask_1: 0.05199  loss_dice_1: 1.256  loss_ce_2: 3.164e-05  loss_mask_2: 0.05905  loss_dice_2: 1.274  loss_ce_3: 7.497e-05  loss_mask_3: 0.05325  loss_dice_3: 1.154  loss_ce_4: 4.495e-05  loss_mask_4: 0.05556  loss_dice_4: 1.135  loss_ce_5: 0.01194  loss_mask_5: 0.04825  loss_dice_5: 1.159  loss_ce_6: 0.0546  loss_mask_6: 0.05003  loss_dice_6: 1.204  loss_ce_7: 0.06084  loss_mask_7: 0.05213  loss_dice_7: 1.207  loss_ce_8: 0.06128  loss_mask_8: 0.05277  loss_dice_8: 1.189  time: 0.4127  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:20 d2.utils.events]: [0m eta: 1:06:41  iter: 339  total_loss: 13.45  loss_ce: 0.06131  loss_mask: 0.04749  loss_dice: 1.312  loss_ce_0: 0.08341  loss_mask_0: 0.04971  loss_dice_0: 1.093  loss_ce_1: 3.225e-05  loss_mask_1: 0.04718  loss_dice_1: 1.168  loss_ce_2: 1.304e-05  loss_mask_2: 0.04747  loss_dice_2: 1.178  loss_ce_3: 2.271e-05  loss_mask_3: 0.04863  loss_dice_3: 1.18  loss_ce_4: 4.614e-05  loss_mask_4: 0.05676  loss_dice_4: 1.339  loss_ce_5: 0.000147  loss_mask_5: 0.04848  loss_dice_5: 1.205  loss_ce_6: 0.0215  loss_mask_6: 0.05311  loss_dice_6: 1.407  loss_ce_7: 0.05744  loss_mask_7: 0.05458  loss_dice_7: 1.304  loss_ce_8: 0.06102  loss_mask_8: 0.0514  loss_dice_8: 1.192  time: 0.4127  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:24 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:47:24 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:47:24 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:47:24 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:47:24 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:47:24 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:47:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273735 (0.273735 s / iter per device, on 1 devices)
[32m[06/02 13:47:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.214561 s / iter per device, on 1 devices)
[32m[06/02 13:47:25 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:47:25 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:47:25 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:47:25 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:47:25 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:47:25 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:47:25 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:47:29 d2.utils.events]: [0m eta: 1:06:34  iter: 359  total_loss: 15.11  loss_ce: 0.06118  loss_mask: 0.05831  loss_dice: 1.495  loss_ce_0: 0.07862  loss_mask_0: 0.05635  loss_dice_0: 1.473  loss_ce_1: 2.497e-05  loss_mask_1: 0.0587  loss_dice_1: 1.445  loss_ce_2: 1.089e-05  loss_mask_2: 0.05353  loss_dice_2: 1.348  loss_ce_3: 6.588e-06  loss_mask_3: 0.06413  loss_dice_3: 1.375  loss_ce_4: 4.774e-05  loss_mask_4: 0.06221  loss_dice_4: 1.304  loss_ce_5: 0.0002425  loss_mask_5: 0.05724  loss_dice_5: 1.533  loss_ce_6: 0.0007878  loss_mask_6: 0.05493  loss_dice_6: 1.435  loss_ce_7: 0.04222  loss_mask_7: 0.06155  loss_dice_7: 1.427  loss_ce_8: 0.05961  loss_mask_8: 0.05672  loss_dice_8: 1.412  time: 0.4129  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:37 d2.utils.events]: [0m eta: 1:06:25  iter: 379  total_loss: 14.08  loss_ce: 0.06081  loss_mask: 0.05136  loss_dice: 1.277  loss_ce_0: 0.07354  loss_mask_0: 0.04724  loss_dice_0: 1.306  loss_ce_1: 1.952e-05  loss_mask_1: 0.05125  loss_dice_1: 1.286  loss_ce_2: 6.538e-06  loss_mask_2: 0.05477  loss_dice_2: 1.28  loss_ce_3: 2.239e-06  loss_mask_3: 0.0528  loss_dice_3: 1.346  loss_ce_4: 3.275e-06  loss_mask_4: 0.05422  loss_dice_4: 1.374  loss_ce_5: 1.13e-05  loss_mask_5: 0.05626  loss_dice_5: 1.266  loss_ce_6: 6.062e-05  loss_mask_6: 0.05205  loss_dice_6: 1.314  loss_ce_7: 0.01355  loss_mask_7: 0.04972  loss_dice_7: 1.183  loss_ce_8: 0.05504  loss_mask_8: 0.04927  loss_dice_8: 1.293  time: 0.4128  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:46 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:47:46 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:47:46 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:47:46 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:47:46 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:47:46 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:47:46 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273426 (0.273426 s / iter per device, on 1 devices)
[32m[06/02 13:47:46 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.214804 s / iter per device, on 1 devices)
[32m[06/02 13:47:46 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:47:46 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:47:46 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:47:46 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:47:46 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:47:46 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:47:46 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:47:46 d2.utils.events]: [0m eta: 1:06:16  iter: 399  total_loss: 14.46  loss_ce: 0.05747  loss_mask: 0.05892  loss_dice: 1.284  loss_ce_0: 0.06875  loss_mask_0: 0.05473  loss_dice_0: 1.31  loss_ce_1: 1.586e-05  loss_mask_1: 0.05514  loss_dice_1: 1.184  loss_ce_2: 5.214e-06  loss_mask_2: 0.05572  loss_dice_2: 1.46  loss_ce_3: 1.953e-06  loss_mask_3: 0.05856  loss_dice_3: 1.35  loss_ce_4: 1.736e-06  loss_mask_4: 0.05671  loss_dice_4: 1.299  loss_ce_5: 1.837e-06  loss_mask_5: 0.05908  loss_dice_5: 1.353  loss_ce_6: 9.113e-06  loss_mask_6: 0.05164  loss_dice_6: 1.462  loss_ce_7: 0.0001607  loss_mask_7: 0.05881  loss_dice_7: 1.481  loss_ce_8: 0.02641  loss_mask_8: 0.05711  loss_dice_8: 1.42  time: 0.4128  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:47:55 d2.utils.events]: [0m eta: 1:06:08  iter: 419  total_loss: 13.57  loss_ce: 0.02548  loss_mask: 0.05453  loss_dice: 1.313  loss_ce_0: 0.06425  loss_mask_0: 0.0507  loss_dice_0: 1.352  loss_ce_1: 1.471e-05  loss_mask_1: 0.04739  loss_dice_1: 1.248  loss_ce_2: 3.74e-06  loss_mask_2: 0.04149  loss_dice_2: 1.34  loss_ce_3: 1.529e-06  loss_mask_3: 0.04187  loss_dice_3: 1.255  loss_ce_4: 2.512e-06  loss_mask_4: 0.05074  loss_dice_4: 1.239  loss_ce_5: 1.356e-06  loss_mask_5: 0.04651  loss_dice_5: 1.301  loss_ce_6: 3.783e-06  loss_mask_6: 0.04832  loss_dice_6: 1.295  loss_ce_7: 3.547e-05  loss_mask_7: 0.05392  loss_dice_7: 1.323  loss_ce_8: 0.0001136  loss_mask_8: 0.05099  loss_dice_8: 1.343  time: 0.4129  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:03 d2.utils.events]: [0m eta: 1:05:58  iter: 439  total_loss: 12.9  loss_ce: 0.001654  loss_mask: 0.04511  loss_dice: 1.258  loss_ce_0: 0.06007  loss_mask_0: 0.0426  loss_dice_0: 1.19  loss_ce_1: 1.277e-05  loss_mask_1: 0.04974  loss_dice_1: 1.265  loss_ce_2: 6.306e-06  loss_mask_2: 0.04944  loss_dice_2: 1.176  loss_ce_3: 5.085e-06  loss_mask_3: 0.05574  loss_dice_3: 1.215  loss_ce_4: 2.61e-05  loss_mask_4: 0.05833  loss_dice_4: 1.233  loss_ce_5: 7.854e-05  loss_mask_5: 0.05236  loss_dice_5: 1.234  loss_ce_6: 1.925e-05  loss_mask_6: 0.0553  loss_dice_6: 1.229  loss_ce_7: 4.368e-05  loss_mask_7: 0.04561  loss_dice_7: 1.27  loss_ce_8: 7.212e-05  loss_mask_8: 0.0449  loss_dice_8: 1.141  time: 0.4126  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:07 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:48:07 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:48:07 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:48:07 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:48:07 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:48:07 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:48:08 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.302022 (0.302022 s / iter per device, on 1 devices)
[32m[06/02 13:48:08 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.238064 s / iter per device, on 1 devices)
[32m[06/02 13:48:08 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:48:08 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:48:08 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:48:08 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:48:08 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:48:08 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:48:08 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:48:12 d2.utils.events]: [0m eta: 1:05:49  iter: 459  total_loss: 12.48  loss_ce: 0.003455  loss_mask: 0.05201  loss_dice: 1.313  loss_ce_0: 0.05548  loss_mask_0: 0.048  loss_dice_0: 1.211  loss_ce_1: 1.165e-05  loss_mask_1: 0.05235  loss_dice_1: 1.22  loss_ce_2: 4.194e-06  loss_mask_2: 0.04996  loss_dice_2: 1.168  loss_ce_3: 2.898e-06  loss_mask_3: 0.04825  loss_dice_3: 1.197  loss_ce_4: 1.062e-05  loss_mask_4: 0.05064  loss_dice_4: 1.275  loss_ce_5: 1.478e-05  loss_mask_5: 0.05153  loss_dice_5: 1.226  loss_ce_6: 1.684e-05  loss_mask_6: 0.05014  loss_dice_6: 1.18  loss_ce_7: 1.895e-05  loss_mask_7: 0.05898  loss_dice_7: 1.149  loss_ce_8: 7.472e-05  loss_mask_8: 0.04804  loss_dice_8: 1.205  time: 0.4125  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:20 d2.utils.events]: [0m eta: 1:05:42  iter: 479  total_loss: 12.38  loss_ce: 0.002657  loss_mask: 0.0487  loss_dice: 1.203  loss_ce_0: 0.05172  loss_mask_0: 0.04491  loss_dice_0: 1.078  loss_ce_1: 1.005e-05  loss_mask_1: 0.04425  loss_dice_1: 1.1  loss_ce_2: 2.993e-06  loss_mask_2: 0.04533  loss_dice_2: 1.164  loss_ce_3: 2.138e-06  loss_mask_3: 0.04667  loss_dice_3: 1.168  loss_ce_4: 1.167e-05  loss_mask_4: 0.04024  loss_dice_4: 1.133  loss_ce_5: 7.207e-06  loss_mask_5: 0.04897  loss_dice_5: 1.267  loss_ce_6: 3.837e-06  loss_mask_6: 0.05177  loss_dice_6: 1.176  loss_ce_7: 1.195e-05  loss_mask_7: 0.04771  loss_dice_7: 1.181  loss_ce_8: 9.734e-05  loss_mask_8: 0.04576  loss_dice_8: 1.195  time: 0.4127  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:29 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:48:29 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:48:29 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:48:29 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:48:29 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:48:29 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:48:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282621 (0.282621 s / iter per device, on 1 devices)
[32m[06/02 13:48:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217154 s / iter per device, on 1 devices)
[32m[06/02 13:48:29 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:48:29 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:48:30 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:48:30 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:48:30 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:48:30 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:48:30 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:48:30 d2.utils.events]: [0m eta: 1:05:35  iter: 499  total_loss: 13.77  loss_ce: 8.106e-05  loss_mask: 0.05263  loss_dice: 1.124  loss_ce_0: 0.04812  loss_mask_0: 0.05336  loss_dice_0: 1.338  loss_ce_1: 9.482e-06  loss_mask_1: 0.04622  loss_dice_1: 1.422  loss_ce_2: 2.975e-06  loss_mask_2: 0.05348  loss_dice_2: 1.211  loss_ce_3: 1.762e-06  loss_mask_3: 0.05342  loss_dice_3: 1.375  loss_ce_4: 2.088e-05  loss_mask_4: 0.05455  loss_dice_4: 1.348  loss_ce_5: 5.326e-06  loss_mask_5: 0.05563  loss_dice_5: 1.373  loss_ce_6: 2.607e-06  loss_mask_6: 0.05853  loss_dice_6: 1.358  loss_ce_7: 3.61e-06  loss_mask_7: 0.04997  loss_dice_7: 1.119  loss_ce_8: 3.939e-06  loss_mask_8: 0.05374  loss_dice_8: 1.221  time: 0.4129  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:38 d2.utils.events]: [0m eta: 1:05:27  iter: 519  total_loss: 13.98  loss_ce: 2.001e-05  loss_mask: 0.04445  loss_dice: 1.277  loss_ce_0: 0.04484  loss_mask_0: 0.05372  loss_dice_0: 1.28  loss_ce_1: 1.054e-05  loss_mask_1: 0.05093  loss_dice_1: 1.349  loss_ce_2: 2.048e-06  loss_mask_2: 0.0581  loss_dice_2: 1.36  loss_ce_3: 9.385e-07  loss_mask_3: 0.05222  loss_dice_3: 1.367  loss_ce_4: 1.666e-06  loss_mask_4: 0.05141  loss_dice_4: 1.373  loss_ce_5: 3.355e-07  loss_mask_5: 0.04651  loss_dice_5: 1.317  loss_ce_6: 3.125e-07  loss_mask_6: 0.05693  loss_dice_6: 1.295  loss_ce_7: 3.585e-07  loss_mask_7: 0.04671  loss_dice_7: 1.313  loss_ce_8: 4.194e-07  loss_mask_8: 0.05112  loss_dice_8: 1.177  time: 0.4130  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:46 d2.utils.events]: [0m eta: 1:05:21  iter: 539  total_loss: 12.48  loss_ce: 0.001139  loss_mask: 0.0512  loss_dice: 1.153  loss_ce_0: 0.04131  loss_mask_0: 0.04429  loss_dice_0: 1.209  loss_ce_1: 1.474e-05  loss_mask_1: 0.04528  loss_dice_1: 1.111  loss_ce_2: 2e-06  loss_mask_2: 0.04787  loss_dice_2: 1.223  loss_ce_3: 8.603e-07  loss_mask_3: 0.04168  loss_dice_3: 1.129  loss_ce_4: 2.776e-06  loss_mask_4: 0.0479  loss_dice_4: 1.223  loss_ce_5: 5.347e-07  loss_mask_5: 0.04954  loss_dice_5: 1.129  loss_ce_6: 5.792e-07  loss_mask_6: 0.0438  loss_dice_6: 1.154  loss_ce_7: 7.564e-06  loss_mask_7: 0.05065  loss_dice_7: 1.227  loss_ce_8: 3.717e-05  loss_mask_8: 0.04589  loss_dice_8: 1.102  time: 0.4131  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:48:51 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:48:51 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:48:51 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:48:51 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:48:51 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:48:51 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:48:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271521 (0.271521 s / iter per device, on 1 devices)
[32m[06/02 13:48:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.207718 s / iter per device, on 1 devices)
[32m[06/02 13:48:51 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:48:51 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:48:51 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:48:51 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:48:51 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:48:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:48:51 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:48:55 d2.utils.events]: [0m eta: 1:05:13  iter: 559  total_loss: 10.8  loss_ce: 0.0003326  loss_mask: 0.0436  loss_dice: 1.006  loss_ce_0: 0.03741  loss_mask_0: 0.04314  loss_dice_0: 1.023  loss_ce_1: 1.265e-05  loss_mask_1: 0.04195  loss_dice_1: 1.005  loss_ce_2: 1.703e-06  loss_mask_2: 0.04404  loss_dice_2: 1.015  loss_ce_3: 6.552e-07  loss_mask_3: 0.04311  loss_dice_3: 1.004  loss_ce_4: 3.693e-06  loss_mask_4: 0.05101  loss_dice_4: 1.143  loss_ce_5: 1.048e-06  loss_mask_5: 0.04533  loss_dice_5: 1.084  loss_ce_6: 3.376e-06  loss_mask_6: 0.04757  loss_dice_6: 1.038  loss_ce_7: 2.774e-05  loss_mask_7: 0.05085  loss_dice_7: 1.018  loss_ce_8: 4.386e-05  loss_mask_8: 0.0463  loss_dice_8: 1.047  time: 0.4133  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:04 d2.utils.events]: [0m eta: 1:05:05  iter: 579  total_loss: 10.97  loss_ce: 5.024e-05  loss_mask: 0.04389  loss_dice: 1.103  loss_ce_0: 0.03424  loss_mask_0: 0.04354  loss_dice_0: 1.098  loss_ce_1: 1.296e-05  loss_mask_1: 0.04393  loss_dice_1: 1.009  loss_ce_2: 1.799e-06  loss_mask_2: 0.04122  loss_dice_2: 1.026  loss_ce_3: 5.691e-07  loss_mask_3: 0.04042  loss_dice_3: 0.9835  loss_ce_4: 2.292e-06  loss_mask_4: 0.04299  loss_dice_4: 0.945  loss_ce_5: 4.46e-07  loss_mask_5: 0.04613  loss_dice_5: 1.158  loss_ce_6: 5.299e-07  loss_mask_6: 0.03939  loss_dice_6: 1.02  loss_ce_7: 4.506e-06  loss_mask_7: 0.04942  loss_dice_7: 1.082  loss_ce_8: 3.354e-06  loss_mask_8: 0.0462  loss_dice_8: 1.042  time: 0.4133  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:12 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:49:12 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:49:12 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:49:12 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:49:12 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:49:12 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:49:13 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280834 (0.280834 s / iter per device, on 1 devices)
[32m[06/02 13:49:13 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215854 s / iter per device, on 1 devices)
[32m[06/02 13:49:13 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:49:13 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:49:13 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:49:13 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:49:13 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:49:13 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:49:13 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:49:13 d2.utils.events]: [0m eta: 1:04:57  iter: 599  total_loss: 11.15  loss_ce: 9.633e-05  loss_mask: 0.04158  loss_dice: 0.9923  loss_ce_0: 0.03131  loss_mask_0: 0.04459  loss_dice_0: 1.123  loss_ce_1: 1.028e-05  loss_mask_1: 0.04427  loss_dice_1: 1.04  loss_ce_2: 1.308e-06  loss_mask_2: 0.04608  loss_dice_2: 0.9829  loss_ce_3: 6.151e-07  loss_mask_3: 0.04919  loss_dice_3: 1.063  loss_ce_4: 3.425e-06  loss_mask_4: 0.04758  loss_dice_4: 1.09  loss_ce_5: 5.169e-07  loss_mask_5: 0.044  loss_dice_5: 1.057  loss_ce_6: 6.441e-07  loss_mask_6: 0.04561  loss_dice_6: 1.009  loss_ce_7: 4.221e-06  loss_mask_7: 0.04969  loss_dice_7: 1.106  loss_ce_8: 5.859e-06  loss_mask_8: 0.04648  loss_dice_8: 1.072  time: 0.4134  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:21 d2.utils.events]: [0m eta: 1:04:48  iter: 619  total_loss: 11.66  loss_ce: 0.0001518  loss_mask: 0.04952  loss_dice: 1.18  loss_ce_0: 0.02845  loss_mask_0: 0.04973  loss_dice_0: 1.148  loss_ce_1: 8.275e-06  loss_mask_1: 0.04841  loss_dice_1: 1.166  loss_ce_2: 1.201e-06  loss_mask_2: 0.05827  loss_dice_2: 1.164  loss_ce_3: 5.381e-07  loss_mask_3: 0.04876  loss_dice_3: 1.112  loss_ce_4: 1.452e-06  loss_mask_4: 0.04141  loss_dice_4: 1.078  loss_ce_5: 2.133e-07  loss_mask_5: 0.04902  loss_dice_5: 1.153  loss_ce_6: 1.982e-07  loss_mask_6: 0.05201  loss_dice_6: 1.046  loss_ce_7: 1.521e-06  loss_mask_7: 0.05111  loss_dice_7: 1.156  loss_ce_8: 5.821e-06  loss_mask_8: 0.05564  loss_dice_8: 1.141  time: 0.4133  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:30 d2.utils.events]: [0m eta: 1:04:40  iter: 639  total_loss: 11.32  loss_ce: 0.0003449  loss_mask: 0.04654  loss_dice: 1.102  loss_ce_0: 0.02594  loss_mask_0: 0.04255  loss_dice_0: 1.082  loss_ce_1: 8.19e-06  loss_mask_1: 0.04133  loss_dice_1: 1.122  loss_ce_2: 1.087e-06  loss_mask_2: 0.04637  loss_dice_2: 1.045  loss_ce_3: 4.167e-07  loss_mask_3: 0.04262  loss_dice_3: 1.032  loss_ce_4: 2.035e-06  loss_mask_4: 0.0401  loss_dice_4: 1.018  loss_ce_5: 2.088e-07  loss_mask_5: 0.03826  loss_dice_5: 1.005  loss_ce_6: 1.767e-07  loss_mask_6: 0.0426  loss_dice_6: 1.027  loss_ce_7: 2.298e-06  loss_mask_7: 0.04368  loss_dice_7: 1.07  loss_ce_8: 2.18e-05  loss_mask_8: 0.0433  loss_dice_8: 1.1  time: 0.4134  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:34 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:49:34 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:49:34 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:49:34 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:49:34 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:49:34 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:49:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277005 (0.277005 s / iter per device, on 1 devices)
[32m[06/02 13:49:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212976 s / iter per device, on 1 devices)
[32m[06/02 13:49:35 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:49:35 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:49:35 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:49:35 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:49:35 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:49:35 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:49:35 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:49:39 d2.utils.events]: [0m eta: 1:04:32  iter: 659  total_loss: 10.13  loss_ce: 0.0002149  loss_mask: 0.04442  loss_dice: 1.037  loss_ce_0: 0.02371  loss_mask_0: 0.04318  loss_dice_0: 1.005  loss_ce_1: 9.483e-06  loss_mask_1: 0.03878  loss_dice_1: 0.9272  loss_ce_2: 8.614e-07  loss_mask_2: 0.04291  loss_dice_2: 1.045  loss_ce_3: 4.459e-07  loss_mask_3: 0.04245  loss_dice_3: 0.9208  loss_ce_4: 3.154e-06  loss_mask_4: 0.03752  loss_dice_4: 0.926  loss_ce_5: 3.627e-07  loss_mask_5: 0.04008  loss_dice_5: 0.9524  loss_ce_6: 3.814e-07  loss_mask_6: 0.04219  loss_dice_6: 0.8996  loss_ce_7: 5.28e-06  loss_mask_7: 0.04302  loss_dice_7: 0.9095  loss_ce_8: 1.884e-05  loss_mask_8: 0.04278  loss_dice_8: 0.9202  time: 0.4135  data_time: 0.0068  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:47 d2.utils.events]: [0m eta: 1:04:24  iter: 679  total_loss: 9.961  loss_ce: 7.55e-05  loss_mask: 0.039  loss_dice: 0.9079  loss_ce_0: 0.02134  loss_mask_0: 0.04299  loss_dice_0: 0.9226  loss_ce_1: 8.898e-06  loss_mask_1: 0.04584  loss_dice_1: 0.9822  loss_ce_2: 8.078e-07  loss_mask_2: 0.04342  loss_dice_2: 0.924  loss_ce_3: 4.152e-07  loss_mask_3: 0.04181  loss_dice_3: 1.034  loss_ce_4: 1.449e-06  loss_mask_4: 0.04325  loss_dice_4: 0.9761  loss_ce_5: 2.334e-07  loss_mask_5: 0.04138  loss_dice_5: 0.9541  loss_ce_6: 2.15e-07  loss_mask_6: 0.04976  loss_dice_6: 1.074  loss_ce_7: 1.385e-06  loss_mask_7: 0.04624  loss_dice_7: 0.9143  loss_ce_8: 2.651e-06  loss_mask_8: 0.04124  loss_dice_8: 0.9142  time: 0.4136  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:49:55 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:49:56 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:49:56 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:49:56 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:49:56 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:49:56 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:49:56 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.285105 (0.285105 s / iter per device, on 1 devices)
[32m[06/02 13:49:56 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.220712 s / iter per device, on 1 devices)
[32m[06/02 13:49:56 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:49:56 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:49:56 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:49:56 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:49:56 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:49:56 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:49:56 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:49:56 d2.utils.events]: [0m eta: 1:04:16  iter: 699  total_loss: 10.12  loss_ce: 1.43e-05  loss_mask: 0.05138  loss_dice: 1.006  loss_ce_0: 0.01943  loss_mask_0: 0.04879  loss_dice_0: 0.9912  loss_ce_1: 8.481e-06  loss_mask_1: 0.0421  loss_dice_1: 0.9938  loss_ce_2: 6.665e-07  loss_mask_2: 0.04599  loss_dice_2: 0.9759  loss_ce_3: 3.067e-07  loss_mask_3: 0.04301  loss_dice_3: 1.021  loss_ce_4: 3.169e-07  loss_mask_4: 0.04526  loss_dice_4: 0.9494  loss_ce_5: 6.431e-08  loss_mask_5: 0.04799  loss_dice_5: 0.99  loss_ce_6: 6.376e-08  loss_mask_6: 0.04392  loss_dice_6: 0.9009  loss_ce_7: 4.179e-07  loss_mask_7: 0.04272  loss_dice_7: 0.9257  loss_ce_8: 2.163e-07  loss_mask_8: 0.04795  loss_dice_8: 0.9238  time: 0.4137  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:05 d2.utils.events]: [0m eta: 1:04:08  iter: 719  total_loss: 10.28  loss_ce: 9.022e-06  loss_mask: 0.04592  loss_dice: 0.9543  loss_ce_0: 0.01758  loss_mask_0: 0.03618  loss_dice_0: 0.9239  loss_ce_1: 8.956e-06  loss_mask_1: 0.04532  loss_dice_1: 0.9827  loss_ce_2: 4.212e-07  loss_mask_2: 0.03794  loss_dice_2: 0.8665  loss_ce_3: 3.864e-07  loss_mask_3: 0.04448  loss_dice_3: 1.01  loss_ce_4: 1.211e-06  loss_mask_4: 0.04377  loss_dice_4: 1.002  loss_ce_5: 9.34e-08  loss_mask_5: 0.04139  loss_dice_5: 0.9231  loss_ce_6: 8.312e-08  loss_mask_6: 0.04047  loss_dice_6: 0.928  loss_ce_7: 4.845e-07  loss_mask_7: 0.0466  loss_dice_7: 0.8908  loss_ce_8: 8.509e-08  loss_mask_8: 0.0356  loss_dice_8: 0.9161  time: 0.4137  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:13 d2.utils.events]: [0m eta: 1:04:00  iter: 739  total_loss: 10.08  loss_ce: 3.667e-05  loss_mask: 0.04339  loss_dice: 0.953  loss_ce_0: 0.01602  loss_mask_0: 0.03697  loss_dice_0: 0.9379  loss_ce_1: 8.483e-06  loss_mask_1: 0.04322  loss_dice_1: 1.065  loss_ce_2: 5.465e-07  loss_mask_2: 0.04035  loss_dice_2: 0.9485  loss_ce_3: 3.038e-07  loss_mask_3: 0.03724  loss_dice_3: 0.9263  loss_ce_4: 1.162e-06  loss_mask_4: 0.0439  loss_dice_4: 0.9782  loss_ce_5: 1.021e-07  loss_mask_5: 0.04033  loss_dice_5: 0.9785  loss_ce_6: 2.288e-07  loss_mask_6: 0.04625  loss_dice_6: 0.997  loss_ce_7: 4.702e-07  loss_mask_7: 0.04548  loss_dice_7: 0.9178  loss_ce_8: 3.161e-07  loss_mask_8: 0.04105  loss_dice_8: 0.8866  time: 0.4137  data_time: 0.0085  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:17 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:50:17 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:50:17 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:50:17 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:50:17 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:50:17 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:50:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.270099 (0.270099 s / iter per device, on 1 devices)
[32m[06/02 13:50:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.211182 s / iter per device, on 1 devices)
[32m[06/02 13:50:18 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:50:18 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:50:18 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:50:18 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:50:18 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:50:18 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:50:18 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:50:22 d2.utils.events]: [0m eta: 1:03:52  iter: 759  total_loss: 10.15  loss_ce: 1.903e-05  loss_mask: 0.03956  loss_dice: 0.9569  loss_ce_0: 0.01461  loss_mask_0: 0.04391  loss_dice_0: 0.9208  loss_ce_1: 1.106e-05  loss_mask_1: 0.03558  loss_dice_1: 0.9273  loss_ce_2: 4.609e-07  loss_mask_2: 0.0378  loss_dice_2: 0.9589  loss_ce_3: 2.49e-07  loss_mask_3: 0.04045  loss_dice_3: 1.003  loss_ce_4: 6.06e-07  loss_mask_4: 0.04555  loss_dice_4: 0.9622  loss_ce_5: 1.433e-07  loss_mask_5: 0.04346  loss_dice_5: 1.005  loss_ce_6: 2.139e-07  loss_mask_6: 0.04193  loss_dice_6: 0.9797  loss_ce_7: 1.841e-07  loss_mask_7: 0.04285  loss_dice_7: 0.9317  loss_ce_8: 1.059e-07  loss_mask_8: 0.04318  loss_dice_8: 0.9568  time: 0.4137  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:30 d2.utils.events]: [0m eta: 1:03:44  iter: 779  total_loss: 9.506  loss_ce: 5.145e-06  loss_mask: 0.04492  loss_dice: 0.8779  loss_ce_0: 0.01325  loss_mask_0: 0.04234  loss_dice_0: 0.9555  loss_ce_1: 1.47e-05  loss_mask_1: 0.05024  loss_dice_1: 0.8733  loss_ce_2: 9.145e-07  loss_mask_2: 0.04446  loss_dice_2: 0.8361  loss_ce_3: 3.895e-07  loss_mask_3: 0.04698  loss_dice_3: 0.8838  loss_ce_4: 7.266e-07  loss_mask_4: 0.04088  loss_dice_4: 0.8836  loss_ce_5: 1.509e-07  loss_mask_5: 0.04377  loss_dice_5: 0.9139  loss_ce_6: 1.099e-07  loss_mask_6: 0.03962  loss_dice_6: 0.9616  loss_ce_7: 9.679e-08  loss_mask_7: 0.04097  loss_dice_7: 0.9029  loss_ce_8: 5.436e-08  loss_mask_8: 0.04013  loss_dice_8: 0.8716  time: 0.4137  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:39 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:50:39 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:50:39 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:50:39 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:50:39 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:50:39 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:50:39 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.269046 (0.269046 s / iter per device, on 1 devices)
[32m[06/02 13:50:39 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206752 s / iter per device, on 1 devices)
[32m[06/02 13:50:39 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:50:39 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:50:39 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:50:39 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:50:39 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:50:39 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:50:39 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:50:39 d2.utils.events]: [0m eta: 1:03:35  iter: 799  total_loss: 9.439  loss_ce: 1.916e-06  loss_mask: 0.04698  loss_dice: 0.927  loss_ce_0: 0.01203  loss_mask_0: 0.04279  loss_dice_0: 0.8886  loss_ce_1: 7.879e-06  loss_mask_1: 0.03939  loss_dice_1: 0.8863  loss_ce_2: 6.482e-07  loss_mask_2: 0.04176  loss_dice_2: 0.939  loss_ce_3: 3.063e-07  loss_mask_3: 0.03874  loss_dice_3: 0.8696  loss_ce_4: 5.013e-07  loss_mask_4: 0.04351  loss_dice_4: 0.9202  loss_ce_5: 5.064e-08  loss_mask_5: 0.04232  loss_dice_5: 0.9469  loss_ce_6: 8.093e-08  loss_mask_6: 0.0389  loss_dice_6: 0.9212  loss_ce_7: 1.378e-07  loss_mask_7: 0.04337  loss_dice_7: 0.8667  loss_ce_8: 2.625e-08  loss_mask_8: 0.04379  loss_dice_8: 0.9412  time: 0.4137  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:48 d2.utils.events]: [0m eta: 1:03:27  iter: 819  total_loss: 9.317  loss_ce: 1.529e-05  loss_mask: 0.0428  loss_dice: 0.9133  loss_ce_0: 0.01106  loss_mask_0: 0.03907  loss_dice_0: 0.9156  loss_ce_1: 6.866e-06  loss_mask_1: 0.03916  loss_dice_1: 0.8921  loss_ce_2: 5.326e-07  loss_mask_2: 0.03969  loss_dice_2: 0.9211  loss_ce_3: 2.969e-07  loss_mask_3: 0.042  loss_dice_3: 0.8782  loss_ce_4: 4.859e-07  loss_mask_4: 0.03964  loss_dice_4: 0.8788  loss_ce_5: 7.568e-08  loss_mask_5: 0.04008  loss_dice_5: 0.896  loss_ce_6: 1.246e-07  loss_mask_6: 0.04299  loss_dice_6: 0.8882  loss_ce_7: 1.713e-07  loss_mask_7: 0.04167  loss_dice_7: 0.9869  loss_ce_8: 6.879e-08  loss_mask_8: 0.04174  loss_dice_8: 0.9045  time: 0.4137  data_time: 0.0068  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:50:56 d2.utils.events]: [0m eta: 1:03:19  iter: 839  total_loss: 10.71  loss_ce: 1.996e-05  loss_mask: 0.04719  loss_dice: 1.022  loss_ce_0: 0.01021  loss_mask_0: 0.04716  loss_dice_0: 1.046  loss_ce_1: 9.643e-06  loss_mask_1: 0.05093  loss_dice_1: 1.062  loss_ce_2: 5.399e-07  loss_mask_2: 0.04046  loss_dice_2: 0.9967  loss_ce_3: 3.138e-07  loss_mask_3: 0.04691  loss_dice_3: 1.024  loss_ce_4: 5.523e-07  loss_mask_4: 0.03839  loss_dice_4: 1.014  loss_ce_5: 3.915e-08  loss_mask_5: 0.04925  loss_dice_5: 1.076  loss_ce_6: 8.979e-08  loss_mask_6: 0.04081  loss_dice_6: 0.9345  loss_ce_7: 2.188e-07  loss_mask_7: 0.04831  loss_dice_7: 1.095  loss_ce_8: 7.623e-08  loss_mask_8: 0.04532  loss_dice_8: 0.9878  time: 0.4137  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:00 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:51:00 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:51:00 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:51:00 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:51:00 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:51:00 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:51:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.286797 (0.286797 s / iter per device, on 1 devices)
[32m[06/02 13:51:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.227245 s / iter per device, on 1 devices)
[32m[06/02 13:51:01 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:51:01 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.825
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.825
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.850
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:51:01 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 82.525 | 100.000 | 100.000 | 82.525 |  nan  |  nan  | 85.000 | 85.000 |
[32m[06/02 13:51:01 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:51:01 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:51:01 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:51:01 d2.evaluation.testing]: [0mcopypaste: 82.5248,100.0000,100.0000,82.5248,nan,nan,85.0000,85.0000
[32m[06/02 13:51:05 d2.utils.events]: [0m eta: 1:03:11  iter: 859  total_loss: 10.8  loss_ce: 3.208e-06  loss_mask: 0.03894  loss_dice: 0.9753  loss_ce_0: 0.009487  loss_mask_0: 0.04449  loss_dice_0: 1.049  loss_ce_1: 5.143e-06  loss_mask_1: 0.04612  loss_dice_1: 0.9909  loss_ce_2: 4.737e-07  loss_mask_2: 0.04192  loss_dice_2: 0.8914  loss_ce_3: 1.709e-07  loss_mask_3: 0.04463  loss_dice_3: 1.037  loss_ce_4: 2.931e-07  loss_mask_4: 0.04185  loss_dice_4: 1.007  loss_ce_5: 1.094e-08  loss_mask_5: 0.03917  loss_dice_5: 0.9365  loss_ce_6: 9.405e-08  loss_mask_6: 0.04301  loss_dice_6: 1.064  loss_ce_7: 1.205e-07  loss_mask_7: 0.03808  loss_dice_7: 0.9214  loss_ce_8: 5.031e-08  loss_mask_8: 0.03884  loss_dice_8: 0.8174  time: 0.4137  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:14 d2.utils.events]: [0m eta: 1:03:02  iter: 879  total_loss: 11.07  loss_ce: 4.756e-06  loss_mask: 0.04451  loss_dice: 1.006  loss_ce_0: 0.008823  loss_mask_0: 0.0498  loss_dice_0: 1.2  loss_ce_1: 2.804e-06  loss_mask_1: 0.05136  loss_dice_1: 1.135  loss_ce_2: 1.359e-07  loss_mask_2: 0.04173  loss_dice_2: 1.097  loss_ce_3: 5.031e-08  loss_mask_3: 0.04846  loss_dice_3: 1.085  loss_ce_4: 2.187e-07  loss_mask_4: 0.04243  loss_dice_4: 0.9561  loss_ce_5: 1.094e-08  loss_mask_5: 0.03965  loss_dice_5: 1.008  loss_ce_6: 1.783e-07  loss_mask_6: 0.04903  loss_dice_6: 1.036  loss_ce_7: 2.865e-07  loss_mask_7: 0.04593  loss_dice_7: 1.006  loss_ce_8: 1.673e-07  loss_mask_8: 0.04741  loss_dice_8: 1.085  time: 0.4137  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:22 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:51:22 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:51:22 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:51:22 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:51:22 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:51:22 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:51:23 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277020 (0.277020 s / iter per device, on 1 devices)
[32m[06/02 13:51:23 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212865 s / iter per device, on 1 devices)
[32m[06/02 13:51:23 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:51:23 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:51:23 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:51:23 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:51:23 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:51:23 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:51:23 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:51:23 d2.utils.events]: [0m eta: 1:02:54  iter: 899  total_loss: 9.643  loss_ce: 1.774e-05  loss_mask: 0.04432  loss_dice: 1.089  loss_ce_0: 0.008229  loss_mask_0: 0.04203  loss_dice_0: 0.9432  loss_ce_1: 2.734e-06  loss_mask_1: 0.04018  loss_dice_1: 0.9066  loss_ce_2: 1.611e-07  loss_mask_2: 0.04036  loss_dice_2: 0.9329  loss_ce_3: 4.593e-08  loss_mask_3: 0.04353  loss_dice_3: 1.005  loss_ce_4: 4.189e-07  loss_mask_4: 0.04375  loss_dice_4: 0.977  loss_ce_5: 1.75e-08  loss_mask_5: 0.03988  loss_dice_5: 0.9073  loss_ce_6: 2.111e-07  loss_mask_6: 0.04413  loss_dice_6: 1.022  loss_ce_7: 2.299e-06  loss_mask_7: 0.04445  loss_dice_7: 0.8384  loss_ce_8: 2.887e-07  loss_mask_8: 0.04016  loss_dice_8: 0.9261  time: 0.4138  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:31 d2.utils.events]: [0m eta: 1:02:45  iter: 919  total_loss: 9.684  loss_ce: 0.0001466  loss_mask: 0.04013  loss_dice: 0.9051  loss_ce_0: 0.007601  loss_mask_0: 0.0475  loss_dice_0: 0.9238  loss_ce_1: 3.763e-06  loss_mask_1: 0.04527  loss_dice_1: 0.9636  loss_ce_2: 1.76e-07  loss_mask_2: 0.04664  loss_dice_2: 0.9306  loss_ce_3: 4.812e-08  loss_mask_3: 0.03837  loss_dice_3: 0.9383  loss_ce_4: 2.865e-07  loss_mask_4: 0.03961  loss_dice_4: 0.927  loss_ce_5: 2.515e-08  loss_mask_5: 0.03775  loss_dice_5: 0.8876  loss_ce_6: 3.861e-07  loss_mask_6: 0.04228  loss_dice_6: 0.8887  loss_ce_7: 9.734e-06  loss_mask_7: 0.04564  loss_dice_7: 0.9413  loss_ce_8: 4.945e-06  loss_mask_8: 0.03997  loss_dice_8: 0.8765  time: 0.4138  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:39 d2.utils.events]: [0m eta: 1:02:37  iter: 939  total_loss: 9.269  loss_ce: 8.793e-05  loss_mask: 0.04098  loss_dice: 0.8896  loss_ce_0: 0.006994  loss_mask_0: 0.03754  loss_dice_0: 0.8451  loss_ce_1: 3.373e-06  loss_mask_1: 0.04366  loss_dice_1: 0.8717  loss_ce_2: 1.485e-07  loss_mask_2: 0.03972  loss_dice_2: 0.8893  loss_ce_3: 8.64e-08  loss_mask_3: 0.04094  loss_dice_3: 0.9055  loss_ce_4: 1.093e-06  loss_mask_4: 0.04215  loss_dice_4: 0.8715  loss_ce_5: 1.028e-07  loss_mask_5: 0.03975  loss_dice_5: 0.8615  loss_ce_6: 2.362e-07  loss_mask_6: 0.03892  loss_dice_6: 0.8875  loss_ce_7: 3.528e-06  loss_mask_7: 0.04021  loss_dice_7: 0.83  loss_ce_8: 5.307e-06  loss_mask_8: 0.04202  loss_dice_8: 0.9224  time: 0.4138  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:44 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:51:44 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:51:44 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:51:44 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:51:44 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:51:44 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:51:44 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.281516 (0.281516 s / iter per device, on 1 devices)
[32m[06/02 13:51:44 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.221822 s / iter per device, on 1 devices)
[32m[06/02 13:51:44 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:51:44 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:51:44 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:51:44 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:51:44 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:51:44 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:51:44 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:51:49 d2.utils.events]: [0m eta: 1:02:29  iter: 959  total_loss: 8.849  loss_ce: 7.524e-05  loss_mask: 0.03788  loss_dice: 0.8246  loss_ce_0: 0.006448  loss_mask_0: 0.03642  loss_dice_0: 0.8385  loss_ce_1: 4.39e-06  loss_mask_1: 0.04178  loss_dice_1: 0.9061  loss_ce_2: 1.578e-07  loss_mask_2: 0.04339  loss_dice_2: 0.8494  loss_ce_3: 1.531e-07  loss_mask_3: 0.04343  loss_dice_3: 0.8182  loss_ce_4: 8.946e-07  loss_mask_4: 0.04394  loss_dice_4: 0.8139  loss_ce_5: 5.578e-08  loss_mask_5: 0.03998  loss_dice_5: 0.7713  loss_ce_6: 1.17e-07  loss_mask_6: 0.04118  loss_dice_6: 0.7922  loss_ce_7: 8.301e-07  loss_mask_7: 0.03826  loss_dice_7: 0.8453  loss_ce_8: 1.208e-06  loss_mask_8: 0.03829  loss_dice_8: 0.8179  time: 0.4139  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:51:57 d2.utils.events]: [0m eta: 1:02:21  iter: 979  total_loss: 8.585  loss_ce: 4.587e-05  loss_mask: 0.03829  loss_dice: 0.8029  loss_ce_0: 0.005983  loss_mask_0: 0.03789  loss_dice_0: 0.8152  loss_ce_1: 4.004e-06  loss_mask_1: 0.04024  loss_dice_1: 0.8518  loss_ce_2: 9.11e-08  loss_mask_2: 0.03733  loss_dice_2: 0.8507  loss_ce_3: 8.749e-08  loss_mask_3: 0.03913  loss_dice_3: 0.7476  loss_ce_4: 2.067e-07  loss_mask_4: 0.03427  loss_dice_4: 0.7444  loss_ce_5: 2.406e-08  loss_mask_5: 0.04318  loss_dice_5: 0.7994  loss_ce_6: 7.765e-08  loss_mask_6: 0.03869  loss_dice_6: 0.8082  loss_ce_7: 1.15e-06  loss_mask_7: 0.03924  loss_dice_7: 0.8005  loss_ce_8: 3.258e-06  loss_mask_8: 0.04086  loss_dice_8: 0.794  time: 0.4140  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:52:05 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0000999.pth
[32m[06/02 13:52:09 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:52:09 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:52:09 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:52:09 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:52:09 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:52:09 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:52:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.285641 (0.285641 s / iter per device, on 1 devices)
[32m[06/02 13:52:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.224211 s / iter per device, on 1 devices)
[32m[06/02 13:52:09 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:52:09 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:52:09 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:52:09 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:52:09 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:52:09 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:52:09 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:52:09 d2.utils.events]: [0m eta: 1:02:13  iter: 999  total_loss: 8.515  loss_ce: 0.0001057  loss_mask: 0.03935  loss_dice: 0.7961  loss_ce_0: 0.005521  loss_mask_0: 0.03709  loss_dice_0: 0.88  loss_ce_1: 3.412e-06  loss_mask_1: 0.03838  loss_dice_1: 0.7927  loss_ce_2: 9.974e-08  loss_mask_2: 0.04313  loss_dice_2: 0.8218  loss_ce_3: 6.781e-08  loss_mask_3: 0.0419  loss_dice_3: 0.8337  loss_ce_4: 5.949e-07  loss_mask_4: 0.03968  loss_dice_4: 0.8018  loss_ce_5: 5.796e-08  loss_mask_5: 0.04618  loss_dice_5: 0.7886  loss_ce_6: 2.187e-07  loss_mask_6: 0.04539  loss_dice_6: 0.8567  loss_ce_7: 1.903e-06  loss_mask_7: 0.0465  loss_dice_7: 0.7801  loss_ce_8: 8.99e-06  loss_mask_8: 0.04172  loss_dice_8: 0.8263  time: 0.4140  data_time: 0.0070  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:52:18 d2.utils.events]: [0m eta: 1:02:05  iter: 1019  total_loss: 8.591  loss_ce: 9.379e-05  loss_mask: 0.04039  loss_dice: 0.8791  loss_ce_0: 0.00517  loss_mask_0: 0.03988  loss_dice_0: 0.7966  loss_ce_1: 3.725e-06  loss_mask_1: 0.04315  loss_dice_1: 0.8929  loss_ce_2: 7.568e-08  loss_mask_2: 0.04305  loss_dice_2: 0.8437  loss_ce_3: 2.844e-08  loss_mask_3: 0.03659  loss_dice_3: 0.7987  loss_ce_4: 2.22e-07  loss_mask_4: 0.03572  loss_dice_4: 0.7981  loss_ce_5: 2.625e-08  loss_mask_5: 0.04019  loss_dice_5: 0.8087  loss_ce_6: 1.312e-07  loss_mask_6: 0.03819  loss_dice_6: 0.8283  loss_ce_7: 7.382e-07  loss_mask_7: 0.03984  loss_dice_7: 0.806  loss_ce_8: 1.972e-06  loss_mask_8: 0.04264  loss_dice_8: 0.8538  time: 0.4141  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:52:26 d2.utils.events]: [0m eta: 1:01:57  iter: 1039  total_loss: 9.161  loss_ce: 2.642e-05  loss_mask: 0.04595  loss_dice: 0.8639  loss_ce_0: 0.004822  loss_mask_0: 0.04047  loss_dice_0: 0.9286  loss_ce_1: 2.018e-06  loss_mask_1: 0.03614  loss_dice_1: 0.8935  loss_ce_2: 4.331e-08  loss_mask_2: 0.03689  loss_dice_2: 0.8823  loss_ce_3: 2.625e-08  loss_mask_3: 0.03647  loss_dice_3: 0.8728  loss_ce_4: 6.529e-07  loss_mask_4: 0.04121  loss_dice_4: 0.9128  loss_ce_5: 3.062e-08  loss_mask_5: 0.03889  loss_dice_5: 0.8788  loss_ce_6: 1.947e-07  loss_mask_6: 0.04178  loss_dice_6: 0.8651  loss_ce_7: 9.515e-07  loss_mask_7: 0.0451  loss_dice_7: 0.8662  loss_ce_8: 1.434e-06  loss_mask_8: 0.03979  loss_dice_8: 0.9115  time: 0.4141  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:52:30 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:52:30 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:52:30 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:52:30 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:52:30 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:52:30 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:52:31 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282852 (0.282852 s / iter per device, on 1 devices)
[32m[06/02 13:52:31 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.221242 s / iter per device, on 1 devices)
[32m[06/02 13:52:31 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:52:31 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:52:31 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:52:31 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:52:31 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:52:31 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:52:31 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:52:35 d2.utils.events]: [0m eta: 1:01:49  iter: 1059  total_loss: 8.795  loss_ce: 2.633e-05  loss_mask: 0.04263  loss_dice: 0.9284  loss_ce_0: 0.00453  loss_mask_0: 0.03557  loss_dice_0: 0.8212  loss_ce_1: 2.515e-06  loss_mask_1: 0.04101  loss_dice_1: 0.8497  loss_ce_2: 4.517e-08  loss_mask_2: 0.03442  loss_dice_2: 0.8144  loss_ce_3: 4.265e-08  loss_mask_3: 0.0392  loss_dice_3: 0.8507  loss_ce_4: 1.035e-06  loss_mask_4: 0.04373  loss_dice_4: 0.7726  loss_ce_5: 6.015e-08  loss_mask_5: 0.03961  loss_dice_5: 0.8826  loss_ce_6: 2.023e-07  loss_mask_6: 0.03822  loss_dice_6: 0.7952  loss_ce_7: 8.016e-07  loss_mask_7: 0.03681  loss_dice_7: 0.806  loss_ce_8: 2.327e-06  loss_mask_8: 0.04089  loss_dice_8: 0.8158  time: 0.4140  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:52:43 d2.utils.events]: [0m eta: 1:01:41  iter: 1079  total_loss: 8.86  loss_ce: 9.897e-06  loss_mask: 0.03626  loss_dice: 0.7894  loss_ce_0: 0.004225  loss_mask_0: 0.0404  loss_dice_0: 0.8007  loss_ce_1: 2.242e-06  loss_mask_1: 0.03816  loss_dice_1: 0.8038  loss_ce_2: 5.053e-08  loss_mask_2: 0.03884  loss_dice_2: 0.8129  loss_ce_3: 3.609e-08  loss_mask_3: 0.04412  loss_dice_3: 0.8934  loss_ce_4: 4.32e-07  loss_mask_4: 0.03954  loss_dice_4: 0.8637  loss_ce_5: 5.14e-08  loss_mask_5: 0.03635  loss_dice_5: 0.8485  loss_ce_6: 1.247e-07  loss_mask_6: 0.03876  loss_dice_6: 0.8334  loss_ce_7: 3.018e-07  loss_mask_7: 0.0398  loss_dice_7: 0.8021  loss_ce_8: 3.839e-07  loss_mask_8: 0.04361  loss_dice_8: 0.8903  time: 0.4141  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:52:52 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:52:52 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:52:52 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:52:52 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:52:52 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:52:52 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:52:53 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.269364 (0.269364 s / iter per device, on 1 devices)
[32m[06/02 13:52:53 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208433 s / iter per device, on 1 devices)
[32m[06/02 13:52:53 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:52:53 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:52:53 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:52:53 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:52:53 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:52:53 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:52:53 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:52:53 d2.utils.events]: [0m eta: 1:01:32  iter: 1099  total_loss: 9.034  loss_ce: 7.021e-05  loss_mask: 0.04077  loss_dice: 0.7899  loss_ce_0: 0.00396  loss_mask_0: 0.04193  loss_dice_0: 0.8702  loss_ce_1: 2.22e-06  loss_mask_1: 0.04181  loss_dice_1: 0.8779  loss_ce_2: 7.415e-08  loss_mask_2: 0.04051  loss_dice_2: 0.8403  loss_ce_3: 3.281e-08  loss_mask_3: 0.0444  loss_dice_3: 0.8433  loss_ce_4: 2.45e-07  loss_mask_4: 0.04185  loss_dice_4: 0.8675  loss_ce_5: 4.375e-08  loss_mask_5: 0.04613  loss_dice_5: 0.8765  loss_ce_6: 2.1e-07  loss_mask_6: 0.04219  loss_dice_6: 0.8841  loss_ce_7: 3.839e-07  loss_mask_7: 0.04276  loss_dice_7: 0.7882  loss_ce_8: 2.544e-06  loss_mask_8: 0.04561  loss_dice_8: 0.823  time: 0.4141  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:01 d2.utils.events]: [0m eta: 1:01:23  iter: 1119  total_loss: 9.219  loss_ce: 2.288e-05  loss_mask: 0.04218  loss_dice: 0.8764  loss_ce_0: 0.003746  loss_mask_0: 0.0401  loss_dice_0: 0.8345  loss_ce_1: 2.355e-06  loss_mask_1: 0.03895  loss_dice_1: 0.841  loss_ce_2: 5.873e-08  loss_mask_2: 0.04424  loss_dice_2: 0.858  loss_ce_3: 3.5e-08  loss_mask_3: 0.03984  loss_dice_3: 0.9048  loss_ce_4: 2.165e-07  loss_mask_4: 0.04089  loss_dice_4: 0.7875  loss_ce_5: 3.062e-08  loss_mask_5: 0.04219  loss_dice_5: 0.8566  loss_ce_6: 1.487e-07  loss_mask_6: 0.0405  loss_dice_6: 0.8365  loss_ce_7: 1.531e-07  loss_mask_7: 0.03955  loss_dice_7: 0.8181  loss_ce_8: 4.047e-07  loss_mask_8: 0.0348  loss_dice_8: 0.8823  time: 0.4140  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:09 d2.utils.events]: [0m eta: 1:01:16  iter: 1139  total_loss: 8.928  loss_ce: 9.588e-06  loss_mask: 0.03855  loss_dice: 0.8168  loss_ce_0: 0.003535  loss_mask_0: 0.03757  loss_dice_0: 0.8359  loss_ce_1: 2.046e-06  loss_mask_1: 0.03646  loss_dice_1: 0.8232  loss_ce_2: 5.534e-08  loss_mask_2: 0.0417  loss_dice_2: 0.8686  loss_ce_3: 3.609e-08  loss_mask_3: 0.03878  loss_dice_3: 0.8503  loss_ce_4: 1.444e-07  loss_mask_4: 0.04165  loss_dice_4: 0.8406  loss_ce_5: 1.094e-08  loss_mask_5: 0.04309  loss_dice_5: 0.8626  loss_ce_6: 9.734e-08  loss_mask_6: 0.03867  loss_dice_6: 0.8371  loss_ce_7: 1.236e-07  loss_mask_7: 0.03873  loss_dice_7: 0.8427  loss_ce_8: 9.624e-08  loss_mask_8: 0.04137  loss_dice_8: 0.8617  time: 0.4141  data_time: 0.0084  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:13 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:53:13 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:53:13 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:53:13 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:53:13 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:53:13 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:53:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280236 (0.280236 s / iter per device, on 1 devices)
[32m[06/02 13:53:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.219980 s / iter per device, on 1 devices)
[32m[06/02 13:53:14 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:53:14 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:53:14 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:53:14 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:53:14 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:53:14 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:53:14 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:53:18 d2.utils.events]: [0m eta: 1:01:07  iter: 1159  total_loss: 8.826  loss_ce: 9.42e-06  loss_mask: 0.04095  loss_dice: 0.8285  loss_ce_0: 0.003332  loss_mask_0: 0.0448  loss_dice_0: 0.8063  loss_ce_1: 1.471e-06  loss_mask_1: 0.04411  loss_dice_1: 0.8923  loss_ce_2: 5.195e-08  loss_mask_2: 0.04416  loss_dice_2: 0.8105  loss_ce_3: 4.298e-08  loss_mask_3: 0.03986  loss_dice_3: 0.8099  loss_ce_4: 1.476e-07  loss_mask_4: 0.03999  loss_dice_4: 0.8353  loss_ce_5: 6.562e-09  loss_mask_5: 0.03928  loss_dice_5: 0.7685  loss_ce_6: 6.781e-08  loss_mask_6: 0.04258  loss_dice_6: 0.8225  loss_ce_7: 1.203e-07  loss_mask_7: 0.04002  loss_dice_7: 0.7672  loss_ce_8: 6.89e-08  loss_mask_8: 0.03571  loss_dice_8: 0.8132  time: 0.4140  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:27 d2.utils.events]: [0m eta: 1:00:59  iter: 1179  total_loss: 9.044  loss_ce: 1.315e-05  loss_mask: 0.0367  loss_dice: 0.7378  loss_ce_0: 0.00316  loss_mask_0: 0.04035  loss_dice_0: 0.7976  loss_ce_1: 1.252e-06  loss_mask_1: 0.04098  loss_dice_1: 0.7977  loss_ce_2: 5.479e-08  loss_mask_2: 0.03945  loss_dice_2: 0.8852  loss_ce_3: 5.775e-08  loss_mask_3: 0.03596  loss_dice_3: 0.9244  loss_ce_4: 5.217e-07  loss_mask_4: 0.03949  loss_dice_4: 0.8005  loss_ce_5: 1.312e-08  loss_mask_5: 0.04256  loss_dice_5: 0.7781  loss_ce_6: 4.156e-08  loss_mask_6: 0.03363  loss_dice_6: 0.843  loss_ce_7: 2.701e-07  loss_mask_7: 0.04362  loss_dice_7: 0.8535  loss_ce_8: 3.543e-07  loss_mask_8: 0.03651  loss_dice_8: 0.7984  time: 0.4141  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:35 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:53:35 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:53:35 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:53:35 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:53:35 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:53:35 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:53:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.324401 (0.324401 s / iter per device, on 1 devices)
[32m[06/02 13:53:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.262472 s / iter per device, on 1 devices)
[32m[06/02 13:53:36 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:53:36 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:53:36 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:53:36 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:53:36 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:53:36 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:53:36 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:53:36 d2.utils.events]: [0m eta: 1:00:51  iter: 1199  total_loss: 8.057  loss_ce: 2.943e-05  loss_mask: 0.04026  loss_dice: 0.7774  loss_ce_0: 0.002988  loss_mask_0: 0.03965  loss_dice_0: 0.7734  loss_ce_1: 1.249e-06  loss_mask_1: 0.04154  loss_dice_1: 0.8172  loss_ce_2: 7.896e-08  loss_mask_2: 0.04198  loss_dice_2: 0.8017  loss_ce_3: 4.287e-08  loss_mask_3: 0.04084  loss_dice_3: 0.8438  loss_ce_4: 4.079e-07  loss_mask_4: 0.03601  loss_dice_4: 0.6874  loss_ce_5: 6.562e-09  loss_mask_5: 0.03687  loss_dice_5: 0.7259  loss_ce_6: 3.609e-08  loss_mask_6: 0.03833  loss_dice_6: 0.77  loss_ce_7: 1.958e-07  loss_mask_7: 0.04087  loss_dice_7: 0.797  loss_ce_8: 7.089e-07  loss_mask_8: 0.03763  loss_dice_8: 0.7656  time: 0.4141  data_time: 0.0088  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:44 d2.utils.events]: [0m eta: 1:00:43  iter: 1219  total_loss: 7.944  loss_ce: 2.29e-05  loss_mask: 0.03949  loss_dice: 0.7726  loss_ce_0: 0.002792  loss_mask_0: 0.04239  loss_dice_0: 0.7996  loss_ce_1: 1.476e-06  loss_mask_1: 0.04371  loss_dice_1: 0.8265  loss_ce_2: 5.403e-08  loss_mask_2: 0.03969  loss_dice_2: 0.7975  loss_ce_3: 3.095e-08  loss_mask_3: 0.0394  loss_dice_3: 0.7735  loss_ce_4: 2.1e-07  loss_mask_4: 0.03846  loss_dice_4: 0.7573  loss_ce_5: 6.562e-09  loss_mask_5: 0.03685  loss_dice_5: 0.7935  loss_ce_6: 2.297e-08  loss_mask_6: 0.03756  loss_dice_6: 0.7117  loss_ce_7: 2.319e-07  loss_mask_7: 0.03708  loss_dice_7: 0.7876  loss_ce_8: 8.093e-07  loss_mask_8: 0.0382  loss_dice_8: 0.7969  time: 0.4141  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:53 d2.utils.events]: [0m eta: 1:00:35  iter: 1239  total_loss: 8.066  loss_ce: 2.476e-05  loss_mask: 0.04043  loss_dice: 0.8049  loss_ce_0: 0.002629  loss_mask_0: 0.04065  loss_dice_0: 0.8443  loss_ce_1: 1.115e-06  loss_mask_1: 0.04054  loss_dice_1: 0.7954  loss_ce_2: 4.692e-08  loss_mask_2: 0.03587  loss_dice_2: 0.816  loss_ce_3: 3.543e-08  loss_mask_3: 0.0397  loss_dice_3: 0.8039  loss_ce_4: 3.511e-07  loss_mask_4: 0.04012  loss_dice_4: 0.7154  loss_ce_5: 7.656e-09  loss_mask_5: 0.03921  loss_dice_5: 0.7341  loss_ce_6: 1.969e-08  loss_mask_6: 0.04059  loss_dice_6: 0.7712  loss_ce_7: 3.599e-07  loss_mask_7: 0.03597  loss_dice_7: 0.7557  loss_ce_8: 2.751e-06  loss_mask_8: 0.03996  loss_dice_8: 0.7738  time: 0.4142  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:53:57 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:53:57 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:53:57 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:53:57 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:53:57 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:53:57 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:53:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264357 (0.264357 s / iter per device, on 1 devices)
[32m[06/02 13:53:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.201345 s / iter per device, on 1 devices)
[32m[06/02 13:53:57 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:53:57 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:53:57 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:53:57 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:53:57 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:53:57 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:53:57 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:54:02 d2.utils.events]: [0m eta: 1:00:27  iter: 1259  total_loss: 8.389  loss_ce: 2.083e-05  loss_mask: 0.03482  loss_dice: 0.813  loss_ce_0: 0.002492  loss_mask_0: 0.03723  loss_dice_0: 0.7282  loss_ce_1: 8.079e-07  loss_mask_1: 0.03782  loss_dice_1: 0.7619  loss_ce_2: 4.965e-08  loss_mask_2: 0.03681  loss_dice_2: 0.7881  loss_ce_3: 2.603e-08  loss_mask_3: 0.03436  loss_dice_3: 0.8044  loss_ce_4: 1.203e-07  loss_mask_4: 0.03768  loss_dice_4: 0.7703  loss_ce_5: 4.375e-09  loss_mask_5: 0.03915  loss_dice_5: 0.8235  loss_ce_6: 1.312e-08  loss_mask_6: 0.03567  loss_dice_6: 0.8123  loss_ce_7: 1.829e-07  loss_mask_7: 0.0399  loss_dice_7: 0.8221  loss_ce_8: 7.782e-07  loss_mask_8: 0.03613  loss_dice_8: 0.7658  time: 0.4142  data_time: 0.0064  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:54:10 d2.utils.events]: [0m eta: 1:00:19  iter: 1279  total_loss: 8.442  loss_ce: 1.757e-05  loss_mask: 0.04046  loss_dice: 0.8052  loss_ce_0: 0.002351  loss_mask_0: 0.04141  loss_dice_0: 0.8404  loss_ce_1: 7.526e-07  loss_mask_1: 0.03296  loss_dice_1: 0.7868  loss_ce_2: 7.776e-08  loss_mask_2: 0.043  loss_dice_2: 0.8392  loss_ce_3: 2.034e-08  loss_mask_3: 0.03699  loss_dice_3: 0.7888  loss_ce_4: 6.343e-08  loss_mask_4: 0.04064  loss_dice_4: 0.7973  loss_ce_5: 4.375e-09  loss_mask_5: 0.03687  loss_dice_5: 0.759  loss_ce_6: 1.312e-08  loss_mask_6: 0.03767  loss_dice_6: 0.7643  loss_ce_7: 1.039e-07  loss_mask_7: 0.03439  loss_dice_7: 0.8168  loss_ce_8: 9.471e-07  loss_mask_8: 0.03844  loss_dice_8: 0.8521  time: 0.4142  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:54:18 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:54:18 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:54:18 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:54:18 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:54:18 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:54:18 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:54:19 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.281333 (0.281333 s / iter per device, on 1 devices)
[32m[06/02 13:54:19 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.216617 s / iter per device, on 1 devices)
[32m[06/02 13:54:19 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:54:19 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:54:19 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:54:19 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:54:19 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:54:19 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:54:19 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:54:19 d2.utils.events]: [0m eta: 1:00:11  iter: 1299  total_loss: 8.4  loss_ce: 1.411e-05  loss_mask: 0.04983  loss_dice: 0.8648  loss_ce_0: 0.002226  loss_mask_0: 0.04179  loss_dice_0: 0.7399  loss_ce_1: 7.069e-07  loss_mask_1: 0.04181  loss_dice_1: 0.7993  loss_ce_2: 8.071e-08  loss_mask_2: 0.03931  loss_dice_2: 0.7543  loss_ce_3: 2.515e-08  loss_mask_3: 0.04692  loss_dice_3: 0.8376  loss_ce_4: 6.453e-08  loss_mask_4: 0.04176  loss_dice_4: 0.8934  loss_ce_5: 4.375e-09  loss_mask_5: 0.04118  loss_dice_5: 0.7599  loss_ce_6: 1.422e-08  loss_mask_6: 0.04503  loss_dice_6: 0.7644  loss_ce_7: 9.952e-08  loss_mask_7: 0.03739  loss_dice_7: 0.8369  loss_ce_8: 1.047e-06  loss_mask_8: 0.04341  loss_dice_8: 0.7909  time: 0.4142  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:54:28 d2.utils.events]: [0m eta: 1:00:03  iter: 1319  total_loss: 9.175  loss_ce: 1.597e-05  loss_mask: 0.04148  loss_dice: 0.8158  loss_ce_0: 0.002129  loss_mask_0: 0.0403  loss_dice_0: 0.7895  loss_ce_1: 5.776e-07  loss_mask_1: 0.04608  loss_dice_1: 0.8806  loss_ce_2: 8.082e-08  loss_mask_2: 0.04104  loss_dice_2: 0.8681  loss_ce_3: 1.969e-08  loss_mask_3: 0.04268  loss_dice_3: 0.9678  loss_ce_4: 6.234e-08  loss_mask_4: 0.03639  loss_dice_4: 0.8266  loss_ce_5: 3.281e-09  loss_mask_5: 0.04067  loss_dice_5: 0.7835  loss_ce_6: 8.749e-09  loss_mask_6: 0.04259  loss_dice_6: 0.858  loss_ce_7: 8.531e-08  loss_mask_7: 0.04082  loss_dice_7: 0.8579  loss_ce_8: 2.067e-07  loss_mask_8: 0.04383  loss_dice_8: 0.9024  time: 0.4143  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:54:36 d2.utils.events]: [0m eta: 0:59:55  iter: 1339  total_loss: 8.905  loss_ce: 7.863e-06  loss_mask: 0.0421  loss_dice: 0.7612  loss_ce_0: 0.002042  loss_mask_0: 0.04076  loss_dice_0: 0.7867  loss_ce_1: 5.855e-07  loss_mask_1: 0.04254  loss_dice_1: 0.8331  loss_ce_2: 1.236e-07  loss_mask_2: 0.0416  loss_dice_2: 0.9512  loss_ce_3: 3.172e-08  loss_mask_3: 0.04133  loss_dice_3: 0.8291  loss_ce_4: 1.947e-07  loss_mask_4: 0.0363  loss_dice_4: 0.7377  loss_ce_5: 4.375e-09  loss_mask_5: 0.03936  loss_dice_5: 0.8764  loss_ce_6: 4.375e-09  loss_mask_6: 0.04111  loss_dice_6: 0.9401  loss_ce_7: 6.562e-08  loss_mask_7: 0.037  loss_dice_7: 0.8899  loss_ce_8: 1.116e-07  loss_mask_8: 0.03879  loss_dice_8: 0.846  time: 0.4143  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:54:40 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:54:40 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:54:40 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:54:40 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:54:40 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:54:40 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:54:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274125 (0.274125 s / iter per device, on 1 devices)
[32m[06/02 13:54:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208457 s / iter per device, on 1 devices)
[32m[06/02 13:54:41 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:54:41 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:54:41 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:54:41 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:54:41 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:54:41 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:54:41 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:54:45 d2.utils.events]: [0m eta: 0:59:46  iter: 1359  total_loss: 7.702  loss_ce: 1.214e-05  loss_mask: 0.04163  loss_dice: 0.7712  loss_ce_0: 0.001954  loss_mask_0: 0.03493  loss_dice_0: 0.7461  loss_ce_1: 6.049e-07  loss_mask_1: 0.03641  loss_dice_1: 0.7475  loss_ce_2: 8.859e-08  loss_mask_2: 0.03585  loss_dice_2: 0.7117  loss_ce_3: 1.969e-08  loss_mask_3: 0.03348  loss_dice_3: 0.7069  loss_ce_4: 2.428e-07  loss_mask_4: 0.03974  loss_dice_4: 0.7532  loss_ce_5: 4.375e-09  loss_mask_5: 0.03473  loss_dice_5: 0.7555  loss_ce_6: 6.562e-09  loss_mask_6: 0.03598  loss_dice_6: 0.7128  loss_ce_7: 2.089e-07  loss_mask_7: 0.03574  loss_dice_7: 0.7455  loss_ce_8: 2.712e-07  loss_mask_8: 0.0341  loss_dice_8: 0.6682  time: 0.4143  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:54:53 d2.utils.events]: [0m eta: 0:59:38  iter: 1379  total_loss: 8.118  loss_ce: 1.143e-05  loss_mask: 0.03868  loss_dice: 0.7747  loss_ce_0: 0.00186  loss_mask_0: 0.03378  loss_dice_0: 0.7421  loss_ce_1: 5.662e-07  loss_mask_1: 0.04119  loss_dice_1: 0.7909  loss_ce_2: 5.709e-08  loss_mask_2: 0.03468  loss_dice_2: 0.7302  loss_ce_3: 2.406e-08  loss_mask_3: 0.03664  loss_dice_3: 0.744  loss_ce_4: 2.264e-07  loss_mask_4: 0.03682  loss_dice_4: 0.7638  loss_ce_5: 4.375e-09  loss_mask_5: 0.03817  loss_dice_5: 0.7816  loss_ce_6: 8.749e-09  loss_mask_6: 0.03732  loss_dice_6: 0.7158  loss_ce_7: 2.012e-07  loss_mask_7: 0.03594  loss_dice_7: 0.7482  loss_ce_8: 8.016e-07  loss_mask_8: 0.03886  loss_dice_8: 0.775  time: 0.4144  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:02 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:55:02 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:55:02 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:55:02 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:55:02 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:55:02 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:55:02 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.257226 (0.257226 s / iter per device, on 1 devices)
[32m[06/02 13:55:02 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.199191 s / iter per device, on 1 devices)
[32m[06/02 13:55:02 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:55:02 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:55:03 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:55:03 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:55:03 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:55:03 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:55:03 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:55:03 d2.utils.events]: [0m eta: 0:59:30  iter: 1399  total_loss: 8.04  loss_ce: 5.571e-06  loss_mask: 0.03715  loss_dice: 0.7485  loss_ce_0: 0.00177  loss_mask_0: 0.04042  loss_dice_0: 0.8162  loss_ce_1: 4.564e-07  loss_mask_1: 0.0358  loss_dice_1: 0.7622  loss_ce_2: 4.418e-08  loss_mask_2: 0.03616  loss_dice_2: 0.7434  loss_ce_3: 3.281e-08  loss_mask_3: 0.03778  loss_dice_3: 0.775  loss_ce_4: 2.778e-07  loss_mask_4: 0.03615  loss_dice_4: 0.7284  loss_ce_5: 4.375e-09  loss_mask_5: 0.03234  loss_dice_5: 0.7429  loss_ce_6: 8.749e-09  loss_mask_6: 0.03887  loss_dice_6: 0.7221  loss_ce_7: 2.581e-07  loss_mask_7: 0.03929  loss_dice_7: 0.7736  loss_ce_8: 2.581e-07  loss_mask_8: 0.03737  loss_dice_8: 0.7193  time: 0.4144  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:11 d2.utils.events]: [0m eta: 0:59:22  iter: 1419  total_loss: 8.756  loss_ce: 8.171e-06  loss_mask: 0.0346  loss_dice: 0.8389  loss_ce_0: 0.001686  loss_mask_0: 0.03268  loss_dice_0: 0.7819  loss_ce_1: 5.675e-07  loss_mask_1: 0.0443  loss_dice_1: 0.8333  loss_ce_2: 5.403e-08  loss_mask_2: 0.0392  loss_dice_2: 0.8643  loss_ce_3: 3.5e-08  loss_mask_3: 0.03949  loss_dice_3: 0.8536  loss_ce_4: 3.248e-07  loss_mask_4: 0.0418  loss_dice_4: 0.8583  loss_ce_5: 2.187e-09  loss_mask_5: 0.03544  loss_dice_5: 0.783  loss_ce_6: 1.312e-08  loss_mask_6: 0.03846  loss_dice_6: 0.7956  loss_ce_7: 8.869e-07  loss_mask_7: 0.04131  loss_dice_7: 0.7997  loss_ce_8: 2.898e-07  loss_mask_8: 0.03834  loss_dice_8: 0.8177  time: 0.4144  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:19 d2.utils.events]: [0m eta: 0:59:15  iter: 1439  total_loss: 8.603  loss_ce: 4.745e-05  loss_mask: 0.04122  loss_dice: 0.7811  loss_ce_0: 0.001609  loss_mask_0: 0.04298  loss_dice_0: 0.8225  loss_ce_1: 4.533e-07  loss_mask_1: 0.03942  loss_dice_1: 0.7503  loss_ce_2: 4.845e-08  loss_mask_2: 0.03997  loss_dice_2: 0.7793  loss_ce_3: 2.625e-08  loss_mask_3: 0.03915  loss_dice_3: 0.8035  loss_ce_4: 1.837e-07  loss_mask_4: 0.04061  loss_dice_4: 0.8079  loss_ce_5: 4.375e-09  loss_mask_5: 0.0393  loss_dice_5: 0.8245  loss_ce_6: 1.312e-08  loss_mask_6: 0.03569  loss_dice_6: 0.7229  loss_ce_7: 1.237e-06  loss_mask_7: 0.03847  loss_dice_7: 0.7416  loss_ce_8: 2.17e-06  loss_mask_8: 0.03885  loss_dice_8: 0.7652  time: 0.4144  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:23 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:55:23 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:55:23 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:55:23 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:55:23 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:55:23 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:55:24 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277161 (0.277161 s / iter per device, on 1 devices)
[32m[06/02 13:55:24 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213074 s / iter per device, on 1 devices)
[32m[06/02 13:55:24 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:55:24 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:55:24 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:55:24 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:55:24 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:55:24 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:55:24 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:55:28 d2.utils.events]: [0m eta: 0:59:07  iter: 1459  total_loss: 8.288  loss_ce: 2.807e-05  loss_mask: 0.04258  loss_dice: 0.7973  loss_ce_0: 0.001546  loss_mask_0: 0.03887  loss_dice_0: 0.7422  loss_ce_1: 5.073e-07  loss_mask_1: 0.03831  loss_dice_1: 0.7937  loss_ce_2: 5.753e-08  loss_mask_2: 0.0348  loss_dice_2: 0.7343  loss_ce_3: 1.99e-08  loss_mask_3: 0.03962  loss_dice_3: 0.8007  loss_ce_4: 1.936e-07  loss_mask_4: 0.03957  loss_dice_4: 0.8276  loss_ce_5: 4.375e-09  loss_mask_5: 0.03597  loss_dice_5: 0.7458  loss_ce_6: 1.969e-08  loss_mask_6: 0.03877  loss_dice_6: 0.7863  loss_ce_7: 1.193e-06  loss_mask_7: 0.04624  loss_dice_7: 0.8654  loss_ce_8: 2.305e-06  loss_mask_8: 0.03793  loss_dice_8: 0.7668  time: 0.4144  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:37 d2.utils.events]: [0m eta: 0:58:58  iter: 1479  total_loss: 8.709  loss_ce: 4.972e-05  loss_mask: 0.0421  loss_dice: 0.8261  loss_ce_0: 0.001484  loss_mask_0: 0.03767  loss_dice_0: 0.7825  loss_ce_1: 4.008e-07  loss_mask_1: 0.03764  loss_dice_1: 0.8744  loss_ce_2: 4.932e-08  loss_mask_2: 0.03741  loss_dice_2: 0.8164  loss_ce_3: 1.531e-08  loss_mask_3: 0.03922  loss_dice_3: 0.8298  loss_ce_4: 5.49e-07  loss_mask_4: 0.03822  loss_dice_4: 0.8367  loss_ce_5: 4.375e-09  loss_mask_5: 0.03786  loss_dice_5: 0.8244  loss_ce_6: 3.718e-08  loss_mask_6: 0.04121  loss_dice_6: 0.8286  loss_ce_7: 1.556e-06  loss_mask_7: 0.04308  loss_dice_7: 0.7979  loss_ce_8: 5.085e-06  loss_mask_8: 0.03502  loss_dice_8: 0.7847  time: 0.4144  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:45 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:55:45 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:55:45 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:55:45 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:55:45 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:55:45 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:55:46 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.275430 (0.275430 s / iter per device, on 1 devices)
[32m[06/02 13:55:46 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215707 s / iter per device, on 1 devices)
[32m[06/02 13:55:46 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:55:46 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:55:46 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:55:46 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:55:46 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:55:46 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:55:46 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:55:46 d2.utils.events]: [0m eta: 0:58:50  iter: 1499  total_loss: 8.864  loss_ce: 5.233e-05  loss_mask: 0.04213  loss_dice: 0.8946  loss_ce_0: 0.00143  loss_mask_0: 0.04099  loss_dice_0: 0.8057  loss_ce_1: 3.615e-07  loss_mask_1: 0.04229  loss_dice_1: 0.8503  loss_ce_2: 4.714e-08  loss_mask_2: 0.03874  loss_dice_2: 0.8509  loss_ce_3: 1.094e-08  loss_mask_3: 0.036  loss_dice_3: 0.7733  loss_ce_4: 3.139e-07  loss_mask_4: 0.0405  loss_dice_4: 0.8053  loss_ce_5: 2.187e-09  loss_mask_5: 0.03968  loss_dice_5: 0.8059  loss_ce_6: 2.297e-08  loss_mask_6: 0.0369  loss_dice_6: 0.8084  loss_ce_7: 8.574e-07  loss_mask_7: 0.03584  loss_dice_7: 0.7273  loss_ce_8: 1.386e-06  loss_mask_8: 0.03798  loss_dice_8: 0.8389  time: 0.4144  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:55:54 d2.utils.events]: [0m eta: 0:58:42  iter: 1519  total_loss: 8.219  loss_ce: 5.366e-06  loss_mask: 0.03748  loss_dice: 0.7388  loss_ce_0: 0.001369  loss_mask_0: 0.03447  loss_dice_0: 0.7138  loss_ce_1: 2.935e-07  loss_mask_1: 0.03604  loss_dice_1: 0.7586  loss_ce_2: 6.343e-08  loss_mask_2: 0.03866  loss_dice_2: 0.7757  loss_ce_3: 2.515e-08  loss_mask_3: 0.03821  loss_dice_3: 0.7717  loss_ce_4: 1.531e-07  loss_mask_4: 0.03851  loss_dice_4: 0.8219  loss_ce_5: 2.187e-09  loss_mask_5: 0.0354  loss_dice_5: 0.7351  loss_ce_6: 1.64e-08  loss_mask_6: 0.03549  loss_dice_6: 0.7492  loss_ce_7: 5.698e-07  loss_mask_7: 0.04059  loss_dice_7: 0.8277  loss_ce_8: 9.547e-07  loss_mask_8: 0.03739  loss_dice_8: 0.84  time: 0.4145  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:03 d2.utils.events]: [0m eta: 0:58:33  iter: 1539  total_loss: 7.813  loss_ce: 3.147e-05  loss_mask: 0.03987  loss_dice: 0.7426  loss_ce_0: 0.001311  loss_mask_0: 0.03661  loss_dice_0: 0.7383  loss_ce_1: 2.731e-07  loss_mask_1: 0.03937  loss_dice_1: 0.7225  loss_ce_2: 4.593e-08  loss_mask_2: 0.04037  loss_dice_2: 0.6888  loss_ce_3: 2.406e-08  loss_mask_3: 0.03791  loss_dice_3: 0.7408  loss_ce_4: 1.597e-07  loss_mask_4: 0.03833  loss_dice_4: 0.8317  loss_ce_5: 2.187e-09  loss_mask_5: 0.03508  loss_dice_5: 0.712  loss_ce_6: 1.75e-08  loss_mask_6: 0.04004  loss_dice_6: 0.722  loss_ce_7: 5.654e-07  loss_mask_7: 0.03992  loss_dice_7: 0.7329  loss_ce_8: 9.941e-07  loss_mask_8: 0.03498  loss_dice_8: 0.7058  time: 0.4146  data_time: 0.0069  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:07 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:56:07 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:56:07 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:56:07 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:56:07 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:56:07 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:56:08 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268107 (0.268107 s / iter per device, on 1 devices)
[32m[06/02 13:56:08 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.203987 s / iter per device, on 1 devices)
[32m[06/02 13:56:08 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:56:08 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:56:08 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:56:08 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:56:08 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:56:08 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:56:08 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:56:12 d2.utils.events]: [0m eta: 0:58:25  iter: 1559  total_loss: 7.789  loss_ce: 2.879e-06  loss_mask: 0.03694  loss_dice: 0.7362  loss_ce_0: 0.001261  loss_mask_0: 0.03568  loss_dice_0: 0.723  loss_ce_1: 2.891e-07  loss_mask_1: 0.03829  loss_dice_1: 0.7497  loss_ce_2: 3.522e-08  loss_mask_2: 0.03791  loss_dice_2: 0.7745  loss_ce_3: 2.406e-08  loss_mask_3: 0.03688  loss_dice_3: 0.7476  loss_ce_4: 4.801e-07  loss_mask_4: 0.03505  loss_dice_4: 0.7689  loss_ce_5: 2.187e-09  loss_mask_5: 0.03823  loss_dice_5: 0.7789  loss_ce_6: 1.094e-08  loss_mask_6: 0.03474  loss_dice_6: 0.7985  loss_ce_7: 3.543e-07  loss_mask_7: 0.03416  loss_dice_7: 0.7744  loss_ce_8: 2.286e-07  loss_mask_8: 0.03872  loss_dice_8: 0.736  time: 0.4146  data_time: 0.0085  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:20 d2.utils.events]: [0m eta: 0:58:16  iter: 1579  total_loss: 7.601  loss_ce: 7.211e-06  loss_mask: 0.03853  loss_dice: 0.7252  loss_ce_0: 0.001207  loss_mask_0: 0.03583  loss_dice_0: 0.7474  loss_ce_1: 2.731e-07  loss_mask_1: 0.03715  loss_dice_1: 0.7178  loss_ce_2: 6.015e-08  loss_mask_2: 0.03919  loss_dice_2: 0.6948  loss_ce_3: 1.969e-08  loss_mask_3: 0.03969  loss_dice_3: 0.7681  loss_ce_4: 8.716e-07  loss_mask_4: 0.03829  loss_dice_4: 0.7366  loss_ce_5: 2.187e-09  loss_mask_5: 0.03671  loss_dice_5: 0.6864  loss_ce_6: 1.094e-08  loss_mask_6: 0.03621  loss_dice_6: 0.7053  loss_ce_7: 1.196e-06  loss_mask_7: 0.03847  loss_dice_7: 0.736  loss_ce_8: 2.198e-07  loss_mask_8: 0.03587  loss_dice_8: 0.7327  time: 0.4146  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:29 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:56:29 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:56:29 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:56:29 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:56:29 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:56:29 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:56:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.243961 (0.243961 s / iter per device, on 1 devices)
[32m[06/02 13:56:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.185076 s / iter per device, on 1 devices)
[32m[06/02 13:56:29 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:56:29 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:56:29 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 13:56:29 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:56:29 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:56:29 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:56:29 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 13:56:29 d2.utils.events]: [0m eta: 0:58:08  iter: 1599  total_loss: 6.892  loss_ce: 8.031e-06  loss_mask: 0.03623  loss_dice: 0.6861  loss_ce_0: 0.001159  loss_mask_0: 0.03538  loss_dice_0: 0.6835  loss_ce_1: 2.864e-07  loss_mask_1: 0.03467  loss_dice_1: 0.631  loss_ce_2: 1.039e-07  loss_mask_2: 0.03841  loss_dice_2: 0.6862  loss_ce_3: 1.75e-08  loss_mask_3: 0.03739  loss_dice_3: 0.6633  loss_ce_4: 6.66e-07  loss_mask_4: 0.0343  loss_dice_4: 0.6811  loss_ce_5: 2.187e-09  loss_mask_5: 0.03329  loss_dice_5: 0.6541  loss_ce_6: 8.749e-09  loss_mask_6: 0.03415  loss_dice_6: 0.6773  loss_ce_7: 6.212e-07  loss_mask_7: 0.03369  loss_dice_7: 0.6493  loss_ce_8: 1.466e-07  loss_mask_8: 0.03291  loss_dice_8: 0.6486  time: 0.4147  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:38 d2.utils.events]: [0m eta: 0:58:01  iter: 1619  total_loss: 7.502  loss_ce: 5.322e-06  loss_mask: 0.03483  loss_dice: 0.6992  loss_ce_0: 0.001101  loss_mask_0: 0.0341  loss_dice_0: 0.7607  loss_ce_1: 3.203e-07  loss_mask_1: 0.03672  loss_dice_1: 0.7142  loss_ce_2: 4.342e-08  loss_mask_2: 0.03354  loss_dice_2: 0.7378  loss_ce_3: 1.75e-08  loss_mask_3: 0.03266  loss_dice_3: 0.6463  loss_ce_4: 4.67e-07  loss_mask_4: 0.03657  loss_dice_4: 0.6858  loss_ce_5: 2.187e-09  loss_mask_5: 0.03761  loss_dice_5: 0.7415  loss_ce_6: 1.312e-08  loss_mask_6: 0.03966  loss_dice_6: 0.7106  loss_ce_7: 4.539e-07  loss_mask_7: 0.04047  loss_dice_7: 0.6984  loss_ce_8: 1.345e-07  loss_mask_8: 0.03295  loss_dice_8: 0.704  time: 0.4147  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:46 d2.utils.events]: [0m eta: 0:57:52  iter: 1639  total_loss: 7.662  loss_ce: 6.735e-06  loss_mask: 0.03659  loss_dice: 0.7183  loss_ce_0: 0.001055  loss_mask_0: 0.03564  loss_dice_0: 0.7351  loss_ce_1: 3.183e-07  loss_mask_1: 0.03833  loss_dice_1: 0.7093  loss_ce_2: 3.751e-08  loss_mask_2: 0.03801  loss_dice_2: 0.7739  loss_ce_3: 1.969e-08  loss_mask_3: 0.03511  loss_dice_3: 0.7177  loss_ce_4: 5.392e-07  loss_mask_4: 0.03667  loss_dice_4: 0.7344  loss_ce_5: 4.375e-09  loss_mask_5: 0.0411  loss_dice_5: 0.7758  loss_ce_6: 1.094e-08  loss_mask_6: 0.0366  loss_dice_6: 0.7297  loss_ce_7: 1.673e-07  loss_mask_7: 0.03913  loss_dice_7: 0.7682  loss_ce_8: 1.739e-07  loss_mask_8: 0.03287  loss_dice_8: 0.6793  time: 0.4148  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:56:51 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:56:51 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:56:51 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:56:51 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:56:51 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:56:51 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:56:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.269784 (0.269784 s / iter per device, on 1 devices)
[32m[06/02 13:56:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208458 s / iter per device, on 1 devices)
[32m[06/02 13:56:51 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:56:51 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:56:51 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:56:51 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:56:51 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:56:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:56:51 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:56:55 d2.utils.events]: [0m eta: 0:57:44  iter: 1659  total_loss: 7.489  loss_ce: 3.176e-06  loss_mask: 0.03522  loss_dice: 0.693  loss_ce_0: 0.001012  loss_mask_0: 0.03528  loss_dice_0: 0.6806  loss_ce_1: 2.7e-07  loss_mask_1: 0.03713  loss_dice_1: 0.6961  loss_ce_2: 3.511e-08  loss_mask_2: 0.03582  loss_dice_2: 0.6892  loss_ce_3: 2.515e-08  loss_mask_3: 0.0376  loss_dice_3: 0.6347  loss_ce_4: 6.256e-07  loss_mask_4: 0.0354  loss_dice_4: 0.6613  loss_ce_5: 4.375e-09  loss_mask_5: 0.03619  loss_dice_5: 0.7001  loss_ce_6: 8.749e-09  loss_mask_6: 0.03826  loss_dice_6: 0.7583  loss_ce_7: 1.586e-07  loss_mask_7: 0.03634  loss_dice_7: 0.67  loss_ce_8: 1.214e-07  loss_mask_8: 0.03291  loss_dice_8: 0.6631  time: 0.4148  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:04 d2.utils.events]: [0m eta: 0:57:35  iter: 1679  total_loss: 7.209  loss_ce: 6.034e-06  loss_mask: 0.03933  loss_dice: 0.665  loss_ce_0: 0.0009663  loss_mask_0: 0.03783  loss_dice_0: 0.643  loss_ce_1: 2.296e-07  loss_mask_1: 0.03278  loss_dice_1: 0.6462  loss_ce_2: 4.484e-08  loss_mask_2: 0.03514  loss_dice_2: 0.6786  loss_ce_3: 3.5e-08  loss_mask_3: 0.03772  loss_dice_3: 0.6794  loss_ce_4: 1.053e-06  loss_mask_4: 0.03394  loss_dice_4: 0.6346  loss_ce_5: 4.375e-09  loss_mask_5: 0.03533  loss_dice_5: 0.6501  loss_ce_6: 1.969e-08  loss_mask_6: 0.04264  loss_dice_6: 0.7079  loss_ce_7: 3.106e-07  loss_mask_7: 0.03941  loss_dice_7: 0.686  loss_ce_8: 1.969e-07  loss_mask_8: 0.03773  loss_dice_8: 0.7304  time: 0.4148  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:12 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:57:12 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:57:12 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:57:12 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:57:12 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:57:12 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:57:13 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280162 (0.280162 s / iter per device, on 1 devices)
[32m[06/02 13:57:13 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215551 s / iter per device, on 1 devices)
[32m[06/02 13:57:13 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:57:13 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:57:13 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:57:13 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:57:13 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:57:13 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:57:13 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:57:13 d2.utils.events]: [0m eta: 0:57:27  iter: 1699  total_loss: 7.298  loss_ce: 6.385e-06  loss_mask: 0.0376  loss_dice: 0.8105  loss_ce_0: 0.0009269  loss_mask_0: 0.03685  loss_dice_0: 0.6857  loss_ce_1: 2.137e-07  loss_mask_1: 0.03392  loss_dice_1: 0.6379  loss_ce_2: 3.937e-08  loss_mask_2: 0.03334  loss_dice_2: 0.7483  loss_ce_3: 3.172e-08  loss_mask_3: 0.03452  loss_dice_3: 0.7042  loss_ce_4: 1.164e-06  loss_mask_4: 0.04136  loss_dice_4: 0.7649  loss_ce_5: 4.375e-09  loss_mask_5: 0.03693  loss_dice_5: 0.6715  loss_ce_6: 1.531e-08  loss_mask_6: 0.03869  loss_dice_6: 0.6523  loss_ce_7: 1.99e-07  loss_mask_7: 0.03643  loss_dice_7: 0.6725  loss_ce_8: 8.312e-08  loss_mask_8: 0.03509  loss_dice_8: 0.662  time: 0.4148  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:21 d2.utils.events]: [0m eta: 0:57:19  iter: 1719  total_loss: 7.476  loss_ce: 1.315e-05  loss_mask: 0.0364  loss_dice: 0.7215  loss_ce_0: 0.000891  loss_mask_0: 0.03714  loss_dice_0: 0.666  loss_ce_1: 1.971e-07  loss_mask_1: 0.03343  loss_dice_1: 0.713  loss_ce_2: 3.642e-08  loss_mask_2: 0.03552  loss_dice_2: 0.7372  loss_ce_3: 5.468e-08  loss_mask_3: 0.04153  loss_dice_3: 0.733  loss_ce_4: 1.203e-06  loss_mask_4: 0.03433  loss_dice_4: 0.6906  loss_ce_5: 4.484e-09  loss_mask_5: 0.0359  loss_dice_5: 0.6908  loss_ce_6: 8.749e-09  loss_mask_6: 0.03634  loss_dice_6: 0.7517  loss_ce_7: 1.105e-07  loss_mask_7: 0.03746  loss_dice_7: 0.6899  loss_ce_8: 9.941e-08  loss_mask_8: 0.03372  loss_dice_8: 0.7017  time: 0.4148  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:30 d2.utils.events]: [0m eta: 0:57:11  iter: 1739  total_loss: 7.049  loss_ce: 1.566e-05  loss_mask: 0.03333  loss_dice: 0.6914  loss_ce_0: 0.000855  loss_mask_0: 0.03387  loss_dice_0: 0.6213  loss_ce_1: 1.758e-07  loss_mask_1: 0.03651  loss_dice_1: 0.629  loss_ce_2: 3.281e-08  loss_mask_2: 0.03478  loss_dice_2: 0.6638  loss_ce_3: 5.031e-08  loss_mask_3: 0.03609  loss_dice_3: 0.7221  loss_ce_4: 1.403e-06  loss_mask_4: 0.03573  loss_dice_4: 0.6793  loss_ce_5: 4.375e-09  loss_mask_5: 0.03167  loss_dice_5: 0.6445  loss_ce_6: 6.562e-09  loss_mask_6: 0.0349  loss_dice_6: 0.6552  loss_ce_7: 2.767e-07  loss_mask_7: 0.03541  loss_dice_7: 0.6586  loss_ce_8: 2.253e-07  loss_mask_8: 0.03287  loss_dice_8: 0.6289  time: 0.4148  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:34 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:57:34 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:57:34 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:57:34 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:57:34 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:57:34 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:57:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.276853 (0.276853 s / iter per device, on 1 devices)
[32m[06/02 13:57:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212451 s / iter per device, on 1 devices)
[32m[06/02 13:57:34 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:57:34 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.917
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.917
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:57:34 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 91.683 | 100.000 | 100.000 | 91.683 |  nan  |  nan  | 90.000 | 95.000 |
[32m[06/02 13:57:34 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:57:34 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:57:34 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:57:34 d2.evaluation.testing]: [0mcopypaste: 91.6832,100.0000,100.0000,91.6832,nan,nan,90.0000,95.0000
[32m[06/02 13:57:39 d2.utils.events]: [0m eta: 0:57:02  iter: 1759  total_loss: 7.298  loss_ce: 1.631e-05  loss_mask: 0.03292  loss_dice: 0.7063  loss_ce_0: 0.0008224  loss_mask_0: 0.04104  loss_dice_0: 0.7092  loss_ce_1: 1.774e-07  loss_mask_1: 0.03436  loss_dice_1: 0.7066  loss_ce_2: 6.015e-08  loss_mask_2: 0.03718  loss_dice_2: 0.6409  loss_ce_3: 6.453e-08  loss_mask_3: 0.04261  loss_dice_3: 0.7343  loss_ce_4: 5.357e-06  loss_mask_4: 0.04141  loss_dice_4: 0.6968  loss_ce_5: 4.375e-09  loss_mask_5: 0.03539  loss_dice_5: 0.7171  loss_ce_6: 1.094e-08  loss_mask_6: 0.03775  loss_dice_6: 0.7281  loss_ce_7: 1.881e-07  loss_mask_7: 0.03498  loss_dice_7: 0.6206  loss_ce_8: 2.581e-07  loss_mask_8: 0.03562  loss_dice_8: 0.6445  time: 0.4148  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:47 d2.utils.events]: [0m eta: 0:56:54  iter: 1779  total_loss: 7.557  loss_ce: 1.291e-05  loss_mask: 0.03653  loss_dice: 0.6642  loss_ce_0: 0.0007901  loss_mask_0: 0.03642  loss_dice_0: 0.7014  loss_ce_1: 2.267e-07  loss_mask_1: 0.03463  loss_dice_1: 0.6716  loss_ce_2: 4.703e-08  loss_mask_2: 0.04066  loss_dice_2: 0.7372  loss_ce_3: 5.359e-08  loss_mask_3: 0.0379  loss_dice_3: 0.7718  loss_ce_4: 2.516e-06  loss_mask_4: 0.0403  loss_dice_4: 0.7427  loss_ce_5: 8.202e-09  loss_mask_5: 0.03553  loss_dice_5: 0.7162  loss_ce_6: 1.969e-08  loss_mask_6: 0.03729  loss_dice_6: 0.7014  loss_ce_7: 9.077e-08  loss_mask_7: 0.037  loss_dice_7: 0.743  loss_ce_8: 3.752e-07  loss_mask_8: 0.03758  loss_dice_8: 0.6696  time: 0.4148  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:57:55 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:57:55 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:57:55 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:57:55 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:57:55 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:57:55 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:57:56 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.286904 (0.286904 s / iter per device, on 1 devices)
[32m[06/02 13:57:56 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.227553 s / iter per device, on 1 devices)
[32m[06/02 13:57:56 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:57:56 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:57:56 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:57:56 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:57:56 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:57:56 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:57:56 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:57:56 d2.utils.events]: [0m eta: 0:56:45  iter: 1799  total_loss: 7.923  loss_ce: 8.081e-06  loss_mask: 0.03593  loss_dice: 0.723  loss_ce_0: 0.0007612  loss_mask_0: 0.03552  loss_dice_0: 0.763  loss_ce_1: 2.548e-07  loss_mask_1: 0.03285  loss_dice_1: 0.6868  loss_ce_2: 5.031e-08  loss_mask_2: 0.03886  loss_dice_2: 0.7787  loss_ce_3: 4.812e-08  loss_mask_3: 0.03514  loss_dice_3: 0.7521  loss_ce_4: 9.591e-07  loss_mask_4: 0.03588  loss_dice_4: 0.7689  loss_ce_5: 1.017e-08  loss_mask_5: 0.03828  loss_dice_5: 0.7128  loss_ce_6: 3.39e-08  loss_mask_6: 0.03943  loss_dice_6: 0.7666  loss_ce_7: 3.828e-08  loss_mask_7: 0.0333  loss_dice_7: 0.6893  loss_ce_8: 1.17e-07  loss_mask_8: 0.03858  loss_dice_8: 0.7789  time: 0.4148  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:04 d2.utils.events]: [0m eta: 0:56:37  iter: 1819  total_loss: 7.921  loss_ce: 6.685e-06  loss_mask: 0.03281  loss_dice: 0.7748  loss_ce_0: 0.0007364  loss_mask_0: 0.03418  loss_dice_0: 0.7146  loss_ce_1: 3.177e-07  loss_mask_1: 0.03678  loss_dice_1: 0.8183  loss_ce_2: 5.031e-08  loss_mask_2: 0.03528  loss_dice_2: 0.728  loss_ce_3: 1.214e-07  loss_mask_3: 0.03971  loss_dice_3: 0.7436  loss_ce_4: 1.061e-06  loss_mask_4: 0.03384  loss_dice_4: 0.7386  loss_ce_5: 9.406e-09  loss_mask_5: 0.0388  loss_dice_5: 0.7707  loss_ce_6: 1.017e-07  loss_mask_6: 0.03916  loss_dice_6: 0.7264  loss_ce_7: 5.468e-08  loss_mask_7: 0.03924  loss_dice_7: 0.7611  loss_ce_8: 4.921e-08  loss_mask_8: 0.03633  loss_dice_8: 0.7745  time: 0.4147  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:13 d2.utils.events]: [0m eta: 0:56:29  iter: 1839  total_loss: 7.285  loss_ce: 1.477e-05  loss_mask: 0.0355  loss_dice: 0.7228  loss_ce_0: 0.0007107  loss_mask_0: 0.03395  loss_dice_0: 0.6596  loss_ce_1: 3.001e-07  loss_mask_1: 0.03296  loss_dice_1: 0.6634  loss_ce_2: 5.031e-08  loss_mask_2: 0.03599  loss_dice_2: 0.6083  loss_ce_3: 1.128e-07  loss_mask_3: 0.03747  loss_dice_3: 0.7025  loss_ce_4: 1.074e-06  loss_mask_4: 0.03633  loss_dice_4: 0.662  loss_ce_5: 7.656e-09  loss_mask_5: 0.03269  loss_dice_5: 0.6998  loss_ce_6: 6.999e-08  loss_mask_6: 0.03698  loss_dice_6: 0.7075  loss_ce_7: 6.343e-08  loss_mask_7: 0.0358  loss_dice_7: 0.6613  loss_ce_8: 8.749e-08  loss_mask_8: 0.03781  loss_dice_8: 0.7125  time: 0.4148  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:17 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:58:17 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:58:17 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:58:17 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:58:17 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:58:17 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:58:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278284 (0.278284 s / iter per device, on 1 devices)
[32m[06/02 13:58:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.216389 s / iter per device, on 1 devices)
[32m[06/02 13:58:18 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:58:18 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:58:18 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 13:58:18 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:58:18 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:58:18 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:58:18 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 13:58:22 d2.utils.events]: [0m eta: 0:56:20  iter: 1859  total_loss: 7.716  loss_ce: 4.378e-06  loss_mask: 0.03272  loss_dice: 0.7392  loss_ce_0: 0.0006872  loss_mask_0: 0.03637  loss_dice_0: 0.7549  loss_ce_1: 2.776e-07  loss_mask_1: 0.03902  loss_dice_1: 0.6928  loss_ce_2: 4.068e-08  loss_mask_2: 0.03884  loss_dice_2: 0.7602  loss_ce_3: 8.771e-08  loss_mask_3: 0.04072  loss_dice_3: 0.6949  loss_ce_4: 1.018e-06  loss_mask_4: 0.03814  loss_dice_4: 0.7063  loss_ce_5: 6.562e-09  loss_mask_5: 0.03654  loss_dice_5: 0.7332  loss_ce_6: 5.25e-08  loss_mask_6: 0.03549  loss_dice_6: 0.7112  loss_ce_7: 6.234e-08  loss_mask_7: 0.04019  loss_dice_7: 0.7241  loss_ce_8: 3.172e-08  loss_mask_8: 0.03876  loss_dice_8: 0.717  time: 0.4148  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:30 d2.utils.events]: [0m eta: 0:56:12  iter: 1879  total_loss: 7.559  loss_ce: 6.858e-06  loss_mask: 0.03809  loss_dice: 0.7308  loss_ce_0: 0.0006635  loss_mask_0: 0.03273  loss_dice_0: 0.6905  loss_ce_1: 2.472e-07  loss_mask_1: 0.03579  loss_dice_1: 0.7088  loss_ce_2: 2.406e-08  loss_mask_2: 0.03978  loss_dice_2: 0.7761  loss_ce_3: 6.048e-08  loss_mask_3: 0.03618  loss_dice_3: 0.7711  loss_ce_4: 2.679e-07  loss_mask_4: 0.03195  loss_dice_4: 0.7415  loss_ce_5: 8.749e-09  loss_mask_5: 0.03668  loss_dice_5: 0.6988  loss_ce_6: 5.031e-08  loss_mask_6: 0.03652  loss_dice_6: 0.709  loss_ce_7: 3.062e-08  loss_mask_7: 0.03293  loss_dice_7: 0.7126  loss_ce_8: 2.515e-08  loss_mask_8: 0.03354  loss_dice_8: 0.7501  time: 0.4148  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:39 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:58:39 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:58:39 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:58:39 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:58:39 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:58:39 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:58:39 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.245653 (0.245653 s / iter per device, on 1 devices)
[32m[06/02 13:58:39 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.184779 s / iter per device, on 1 devices)
[32m[06/02 13:58:39 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:58:39 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:58:39 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:58:39 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:58:39 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:58:39 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:58:39 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:58:39 d2.utils.events]: [0m eta: 0:56:04  iter: 1899  total_loss: 8.299  loss_ce: 5.196e-06  loss_mask: 0.03757  loss_dice: 0.7694  loss_ce_0: 0.0006433  loss_mask_0: 0.04044  loss_dice_0: 0.7337  loss_ce_1: 2.659e-07  loss_mask_1: 0.04033  loss_dice_1: 0.7549  loss_ce_2: 2.526e-08  loss_mask_2: 0.03712  loss_dice_2: 0.7516  loss_ce_3: 6.234e-08  loss_mask_3: 0.03769  loss_dice_3: 0.6979  loss_ce_4: 8.968e-08  loss_mask_4: 0.03558  loss_dice_4: 0.7688  loss_ce_5: 3.281e-09  loss_mask_5: 0.03844  loss_dice_5: 0.8032  loss_ce_6: 2.406e-08  loss_mask_6: 0.03929  loss_dice_6: 0.7614  loss_ce_7: 4.812e-08  loss_mask_7: 0.03834  loss_dice_7: 0.7772  loss_ce_8: 3.718e-08  loss_mask_8: 0.04  loss_dice_8: 0.8299  time: 0.4148  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:48 d2.utils.events]: [0m eta: 0:55:56  iter: 1919  total_loss: 7.78  loss_ce: 4.536e-06  loss_mask: 0.03526  loss_dice: 0.7615  loss_ce_0: 0.0006256  loss_mask_0: 0.03428  loss_dice_0: 0.6834  loss_ce_1: 2.874e-07  loss_mask_1: 0.03693  loss_dice_1: 0.7682  loss_ce_2: 2.953e-08  loss_mask_2: 0.04136  loss_dice_2: 0.7637  loss_ce_3: 2.406e-08  loss_mask_3: 0.03622  loss_dice_3: 0.7184  loss_ce_4: 7.546e-08  loss_mask_4: 0.03669  loss_dice_4: 0.6858  loss_ce_5: 4.375e-09  loss_mask_5: 0.03315  loss_dice_5: 0.7101  loss_ce_6: 1.531e-08  loss_mask_6: 0.03377  loss_dice_6: 0.7127  loss_ce_7: 5.687e-08  loss_mask_7: 0.03628  loss_dice_7: 0.6821  loss_ce_8: 5.031e-08  loss_mask_8: 0.03731  loss_dice_8: 0.784  time: 0.4148  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:58:56 d2.utils.events]: [0m eta: 0:55:48  iter: 1939  total_loss: 7.898  loss_ce: 4.184e-06  loss_mask: 0.04316  loss_dice: 0.758  loss_ce_0: 0.0006067  loss_mask_0: 0.03888  loss_dice_0: 0.8117  loss_ce_1: 2.851e-07  loss_mask_1: 0.03827  loss_dice_1: 0.7291  loss_ce_2: 3.609e-08  loss_mask_2: 0.03624  loss_dice_2: 0.7095  loss_ce_3: 1.64e-08  loss_mask_3: 0.03531  loss_dice_3: 0.7194  loss_ce_4: 8.531e-08  loss_mask_4: 0.03679  loss_dice_4: 0.7567  loss_ce_5: 4.375e-09  loss_mask_5: 0.03374  loss_dice_5: 0.7436  loss_ce_6: 1.094e-08  loss_mask_6: 0.03583  loss_dice_6: 0.7049  loss_ce_7: 4.375e-08  loss_mask_7: 0.03313  loss_dice_7: 0.7065  loss_ce_8: 4.812e-08  loss_mask_8: 0.03965  loss_dice_8: 0.8114  time: 0.4148  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:59:00 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:59:00 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:59:00 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:59:00 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:59:00 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:59:00 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:59:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.283581 (0.283581 s / iter per device, on 1 devices)
[32m[06/02 13:59:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.223483 s / iter per device, on 1 devices)
[32m[06/02 13:59:01 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:59:01 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:59:01 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:59:01 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:59:01 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:59:01 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:59:01 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:59:05 d2.utils.events]: [0m eta: 0:55:39  iter: 1959  total_loss: 7.817  loss_ce: 8.076e-06  loss_mask: 0.03647  loss_dice: 0.6996  loss_ce_0: 0.0005916  loss_mask_0: 0.03482  loss_dice_0: 0.7019  loss_ce_1: 2.904e-07  loss_mask_1: 0.03495  loss_dice_1: 0.7158  loss_ce_2: 2.625e-08  loss_mask_2: 0.03698  loss_dice_2: 0.7248  loss_ce_3: 2.625e-08  loss_mask_3: 0.03497  loss_dice_3: 0.7324  loss_ce_4: 9.515e-08  loss_mask_4: 0.03767  loss_dice_4: 0.7217  loss_ce_5: 6.562e-09  loss_mask_5: 0.0369  loss_dice_5: 0.7168  loss_ce_6: 1.422e-08  loss_mask_6: 0.037  loss_dice_6: 0.7315  loss_ce_7: 7.218e-08  loss_mask_7: 0.03745  loss_dice_7: 0.7533  loss_ce_8: 5.687e-08  loss_mask_8: 0.03661  loss_dice_8: 0.7198  time: 0.4148  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:59:14 d2.utils.events]: [0m eta: 0:55:31  iter: 1979  total_loss: 7.621  loss_ce: 3.637e-06  loss_mask: 0.03986  loss_dice: 0.7627  loss_ce_0: 0.0005734  loss_mask_0: 0.03785  loss_dice_0: 0.7424  loss_ce_1: 3.691e-07  loss_mask_1: 0.03953  loss_dice_1: 0.7425  loss_ce_2: 3.281e-08  loss_mask_2: 0.03673  loss_dice_2: 0.7519  loss_ce_3: 4.484e-08  loss_mask_3: 0.03617  loss_dice_3: 0.6894  loss_ce_4: 9.624e-08  loss_mask_4: 0.03772  loss_dice_4: 0.7269  loss_ce_5: 6.562e-09  loss_mask_5: 0.03478  loss_dice_5: 0.7034  loss_ce_6: 1.094e-08  loss_mask_6: 0.03614  loss_dice_6: 0.6945  loss_ce_7: 7.218e-08  loss_mask_7: 0.03754  loss_dice_7: 0.7181  loss_ce_8: 8.421e-08  loss_mask_8: 0.03464  loss_dice_8: 0.7067  time: 0.4148  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:59:22 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0001999.pth
[32m[06/02 13:59:25 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:59:25 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:59:25 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:59:25 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:59:25 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:59:25 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:59:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266034 (0.266034 s / iter per device, on 1 devices)
[32m[06/02 13:59:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206001 s / iter per device, on 1 devices)
[32m[06/02 13:59:26 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:59:26 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:59:26 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:59:26 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:59:26 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:59:26 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:59:26 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:59:26 d2.utils.events]: [0m eta: 0:55:22  iter: 1999  total_loss: 7.561  loss_ce: 1.305e-06  loss_mask: 0.03427  loss_dice: 0.7445  loss_ce_0: 0.0005548  loss_mask_0: 0.03574  loss_dice_0: 0.6928  loss_ce_1: 2.019e-07  loss_mask_1: 0.03748  loss_dice_1: 0.7164  loss_ce_2: 1.75e-08  loss_mask_2: 0.03905  loss_dice_2: 0.7848  loss_ce_3: 3.062e-08  loss_mask_3: 0.04096  loss_dice_3: 0.708  loss_ce_4: 8.64e-08  loss_mask_4: 0.04417  loss_dice_4: 0.7249  loss_ce_5: 4.375e-09  loss_mask_5: 0.03559  loss_dice_5: 0.7061  loss_ce_6: 8.749e-09  loss_mask_6: 0.03796  loss_dice_6: 0.6397  loss_ce_7: 7.984e-08  loss_mask_7: 0.0339  loss_dice_7: 0.7069  loss_ce_8: 5.468e-08  loss_mask_8: 0.0366  loss_dice_8: 0.7452  time: 0.4148  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:59:34 d2.utils.events]: [0m eta: 0:55:14  iter: 2019  total_loss: 7.413  loss_ce: 3.131e-06  loss_mask: 0.03704  loss_dice: 0.6873  loss_ce_0: 0.0005393  loss_mask_0: 0.03723  loss_dice_0: 0.7158  loss_ce_1: 2.442e-07  loss_mask_1: 0.03646  loss_dice_1: 0.6941  loss_ce_2: 1.4e-08  loss_mask_2: 0.03283  loss_dice_2: 0.6895  loss_ce_3: 4.812e-08  loss_mask_3: 0.03227  loss_dice_3: 0.6706  loss_ce_4: 6.343e-08  loss_mask_4: 0.03274  loss_dice_4: 0.6752  loss_ce_5: 2.187e-09  loss_mask_5: 0.03425  loss_dice_5: 0.7131  loss_ce_6: 7.656e-09  loss_mask_6: 0.03786  loss_dice_6: 0.7566  loss_ce_7: 7.328e-08  loss_mask_7: 0.03652  loss_dice_7: 0.7088  loss_ce_8: 5.468e-08  loss_mask_8: 0.03688  loss_dice_8: 0.7019  time: 0.4148  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:59:42 d2.utils.events]: [0m eta: 0:55:05  iter: 2039  total_loss: 7.253  loss_ce: 3.401e-06  loss_mask: 0.0364  loss_dice: 0.6743  loss_ce_0: 0.0005223  loss_mask_0: 0.03309  loss_dice_0: 0.6594  loss_ce_1: 2.29e-07  loss_mask_1: 0.03388  loss_dice_1: 0.666  loss_ce_2: 1.159e-08  loss_mask_2: 0.03496  loss_dice_2: 0.6786  loss_ce_3: 6.234e-08  loss_mask_3: 0.03379  loss_dice_3: 0.6929  loss_ce_4: 5.14e-08  loss_mask_4: 0.03332  loss_dice_4: 0.6127  loss_ce_5: 2.187e-09  loss_mask_5: 0.03694  loss_dice_5: 0.7474  loss_ce_6: 3.281e-09  loss_mask_6: 0.03392  loss_dice_6: 0.701  loss_ce_7: 3.281e-08  loss_mask_7: 0.03392  loss_dice_7: 0.7126  loss_ce_8: 7.437e-08  loss_mask_8: 0.03652  loss_dice_8: 0.6961  time: 0.4148  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 13:59:46 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 13:59:46 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 13:59:46 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 13:59:46 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 13:59:46 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 13:59:46 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 13:59:47 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.260187 (0.260187 s / iter per device, on 1 devices)
[32m[06/02 13:59:47 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.195398 s / iter per device, on 1 devices)
[32m[06/02 13:59:47 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 13:59:47 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 13:59:47 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 13:59:47 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 13:59:47 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 13:59:47 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 13:59:47 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 13:59:51 d2.utils.events]: [0m eta: 0:54:57  iter: 2059  total_loss: 6.761  loss_ce: 4.861e-06  loss_mask: 0.0343  loss_dice: 0.636  loss_ce_0: 0.0005061  loss_mask_0: 0.03463  loss_dice_0: 0.6693  loss_ce_1: 2.219e-07  loss_mask_1: 0.03628  loss_dice_1: 0.6695  loss_ce_2: 1.258e-08  loss_mask_2: 0.03354  loss_dice_2: 0.6706  loss_ce_3: 5.096e-08  loss_mask_3: 0.0363  loss_dice_3: 0.6278  loss_ce_4: 3.937e-08  loss_mask_4: 0.03254  loss_dice_4: 0.6861  loss_ce_5: 0  loss_mask_5: 0.034  loss_dice_5: 0.6623  loss_ce_6: 2.187e-09  loss_mask_6: 0.03697  loss_dice_6: 0.6724  loss_ce_7: 2.515e-08  loss_mask_7: 0.03559  loss_dice_7: 0.6445  loss_ce_8: 1.498e-07  loss_mask_8: 0.03662  loss_dice_8: 0.5969  time: 0.4147  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:00 d2.utils.events]: [0m eta: 0:54:49  iter: 2079  total_loss: 7.72  loss_ce: 4.073e-06  loss_mask: 0.03443  loss_dice: 0.7237  loss_ce_0: 0.0004897  loss_mask_0: 0.03783  loss_dice_0: 0.7085  loss_ce_1: 2.448e-07  loss_mask_1: 0.03534  loss_dice_1: 0.7045  loss_ce_2: 1.312e-08  loss_mask_2: 0.03574  loss_dice_2: 0.6987  loss_ce_3: 4.375e-08  loss_mask_3: 0.03349  loss_dice_3: 0.7409  loss_ce_4: 4.047e-08  loss_mask_4: 0.03627  loss_dice_4: 0.6616  loss_ce_5: 0  loss_mask_5: 0.0407  loss_dice_5: 0.7325  loss_ce_6: 2.187e-09  loss_mask_6: 0.03696  loss_dice_6: 0.7257  loss_ce_7: 3.718e-08  loss_mask_7: 0.03578  loss_dice_7: 0.7929  loss_ce_8: 1.192e-07  loss_mask_8: 0.03856  loss_dice_8: 0.6999  time: 0.4148  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:08 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:00:08 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:00:08 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:00:08 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:00:08 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:00:08 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:00:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282363 (0.282363 s / iter per device, on 1 devices)
[32m[06/02 14:00:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217288 s / iter per device, on 1 devices)
[32m[06/02 14:00:09 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:00:09 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:00:09 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:00:09 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:00:09 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:00:09 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:00:09 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:00:09 d2.utils.events]: [0m eta: 0:54:41  iter: 2099  total_loss: 7.397  loss_ce: 5.171e-06  loss_mask: 0.03423  loss_dice: 0.6621  loss_ce_0: 0.0004743  loss_mask_0: 0.03683  loss_dice_0: 0.735  loss_ce_1: 3.08e-07  loss_mask_1: 0.03974  loss_dice_1: 0.693  loss_ce_2: 8.749e-09  loss_mask_2: 0.03486  loss_dice_2: 0.6658  loss_ce_3: 4.375e-08  loss_mask_3: 0.03577  loss_dice_3: 0.7315  loss_ce_4: 6.234e-08  loss_mask_4: 0.03464  loss_dice_4: 0.6657  loss_ce_5: 0  loss_mask_5: 0.03322  loss_dice_5: 0.6599  loss_ce_6: 4.375e-09  loss_mask_6: 0.03716  loss_dice_6: 0.7265  loss_ce_7: 3.937e-08  loss_mask_7: 0.03432  loss_dice_7: 0.7041  loss_ce_8: 8.531e-08  loss_mask_8: 0.03543  loss_dice_8: 0.7212  time: 0.4148  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:17 d2.utils.events]: [0m eta: 0:54:33  iter: 2119  total_loss: 7.669  loss_ce: 5.396e-06  loss_mask: 0.03633  loss_dice: 0.7322  loss_ce_0: 0.0004622  loss_mask_0: 0.03798  loss_dice_0: 0.7275  loss_ce_1: 3.01e-07  loss_mask_1: 0.03969  loss_dice_1: 0.7381  loss_ce_2: 1.094e-08  loss_mask_2: 0.03904  loss_dice_2: 0.793  loss_ce_3: 6.015e-08  loss_mask_3: 0.03668  loss_dice_3: 0.7027  loss_ce_4: 4.484e-08  loss_mask_4: 0.03536  loss_dice_4: 0.699  loss_ce_5: 0  loss_mask_5: 0.03742  loss_dice_5: 0.7137  loss_ce_6: 4.375e-09  loss_mask_6: 0.0384  loss_dice_6: 0.7354  loss_ce_7: 3.062e-08  loss_mask_7: 0.03821  loss_dice_7: 0.7189  loss_ce_8: 7.984e-08  loss_mask_8: 0.03458  loss_dice_8: 0.7151  time: 0.4148  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:26 d2.utils.events]: [0m eta: 0:54:24  iter: 2139  total_loss: 7.695  loss_ce: 6.565e-06  loss_mask: 0.03686  loss_dice: 0.7169  loss_ce_0: 0.0004485  loss_mask_0: 0.03575  loss_dice_0: 0.6555  loss_ce_1: 3.259e-07  loss_mask_1: 0.03601  loss_dice_1: 0.7012  loss_ce_2: 8.749e-09  loss_mask_2: 0.03371  loss_dice_2: 0.776  loss_ce_3: 5.359e-08  loss_mask_3: 0.03545  loss_dice_3: 0.7495  loss_ce_4: 2.515e-08  loss_mask_4: 0.03789  loss_dice_4: 0.7299  loss_ce_5: 0  loss_mask_5: 0.03742  loss_dice_5: 0.7207  loss_ce_6: 4.375e-09  loss_mask_6: 0.03481  loss_dice_6: 0.7335  loss_ce_7: 3.172e-08  loss_mask_7: 0.03384  loss_dice_7: 0.6713  loss_ce_8: 2.494e-07  loss_mask_8: 0.03708  loss_dice_8: 0.6844  time: 0.4148  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:30 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:00:30 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:00:30 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:00:30 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:00:30 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:00:30 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:00:31 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274035 (0.274035 s / iter per device, on 1 devices)
[32m[06/02 14:00:31 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215228 s / iter per device, on 1 devices)
[32m[06/02 14:00:31 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:00:31 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:00:31 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:00:31 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:00:31 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:00:31 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:00:31 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:00:35 d2.utils.events]: [0m eta: 0:54:16  iter: 2159  total_loss: 7.865  loss_ce: 1.719e-05  loss_mask: 0.04288  loss_dice: 0.8257  loss_ce_0: 0.0004365  loss_mask_0: 0.03919  loss_dice_0: 0.7508  loss_ce_1: 4.337e-07  loss_mask_1: 0.04056  loss_dice_1: 0.7388  loss_ce_2: 1.094e-08  loss_mask_2: 0.03842  loss_dice_2: 0.6629  loss_ce_3: 5.796e-08  loss_mask_3: 0.04147  loss_dice_3: 0.7481  loss_ce_4: 2.187e-08  loss_mask_4: 0.04133  loss_dice_4: 0.7665  loss_ce_5: 0  loss_mask_5: 0.0387  loss_dice_5: 0.7676  loss_ce_6: 6.562e-09  loss_mask_6: 0.03658  loss_dice_6: 0.739  loss_ce_7: 5.796e-08  loss_mask_7: 0.03967  loss_dice_7: 0.7349  loss_ce_8: 5.326e-07  loss_mask_8: 0.04109  loss_dice_8: 0.7566  time: 0.4148  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:43 d2.utils.events]: [0m eta: 0:54:08  iter: 2179  total_loss: 7.408  loss_ce: 4.356e-06  loss_mask: 0.03493  loss_dice: 0.7016  loss_ce_0: 0.0004266  loss_mask_0: 0.03469  loss_dice_0: 0.7295  loss_ce_1: 3.282e-07  loss_mask_1: 0.0383  loss_dice_1: 0.6917  loss_ce_2: 1.094e-08  loss_mask_2: 0.03353  loss_dice_2: 0.7134  loss_ce_3: 4.921e-08  loss_mask_3: 0.03996  loss_dice_3: 0.6613  loss_ce_4: 2.953e-08  loss_mask_4: 0.03551  loss_dice_4: 0.6856  loss_ce_5: 0  loss_mask_5: 0.03743  loss_dice_5: 0.7321  loss_ce_6: 4.375e-09  loss_mask_6: 0.03795  loss_dice_6: 0.7227  loss_ce_7: 3.281e-08  loss_mask_7: 0.03749  loss_dice_7: 0.7344  loss_ce_8: 1.192e-07  loss_mask_8: 0.03367  loss_dice_8: 0.7384  time: 0.4148  data_time: 0.0068  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:00:52 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:00:52 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:00:52 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:00:52 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:00:52 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:00:52 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:00:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.269654 (0.269654 s / iter per device, on 1 devices)
[32m[06/02 14:00:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212070 s / iter per device, on 1 devices)
[32m[06/02 14:00:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:00:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:00:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:00:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:00:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:00:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:00:52 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:00:52 d2.utils.events]: [0m eta: 0:54:00  iter: 2199  total_loss: 7.117  loss_ce: 2.115e-06  loss_mask: 0.03534  loss_dice: 0.6236  loss_ce_0: 0.0004146  loss_mask_0: 0.03754  loss_dice_0: 0.6891  loss_ce_1: 3.181e-07  loss_mask_1: 0.04003  loss_dice_1: 0.6804  loss_ce_2: 1.094e-08  loss_mask_2: 0.0323  loss_dice_2: 0.6673  loss_ce_3: 2.625e-08  loss_mask_3: 0.03125  loss_dice_3: 0.6966  loss_ce_4: 1.969e-08  loss_mask_4: 0.02949  loss_dice_4: 0.6367  loss_ce_5: 0  loss_mask_5: 0.03296  loss_dice_5: 0.7199  loss_ce_6: 2.187e-09  loss_mask_6: 0.034  loss_dice_6: 0.6126  loss_ce_7: 2.406e-08  loss_mask_7: 0.03373  loss_dice_7: 0.6169  loss_ce_8: 7.328e-08  loss_mask_8: 0.03307  loss_dice_8: 0.6779  time: 0.4148  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:01 d2.utils.events]: [0m eta: 0:53:52  iter: 2219  total_loss: 7.067  loss_ce: 2.49e-06  loss_mask: 0.03368  loss_dice: 0.6578  loss_ce_0: 0.0004038  loss_mask_0: 0.0352  loss_dice_0: 0.7247  loss_ce_1: 2.613e-07  loss_mask_1: 0.03706  loss_dice_1: 0.6467  loss_ce_2: 1.094e-08  loss_mask_2: 0.0352  loss_dice_2: 0.6563  loss_ce_3: 3.5e-08  loss_mask_3: 0.03439  loss_dice_3: 0.6752  loss_ce_4: 2.844e-08  loss_mask_4: 0.03603  loss_dice_4: 0.6625  loss_ce_5: 0  loss_mask_5: 0.0365  loss_dice_5: 0.6502  loss_ce_6: 2.187e-09  loss_mask_6: 0.03991  loss_dice_6: 0.6941  loss_ce_7: 2.625e-08  loss_mask_7: 0.0355  loss_dice_7: 0.6591  loss_ce_8: 5.25e-08  loss_mask_8: 0.03662  loss_dice_8: 0.6896  time: 0.4149  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:09 d2.utils.events]: [0m eta: 0:53:43  iter: 2239  total_loss: 7.289  loss_ce: 2.227e-06  loss_mask: 0.03618  loss_dice: 0.7551  loss_ce_0: 0.0003929  loss_mask_0: 0.03471  loss_dice_0: 0.6685  loss_ce_1: 3.04e-07  loss_mask_1: 0.03097  loss_dice_1: 0.6439  loss_ce_2: 8.749e-09  loss_mask_2: 0.0378  loss_dice_2: 0.748  loss_ce_3: 4.156e-08  loss_mask_3: 0.03323  loss_dice_3: 0.6554  loss_ce_4: 2.844e-08  loss_mask_4: 0.03563  loss_dice_4: 0.7001  loss_ce_5: 0  loss_mask_5: 0.03631  loss_dice_5: 0.6838  loss_ce_6: 2.187e-09  loss_mask_6: 0.03661  loss_dice_6: 0.7201  loss_ce_7: 3.718e-08  loss_mask_7: 0.03571  loss_dice_7: 0.7048  loss_ce_8: 5.468e-08  loss_mask_8: 0.03732  loss_dice_8: 0.7033  time: 0.4148  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:13 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:01:13 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:01:13 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:01:13 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:01:13 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:01:13 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:01:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271516 (0.271516 s / iter per device, on 1 devices)
[32m[06/02 14:01:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210463 s / iter per device, on 1 devices)
[32m[06/02 14:01:14 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:01:14 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:01:14 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:01:14 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:01:14 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:01:14 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:01:14 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:01:18 d2.utils.events]: [0m eta: 0:53:35  iter: 2259  total_loss: 7.262  loss_ce: 2.54e-06  loss_mask: 0.03489  loss_dice: 0.6423  loss_ce_0: 0.0003824  loss_mask_0: 0.03538  loss_dice_0: 0.6552  loss_ce_1: 2.875e-07  loss_mask_1: 0.03354  loss_dice_1: 0.681  loss_ce_2: 1.312e-08  loss_mask_2: 0.03252  loss_dice_2: 0.6727  loss_ce_3: 3.172e-08  loss_mask_3: 0.03306  loss_dice_3: 0.6763  loss_ce_4: 2.297e-08  loss_mask_4: 0.03635  loss_dice_4: 0.7473  loss_ce_5: 0  loss_mask_5: 0.0356  loss_dice_5: 0.6668  loss_ce_6: 2.187e-09  loss_mask_6: 0.03525  loss_dice_6: 0.7804  loss_ce_7: 4.156e-08  loss_mask_7: 0.03245  loss_dice_7: 0.6324  loss_ce_8: 4.047e-08  loss_mask_8: 0.03557  loss_dice_8: 0.7689  time: 0.4149  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:27 d2.utils.events]: [0m eta: 0:53:27  iter: 2279  total_loss: 7.56  loss_ce: 1.698e-06  loss_mask: 0.03441  loss_dice: 0.7385  loss_ce_0: 0.0003727  loss_mask_0: 0.03446  loss_dice_0: 0.7339  loss_ce_1: 2.484e-07  loss_mask_1: 0.03389  loss_dice_1: 0.7258  loss_ce_2: 1.094e-08  loss_mask_2: 0.03712  loss_dice_2: 0.7249  loss_ce_3: 2.625e-08  loss_mask_3: 0.03367  loss_dice_3: 0.727  loss_ce_4: 2.625e-08  loss_mask_4: 0.0347  loss_dice_4: 0.7247  loss_ce_5: 0  loss_mask_5: 0.03347  loss_dice_5: 0.7128  loss_ce_6: 2.187e-09  loss_mask_6: 0.03608  loss_dice_6: 0.7065  loss_ce_7: 3.5e-08  loss_mask_7: 0.03564  loss_dice_7: 0.7998  loss_ce_8: 2.078e-08  loss_mask_8: 0.03481  loss_dice_8: 0.6958  time: 0.4149  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:35 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:01:35 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:01:35 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:01:35 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:01:35 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:01:35 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:01:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278786 (0.278786 s / iter per device, on 1 devices)
[32m[06/02 14:01:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217215 s / iter per device, on 1 devices)
[32m[06/02 14:01:36 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:01:36 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:01:36 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:01:36 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:01:36 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:01:36 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:01:36 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:01:36 d2.utils.events]: [0m eta: 0:53:19  iter: 2299  total_loss: 7.722  loss_ce: 1.459e-06  loss_mask: 0.03607  loss_dice: 0.7321  loss_ce_0: 0.0003637  loss_mask_0: 0.03899  loss_dice_0: 0.7755  loss_ce_1: 2.099e-07  loss_mask_1: 0.03347  loss_dice_1: 0.6325  loss_ce_2: 1.094e-08  loss_mask_2: 0.0346  loss_dice_2: 0.74  loss_ce_3: 2.187e-08  loss_mask_3: 0.03952  loss_dice_3: 0.8116  loss_ce_4: 2.187e-08  loss_mask_4: 0.03934  loss_dice_4: 0.7357  loss_ce_5: 0  loss_mask_5: 0.04159  loss_dice_5: 0.7893  loss_ce_6: 4.375e-09  loss_mask_6: 0.03411  loss_dice_6: 0.7132  loss_ce_7: 2.844e-08  loss_mask_7: 0.03355  loss_dice_7: 0.7325  loss_ce_8: 2.515e-08  loss_mask_8: 0.03907  loss_dice_8: 0.7712  time: 0.4149  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:44 d2.utils.events]: [0m eta: 0:53:10  iter: 2319  total_loss: 7.226  loss_ce: 2.052e-06  loss_mask: 0.03656  loss_dice: 0.6361  loss_ce_0: 0.000355  loss_mask_0: 0.03155  loss_dice_0: 0.6946  loss_ce_1: 1.661e-07  loss_mask_1: 0.03346  loss_dice_1: 0.6743  loss_ce_2: 1.531e-08  loss_mask_2: 0.03738  loss_dice_2: 0.722  loss_ce_3: 2.406e-08  loss_mask_3: 0.03772  loss_dice_3: 0.6536  loss_ce_4: 1.75e-08  loss_mask_4: 0.03655  loss_dice_4: 0.7128  loss_ce_5: 0  loss_mask_5: 0.03671  loss_dice_5: 0.7332  loss_ce_6: 4.375e-09  loss_mask_6: 0.03676  loss_dice_6: 0.6763  loss_ce_7: 5.25e-08  loss_mask_7: 0.03642  loss_dice_7: 0.643  loss_ce_8: 3.281e-08  loss_mask_8: 0.03469  loss_dice_8: 0.7141  time: 0.4149  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:52 d2.utils.events]: [0m eta: 0:53:02  iter: 2339  total_loss: 7.459  loss_ce: 2.765e-06  loss_mask: 0.03382  loss_dice: 0.6998  loss_ce_0: 0.0003465  loss_mask_0: 0.03541  loss_dice_0: 0.6875  loss_ce_1: 1.55e-07  loss_mask_1: 0.03753  loss_dice_1: 0.6969  loss_ce_2: 1.312e-08  loss_mask_2: 0.03398  loss_dice_2: 0.7884  loss_ce_3: 3.172e-08  loss_mask_3: 0.04124  loss_dice_3: 0.704  loss_ce_4: 1.531e-08  loss_mask_4: 0.03708  loss_dice_4: 0.7714  loss_ce_5: 0  loss_mask_5: 0.03385  loss_dice_5: 0.7201  loss_ce_6: 4.375e-09  loss_mask_6: 0.03594  loss_dice_6: 0.6279  loss_ce_7: 7.109e-08  loss_mask_7: 0.03569  loss_dice_7: 0.6781  loss_ce_8: 5.578e-08  loss_mask_8: 0.03337  loss_dice_8: 0.7083  time: 0.4149  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:01:57 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:01:57 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:01:57 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:01:57 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:01:57 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:01:57 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:01:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277189 (0.277189 s / iter per device, on 1 devices)
[32m[06/02 14:01:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212371 s / iter per device, on 1 devices)
[32m[06/02 14:01:57 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:01:57 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:01:57 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:01:57 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:01:57 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:01:57 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:01:57 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:02:02 d2.utils.events]: [0m eta: 0:52:54  iter: 2359  total_loss: 7.122  loss_ce: 3.442e-06  loss_mask: 0.03319  loss_dice: 0.6439  loss_ce_0: 0.0003379  loss_mask_0: 0.03453  loss_dice_0: 0.6322  loss_ce_1: 1.773e-07  loss_mask_1: 0.03746  loss_dice_1: 0.6368  loss_ce_2: 1.312e-08  loss_mask_2: 0.03397  loss_dice_2: 0.6294  loss_ce_3: 2.844e-08  loss_mask_3: 0.0393  loss_dice_3: 0.6713  loss_ce_4: 1.64e-08  loss_mask_4: 0.03455  loss_dice_4: 0.7068  loss_ce_5: 0  loss_mask_5: 0.03357  loss_dice_5: 0.6348  loss_ce_6: 2.187e-09  loss_mask_6: 0.03713  loss_dice_6: 0.7169  loss_ce_7: 6.562e-08  loss_mask_7: 0.03712  loss_dice_7: 0.722  loss_ce_8: 6.781e-08  loss_mask_8: 0.03595  loss_dice_8: 0.6747  time: 0.4149  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:02:10 d2.utils.events]: [0m eta: 0:52:45  iter: 2379  total_loss: 6.806  loss_ce: 3.751e-06  loss_mask: 0.03426  loss_dice: 0.6536  loss_ce_0: 0.000329  loss_mask_0: 0.0361  loss_dice_0: 0.6752  loss_ce_1: 1.706e-07  loss_mask_1: 0.03231  loss_dice_1: 0.6401  loss_ce_2: 1.312e-08  loss_mask_2: 0.02977  loss_dice_2: 0.641  loss_ce_3: 2.406e-08  loss_mask_3: 0.03323  loss_dice_3: 0.6505  loss_ce_4: 2.406e-08  loss_mask_4: 0.04158  loss_dice_4: 0.7083  loss_ce_5: 0  loss_mask_5: 0.03184  loss_dice_5: 0.6174  loss_ce_6: 2.187e-09  loss_mask_6: 0.03094  loss_dice_6: 0.6091  loss_ce_7: 7.656e-08  loss_mask_7: 0.03476  loss_dice_7: 0.6646  loss_ce_8: 1.247e-07  loss_mask_8: 0.03386  loss_dice_8: 0.6741  time: 0.4149  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:02:18 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:02:18 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:02:18 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:02:18 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:02:18 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:02:18 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:02:19 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273778 (0.273778 s / iter per device, on 1 devices)
[32m[06/02 14:02:19 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213821 s / iter per device, on 1 devices)
[32m[06/02 14:02:19 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:02:19 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:02:19 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:02:19 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:02:19 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:02:19 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:02:19 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:02:19 d2.utils.events]: [0m eta: 0:52:37  iter: 2399  total_loss: 7.381  loss_ce: 3.221e-06  loss_mask: 0.03114  loss_dice: 0.6037  loss_ce_0: 0.0003199  loss_mask_0: 0.03112  loss_dice_0: 0.6202  loss_ce_1: 1.426e-07  loss_mask_1: 0.03232  loss_dice_1: 0.7164  loss_ce_2: 1.312e-08  loss_mask_2: 0.03712  loss_dice_2: 0.7238  loss_ce_3: 2.187e-08  loss_mask_3: 0.03413  loss_dice_3: 0.728  loss_ce_4: 1.75e-08  loss_mask_4: 0.03348  loss_dice_4: 0.6977  loss_ce_5: 0  loss_mask_5: 0.03579  loss_dice_5: 0.6946  loss_ce_6: 2.187e-09  loss_mask_6: 0.03456  loss_dice_6: 0.7003  loss_ce_7: 1.815e-07  loss_mask_7: 0.03782  loss_dice_7: 0.6826  loss_ce_8: 1.925e-07  loss_mask_8: 0.03523  loss_dice_8: 0.6952  time: 0.4149  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:02:28 d2.utils.events]: [0m eta: 0:52:29  iter: 2419  total_loss: 7.158  loss_ce: 2.879e-06  loss_mask: 0.03453  loss_dice: 0.6153  loss_ce_0: 0.0003107  loss_mask_0: 0.03533  loss_dice_0: 0.6584  loss_ce_1: 1.23e-07  loss_mask_1: 0.03738  loss_dice_1: 0.6745  loss_ce_2: 1.312e-08  loss_mask_2: 0.03747  loss_dice_2: 0.6414  loss_ce_3: 2.406e-08  loss_mask_3: 0.03842  loss_dice_3: 0.7072  loss_ce_4: 1.531e-08  loss_mask_4: 0.03474  loss_dice_4: 0.6485  loss_ce_5: 0  loss_mask_5: 0.03492  loss_dice_5: 0.673  loss_ce_6: 4.375e-09  loss_mask_6: 0.03326  loss_dice_6: 0.634  loss_ce_7: 3.489e-07  loss_mask_7: 0.03684  loss_dice_7: 0.6824  loss_ce_8: 1.761e-07  loss_mask_8: 0.03485  loss_dice_8: 0.6764  time: 0.4149  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:02:36 d2.utils.events]: [0m eta: 0:52:21  iter: 2439  total_loss: 7.312  loss_ce: 1.45e-06  loss_mask: 0.03589  loss_dice: 0.6947  loss_ce_0: 0.0003006  loss_mask_0: 0.03788  loss_dice_0: 0.7142  loss_ce_1: 1.108e-07  loss_mask_1: 0.03568  loss_dice_1: 0.653  loss_ce_2: 1.312e-08  loss_mask_2: 0.03505  loss_dice_2: 0.6801  loss_ce_3: 2.625e-08  loss_mask_3: 0.03149  loss_dice_3: 0.6623  loss_ce_4: 1.312e-08  loss_mask_4: 0.03608  loss_dice_4: 0.681  loss_ce_5: 0  loss_mask_5: 0.03348  loss_dice_5: 0.6566  loss_ce_6: 2.187e-09  loss_mask_6: 0.03544  loss_dice_6: 0.706  loss_ce_7: 1.936e-07  loss_mask_7: 0.03545  loss_dice_7: 0.6849  loss_ce_8: 6.453e-08  loss_mask_8: 0.03563  loss_dice_8: 0.7269  time: 0.4149  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:02:40 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:02:40 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:02:40 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:02:40 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:02:40 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:02:40 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:02:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.283185 (0.283185 s / iter per device, on 1 devices)
[32m[06/02 14:02:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.221626 s / iter per device, on 1 devices)
[32m[06/02 14:02:41 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:02:41 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:02:41 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:02:41 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:02:41 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:02:41 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:02:41 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:02:45 d2.utils.events]: [0m eta: 0:52:12  iter: 2459  total_loss: 7.589  loss_ce: 1.295e-06  loss_mask: 0.03567  loss_dice: 0.7262  loss_ce_0: 0.0002931  loss_mask_0: 0.04229  loss_dice_0: 0.7594  loss_ce_1: 1.128e-07  loss_mask_1: 0.03995  loss_dice_1: 0.7803  loss_ce_2: 1.312e-08  loss_mask_2: 0.0436  loss_dice_2: 0.7367  loss_ce_3: 2.625e-08  loss_mask_3: 0.03272  loss_dice_3: 0.7154  loss_ce_4: 1.312e-08  loss_mask_4: 0.033  loss_dice_4: 0.6516  loss_ce_5: 0  loss_mask_5: 0.03585  loss_dice_5: 0.672  loss_ce_6: 2.187e-09  loss_mask_6: 0.03571  loss_dice_6: 0.705  loss_ce_7: 2.953e-08  loss_mask_7: 0.04245  loss_dice_7: 0.7187  loss_ce_8: 2.625e-08  loss_mask_8: 0.03687  loss_dice_8: 0.7227  time: 0.4150  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:02:53 d2.utils.events]: [0m eta: 0:52:04  iter: 2479  total_loss: 6.836  loss_ce: 1.729e-06  loss_mask: 0.03414  loss_dice: 0.7235  loss_ce_0: 0.0002865  loss_mask_0: 0.03236  loss_dice_0: 0.6717  loss_ce_1: 1.144e-07  loss_mask_1: 0.03114  loss_dice_1: 0.6467  loss_ce_2: 1.094e-08  loss_mask_2: 0.03552  loss_dice_2: 0.6799  loss_ce_3: 2.844e-08  loss_mask_3: 0.03163  loss_dice_3: 0.6992  loss_ce_4: 1.75e-08  loss_mask_4: 0.03198  loss_dice_4: 0.6904  loss_ce_5: 0  loss_mask_5: 0.03574  loss_dice_5: 0.663  loss_ce_6: 2.187e-09  loss_mask_6: 0.03468  loss_dice_6: 0.6649  loss_ce_7: 1.094e-08  loss_mask_7: 0.02993  loss_dice_7: 0.6865  loss_ce_8: 3.281e-08  loss_mask_8: 0.0327  loss_dice_8: 0.631  time: 0.4150  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:02 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:03:02 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:03:02 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:03:02 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:03:02 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:03:02 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:03:03 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266584 (0.266584 s / iter per device, on 1 devices)
[32m[06/02 14:03:03 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.207511 s / iter per device, on 1 devices)
[32m[06/02 14:03:03 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:03:03 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:03:03 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:03:03 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:03:03 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:03:03 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:03:03 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:03:03 d2.utils.events]: [0m eta: 0:51:56  iter: 2499  total_loss: 6.889  loss_ce: 1.526e-06  loss_mask: 0.03226  loss_dice: 0.6334  loss_ce_0: 0.0002797  loss_mask_0: 0.03362  loss_dice_0: 0.6315  loss_ce_1: 1.079e-07  loss_mask_1: 0.03286  loss_dice_1: 0.6375  loss_ce_2: 8.749e-09  loss_mask_2: 0.03263  loss_dice_2: 0.6887  loss_ce_3: 3.172e-08  loss_mask_3: 0.03551  loss_dice_3: 0.6321  loss_ce_4: 2.625e-08  loss_mask_4: 0.03569  loss_dice_4: 0.6238  loss_ce_5: 0  loss_mask_5: 0.03536  loss_dice_5: 0.6825  loss_ce_6: 2.187e-09  loss_mask_6: 0.03293  loss_dice_6: 0.6938  loss_ce_7: 6.562e-09  loss_mask_7: 0.03231  loss_dice_7: 0.644  loss_ce_8: 3.718e-08  loss_mask_8: 0.03336  loss_dice_8: 0.6104  time: 0.4150  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:11 d2.utils.events]: [0m eta: 0:51:47  iter: 2519  total_loss: 6.929  loss_ce: 1.259e-06  loss_mask: 0.03765  loss_dice: 0.7469  loss_ce_0: 0.0002726  loss_mask_0: 0.03766  loss_dice_0: 0.6473  loss_ce_1: 1.101e-07  loss_mask_1: 0.03199  loss_dice_1: 0.6098  loss_ce_2: 8.749e-09  loss_mask_2: 0.03508  loss_dice_2: 0.6588  loss_ce_3: 3.062e-08  loss_mask_3: 0.03464  loss_dice_3: 0.6964  loss_ce_4: 3.281e-08  loss_mask_4: 0.03371  loss_dice_4: 0.6742  loss_ce_5: 0  loss_mask_5: 0.03393  loss_dice_5: 0.6186  loss_ce_6: 0  loss_mask_6: 0.0314  loss_dice_6: 0.6205  loss_ce_7: 6.562e-09  loss_mask_7: 0.03284  loss_dice_7: 0.6101  loss_ce_8: 3.609e-08  loss_mask_8: 0.03576  loss_dice_8: 0.6802  time: 0.4150  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:19 d2.utils.events]: [0m eta: 0:51:39  iter: 2539  total_loss: 7.107  loss_ce: 1.685e-06  loss_mask: 0.03263  loss_dice: 0.6608  loss_ce_0: 0.0002659  loss_mask_0: 0.03172  loss_dice_0: 0.6528  loss_ce_1: 1.119e-07  loss_mask_1: 0.03623  loss_dice_1: 0.6321  loss_ce_2: 8.749e-09  loss_mask_2: 0.03811  loss_dice_2: 0.649  loss_ce_3: 2.625e-08  loss_mask_3: 0.03372  loss_dice_3: 0.6769  loss_ce_4: 2.953e-08  loss_mask_4: 0.03403  loss_dice_4: 0.63  loss_ce_5: 0  loss_mask_5: 0.03074  loss_dice_5: 0.6299  loss_ce_6: 2.187e-09  loss_mask_6: 0.03669  loss_dice_6: 0.6475  loss_ce_7: 8.749e-09  loss_mask_7: 0.03699  loss_dice_7: 0.644  loss_ce_8: 4.156e-08  loss_mask_8: 0.03416  loss_dice_8: 0.69  time: 0.4150  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:23 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:03:23 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:03:23 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:03:23 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:03:23 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:03:23 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:03:24 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.272810 (0.272810 s / iter per device, on 1 devices)
[32m[06/02 14:03:24 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212574 s / iter per device, on 1 devices)
[32m[06/02 14:03:24 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:03:24 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.906
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.906
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:03:24 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.631 | 100.000 | 100.000 | 90.631 |  nan  |  nan  | 90.000 | 95.000 |
[32m[06/02 14:03:24 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:03:24 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:03:24 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:03:24 d2.evaluation.testing]: [0mcopypaste: 90.6312,100.0000,100.0000,90.6312,nan,nan,90.0000,95.0000
[32m[06/02 14:03:28 d2.utils.events]: [0m eta: 0:51:30  iter: 2559  total_loss: 6.697  loss_ce: 2.263e-06  loss_mask: 0.03811  loss_dice: 0.6208  loss_ce_0: 0.0002587  loss_mask_0: 0.03474  loss_dice_0: 0.5947  loss_ce_1: 1.164e-07  loss_mask_1: 0.03323  loss_dice_1: 0.6234  loss_ce_2: 7.656e-09  loss_mask_2: 0.03345  loss_dice_2: 0.6184  loss_ce_3: 2.844e-08  loss_mask_3: 0.03533  loss_dice_3: 0.6679  loss_ce_4: 2.297e-08  loss_mask_4: 0.03515  loss_dice_4: 0.6477  loss_ce_5: 0  loss_mask_5: 0.03169  loss_dice_5: 0.6354  loss_ce_6: 2.187e-09  loss_mask_6: 0.03649  loss_dice_6: 0.6239  loss_ce_7: 1.969e-08  loss_mask_7: 0.03881  loss_dice_7: 0.6343  loss_ce_8: 6.781e-08  loss_mask_8: 0.03431  loss_dice_8: 0.6157  time: 0.4149  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:37 d2.utils.events]: [0m eta: 0:51:22  iter: 2579  total_loss: 6.639  loss_ce: 1.986e-06  loss_mask: 0.03308  loss_dice: 0.583  loss_ce_0: 0.0002524  loss_mask_0: 0.03518  loss_dice_0: 0.5995  loss_ce_1: 1.191e-07  loss_mask_1: 0.03278  loss_dice_1: 0.6116  loss_ce_2: 8.749e-09  loss_mask_2: 0.03453  loss_dice_2: 0.6225  loss_ce_3: 2.844e-08  loss_mask_3: 0.03469  loss_dice_3: 0.6054  loss_ce_4: 1.531e-08  loss_mask_4: 0.03684  loss_dice_4: 0.6019  loss_ce_5: 0  loss_mask_5: 0.03332  loss_dice_5: 0.6191  loss_ce_6: 2.187e-09  loss_mask_6: 0.03316  loss_dice_6: 0.6033  loss_ce_7: 8.749e-09  loss_mask_7: 0.03249  loss_dice_7: 0.6132  loss_ce_8: 6.015e-08  loss_mask_8: 0.03599  loss_dice_8: 0.6243  time: 0.4149  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:45 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:03:45 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:03:45 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:03:45 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:03:45 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:03:45 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:03:46 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.279304 (0.279304 s / iter per device, on 1 devices)
[32m[06/02 14:03:46 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217738 s / iter per device, on 1 devices)
[32m[06/02 14:03:46 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:03:46 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:03:46 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:03:46 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:03:46 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:03:46 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:03:46 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:03:46 d2.utils.events]: [0m eta: 0:51:14  iter: 2599  total_loss: 7.015  loss_ce: 2.094e-06  loss_mask: 0.03729  loss_dice: 0.6465  loss_ce_0: 0.0002466  loss_mask_0: 0.03438  loss_dice_0: 0.619  loss_ce_1: 1.31e-07  loss_mask_1: 0.03657  loss_dice_1: 0.6489  loss_ce_2: 8.749e-09  loss_mask_2: 0.03483  loss_dice_2: 0.6576  loss_ce_3: 2.187e-08  loss_mask_3: 0.03555  loss_dice_3: 0.7484  loss_ce_4: 1.312e-08  loss_mask_4: 0.03425  loss_dice_4: 0.6797  loss_ce_5: 0  loss_mask_5: 0.03841  loss_dice_5: 0.6659  loss_ce_6: 0  loss_mask_6: 0.0376  loss_dice_6: 0.6637  loss_ce_7: 7.656e-09  loss_mask_7: 0.03496  loss_dice_7: 0.6965  loss_ce_8: 5.031e-08  loss_mask_8: 0.03488  loss_dice_8: 0.6189  time: 0.4149  data_time: 0.0082  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:03:54 d2.utils.events]: [0m eta: 0:51:06  iter: 2619  total_loss: 7.456  loss_ce: 1.989e-06  loss_mask: 0.038  loss_dice: 0.7248  loss_ce_0: 0.0002416  loss_mask_0: 0.03156  loss_dice_0: 0.6804  loss_ce_1: 1.432e-07  loss_mask_1: 0.03551  loss_dice_1: 0.6719  loss_ce_2: 6.562e-09  loss_mask_2: 0.03642  loss_dice_2: 0.6521  loss_ce_3: 2.297e-08  loss_mask_3: 0.03464  loss_dice_3: 0.7568  loss_ce_4: 1.312e-08  loss_mask_4: 0.03488  loss_dice_4: 0.6649  loss_ce_5: 0  loss_mask_5: 0.03658  loss_dice_5: 0.7324  loss_ce_6: 0  loss_mask_6: 0.03125  loss_dice_6: 0.718  loss_ce_7: 4.375e-09  loss_mask_7: 0.03208  loss_dice_7: 0.6815  loss_ce_8: 4.375e-08  loss_mask_8: 0.03773  loss_dice_8: 0.6654  time: 0.4150  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:03 d2.utils.events]: [0m eta: 0:50:58  iter: 2639  total_loss: 7.095  loss_ce: 2.203e-06  loss_mask: 0.03561  loss_dice: 0.6304  loss_ce_0: 0.0002361  loss_mask_0: 0.03779  loss_dice_0: 0.6588  loss_ce_1: 2.087e-07  loss_mask_1: 0.03516  loss_dice_1: 0.6647  loss_ce_2: 8.749e-09  loss_mask_2: 0.03682  loss_dice_2: 0.657  loss_ce_3: 2.406e-08  loss_mask_3: 0.0358  loss_dice_3: 0.6383  loss_ce_4: 1.203e-08  loss_mask_4: 0.03393  loss_dice_4: 0.614  loss_ce_5: 0  loss_mask_5: 0.03643  loss_dice_5: 0.6862  loss_ce_6: 0  loss_mask_6: 0.03882  loss_dice_6: 0.6606  loss_ce_7: 4.375e-09  loss_mask_7: 0.03572  loss_dice_7: 0.8088  loss_ce_8: 2.844e-08  loss_mask_8: 0.03714  loss_dice_8: 0.7022  time: 0.4150  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:07 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:04:07 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:04:07 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:04:07 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:04:07 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:04:07 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:04:07 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.258186 (0.258186 s / iter per device, on 1 devices)
[32m[06/02 14:04:07 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.198594 s / iter per device, on 1 devices)
[32m[06/02 14:04:07 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:04:07 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:04:08 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:04:08 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:04:08 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:04:08 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:04:08 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:04:12 d2.utils.events]: [0m eta: 0:50:49  iter: 2659  total_loss: 6.994  loss_ce: 1.42e-06  loss_mask: 0.03702  loss_dice: 0.6626  loss_ce_0: 0.0002306  loss_mask_0: 0.03509  loss_dice_0: 0.6506  loss_ce_1: 2.12e-07  loss_mask_1: 0.0342  loss_dice_1: 0.683  loss_ce_2: 8.749e-09  loss_mask_2: 0.03353  loss_dice_2: 0.6844  loss_ce_3: 2.406e-08  loss_mask_3: 0.0384  loss_dice_3: 0.6345  loss_ce_4: 8.749e-09  loss_mask_4: 0.03482  loss_dice_4: 0.7102  loss_ce_5: 0  loss_mask_5: 0.0331  loss_dice_5: 0.6284  loss_ce_6: 0  loss_mask_6: 0.0368  loss_dice_6: 0.6853  loss_ce_7: 4.375e-09  loss_mask_7: 0.03258  loss_dice_7: 0.6527  loss_ce_8: 2.844e-08  loss_mask_8: 0.03379  loss_dice_8: 0.669  time: 0.4150  data_time: 0.0087  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:20 d2.utils.events]: [0m eta: 0:50:41  iter: 2679  total_loss: 6.679  loss_ce: 1.492e-06  loss_mask: 0.03226  loss_dice: 0.6675  loss_ce_0: 0.0002257  loss_mask_0: 0.03437  loss_dice_0: 0.6489  loss_ce_1: 1.658e-07  loss_mask_1: 0.02828  loss_dice_1: 0.5532  loss_ce_2: 8.749e-09  loss_mask_2: 0.03573  loss_dice_2: 0.6556  loss_ce_3: 2.406e-08  loss_mask_3: 0.03209  loss_dice_3: 0.6419  loss_ce_4: 8.749e-09  loss_mask_4: 0.0356  loss_dice_4: 0.6626  loss_ce_5: 0  loss_mask_5: 0.03346  loss_dice_5: 0.6143  loss_ce_6: 0  loss_mask_6: 0.03296  loss_dice_6: 0.63  loss_ce_7: 4.375e-09  loss_mask_7: 0.03653  loss_dice_7: 0.6165  loss_ce_8: 3.281e-08  loss_mask_8: 0.03208  loss_dice_8: 0.61  time: 0.4150  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:28 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:04:28 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:04:28 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:04:28 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:04:28 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:04:28 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:04:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277352 (0.277352 s / iter per device, on 1 devices)
[32m[06/02 14:04:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212671 s / iter per device, on 1 devices)
[32m[06/02 14:04:29 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:04:29 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:04:29 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:04:29 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:04:29 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:04:29 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:04:29 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:04:29 d2.utils.events]: [0m eta: 0:50:33  iter: 2699  total_loss: 6.978  loss_ce: 1.107e-06  loss_mask: 0.03798  loss_dice: 0.6618  loss_ce_0: 0.0002206  loss_mask_0: 0.03447  loss_dice_0: 0.664  loss_ce_1: 1.317e-07  loss_mask_1: 0.03312  loss_dice_1: 0.6052  loss_ce_2: 6.562e-09  loss_mask_2: 0.03355  loss_dice_2: 0.6555  loss_ce_3: 1.531e-08  loss_mask_3: 0.03383  loss_dice_3: 0.6984  loss_ce_4: 7.656e-09  loss_mask_4: 0.03109  loss_dice_4: 0.6178  loss_ce_5: 0  loss_mask_5: 0.03367  loss_dice_5: 0.6458  loss_ce_6: 0  loss_mask_6: 0.03458  loss_dice_6: 0.66  loss_ce_7: 4.375e-09  loss_mask_7: 0.03282  loss_dice_7: 0.6584  loss_ce_8: 2.844e-08  loss_mask_8: 0.03427  loss_dice_8: 0.671  time: 0.4150  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:37 d2.utils.events]: [0m eta: 0:50:24  iter: 2719  total_loss: 7.18  loss_ce: 8.959e-07  loss_mask: 0.03938  loss_dice: 0.6881  loss_ce_0: 0.0002157  loss_mask_0: 0.03609  loss_dice_0: 0.6856  loss_ce_1: 1.382e-07  loss_mask_1: 0.03085  loss_dice_1: 0.6585  loss_ce_2: 6.562e-09  loss_mask_2: 0.0317  loss_dice_2: 0.6649  loss_ce_3: 8.749e-09  loss_mask_3: 0.03157  loss_dice_3: 0.6946  loss_ce_4: 4.375e-09  loss_mask_4: 0.03613  loss_dice_4: 0.7264  loss_ce_5: 0  loss_mask_5: 0.03878  loss_dice_5: 0.778  loss_ce_6: 0  loss_mask_6: 0.03432  loss_dice_6: 0.7209  loss_ce_7: 4.375e-09  loss_mask_7: 0.03401  loss_dice_7: 0.6885  loss_ce_8: 1.75e-08  loss_mask_8: 0.03661  loss_dice_8: 0.6904  time: 0.4150  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:46 d2.utils.events]: [0m eta: 0:50:16  iter: 2739  total_loss: 6.838  loss_ce: 1.164e-06  loss_mask: 0.0372  loss_dice: 0.6379  loss_ce_0: 0.0002122  loss_mask_0: 0.03196  loss_dice_0: 0.6114  loss_ce_1: 1.192e-07  loss_mask_1: 0.03617  loss_dice_1: 0.6639  loss_ce_2: 4.375e-09  loss_mask_2: 0.03583  loss_dice_2: 0.6399  loss_ce_3: 1.094e-08  loss_mask_3: 0.03328  loss_dice_3: 0.6031  loss_ce_4: 2.187e-09  loss_mask_4: 0.0352  loss_dice_4: 0.6334  loss_ce_5: 0  loss_mask_5: 0.03331  loss_dice_5: 0.7143  loss_ce_6: 0  loss_mask_6: 0.03538  loss_dice_6: 0.6511  loss_ce_7: 4.375e-09  loss_mask_7: 0.03373  loss_dice_7: 0.6842  loss_ce_8: 1.969e-08  loss_mask_8: 0.03615  loss_dice_8: 0.6637  time: 0.4150  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:04:50 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:04:50 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:04:50 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:04:50 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:04:50 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:04:50 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:04:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266531 (0.266531 s / iter per device, on 1 devices)
[32m[06/02 14:04:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206677 s / iter per device, on 1 devices)
[32m[06/02 14:04:51 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:04:51 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:04:51 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:04:51 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:04:51 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:04:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:04:51 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:04:55 d2.utils.events]: [0m eta: 0:50:08  iter: 2759  total_loss: 7.139  loss_ce: 5.52e-07  loss_mask: 0.03728  loss_dice: 0.6574  loss_ce_0: 0.0002073  loss_mask_0: 0.03634  loss_dice_0: 0.6428  loss_ce_1: 1.327e-07  loss_mask_1: 0.03504  loss_dice_1: 0.6884  loss_ce_2: 4.375e-09  loss_mask_2: 0.03659  loss_dice_2: 0.7441  loss_ce_3: 1.094e-08  loss_mask_3: 0.03653  loss_dice_3: 0.6831  loss_ce_4: 2.187e-09  loss_mask_4: 0.03207  loss_dice_4: 0.7051  loss_ce_5: 0  loss_mask_5: 0.03268  loss_dice_5: 0.6868  loss_ce_6: 0  loss_mask_6: 0.03443  loss_dice_6: 0.7028  loss_ce_7: 4.375e-09  loss_mask_7: 0.03527  loss_dice_7: 0.6606  loss_ce_8: 1.312e-08  loss_mask_8: 0.03186  loss_dice_8: 0.6552  time: 0.4150  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:03 d2.utils.events]: [0m eta: 0:50:00  iter: 2779  total_loss: 6.632  loss_ce: 4.976e-07  loss_mask: 0.03612  loss_dice: 0.708  loss_ce_0: 0.0002032  loss_mask_0: 0.03502  loss_dice_0: 0.6268  loss_ce_1: 1.826e-07  loss_mask_1: 0.03611  loss_dice_1: 0.6222  loss_ce_2: 6.562e-09  loss_mask_2: 0.03348  loss_dice_2: 0.6231  loss_ce_3: 1.094e-08  loss_mask_3: 0.03649  loss_dice_3: 0.6121  loss_ce_4: 4.375e-09  loss_mask_4: 0.03667  loss_dice_4: 0.6566  loss_ce_5: 0  loss_mask_5: 0.03665  loss_dice_5: 0.6034  loss_ce_6: 0  loss_mask_6: 0.03369  loss_dice_6: 0.5658  loss_ce_7: 6.562e-09  loss_mask_7: 0.03721  loss_dice_7: 0.6866  loss_ce_8: 1.531e-08  loss_mask_8: 0.03637  loss_dice_8: 0.6489  time: 0.4149  data_time: 0.0064  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:12 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:05:12 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:05:12 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:05:12 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:05:12 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:05:12 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:05:12 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.270465 (0.270465 s / iter per device, on 1 devices)
[32m[06/02 14:05:12 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.211930 s / iter per device, on 1 devices)
[32m[06/02 14:05:12 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:05:12 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:05:12 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:05:12 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:05:12 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:05:12 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:05:12 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:05:12 d2.utils.events]: [0m eta: 0:49:51  iter: 2799  total_loss: 6.675  loss_ce: 6.03e-07  loss_mask: 0.03477  loss_dice: 0.6524  loss_ce_0: 0.0001988  loss_mask_0: 0.03509  loss_dice_0: 0.6157  loss_ce_1: 1.85e-07  loss_mask_1: 0.03765  loss_dice_1: 0.6584  loss_ce_2: 6.562e-09  loss_mask_2: 0.03411  loss_dice_2: 0.6289  loss_ce_3: 1.094e-08  loss_mask_3: 0.03298  loss_dice_3: 0.5881  loss_ce_4: 4.375e-09  loss_mask_4: 0.03419  loss_dice_4: 0.6222  loss_ce_5: 0  loss_mask_5: 0.03181  loss_dice_5: 0.582  loss_ce_6: 0  loss_mask_6: 0.03382  loss_dice_6: 0.6144  loss_ce_7: 6.562e-09  loss_mask_7: 0.03567  loss_dice_7: 0.5718  loss_ce_8: 1.094e-08  loss_mask_8: 0.03803  loss_dice_8: 0.6026  time: 0.4149  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:21 d2.utils.events]: [0m eta: 0:49:43  iter: 2819  total_loss: 6.924  loss_ce: 5.729e-07  loss_mask: 0.03404  loss_dice: 0.6433  loss_ce_0: 0.0001944  loss_mask_0: 0.03235  loss_dice_0: 0.6644  loss_ce_1: 1.927e-07  loss_mask_1: 0.03678  loss_dice_1: 0.607  loss_ce_2: 6.562e-09  loss_mask_2: 0.03671  loss_dice_2: 0.6588  loss_ce_3: 1.094e-08  loss_mask_3: 0.03682  loss_dice_3: 0.6238  loss_ce_4: 4.375e-09  loss_mask_4: 0.0339  loss_dice_4: 0.5918  loss_ce_5: 0  loss_mask_5: 0.03768  loss_dice_5: 0.6675  loss_ce_6: 0  loss_mask_6: 0.03503  loss_dice_6: 0.667  loss_ce_7: 6.562e-09  loss_mask_7: 0.03866  loss_dice_7: 0.7042  loss_ce_8: 1.312e-08  loss_mask_8: 0.03248  loss_dice_8: 0.611  time: 0.4149  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:29 d2.utils.events]: [0m eta: 0:49:35  iter: 2839  total_loss: 6.917  loss_ce: 6.369e-07  loss_mask: 0.03636  loss_dice: 0.6444  loss_ce_0: 0.0001898  loss_mask_0: 0.03399  loss_dice_0: 0.6435  loss_ce_1: 2.01e-07  loss_mask_1: 0.03339  loss_dice_1: 0.7098  loss_ce_2: 6.562e-09  loss_mask_2: 0.03535  loss_dice_2: 0.6759  loss_ce_3: 1.094e-08  loss_mask_3: 0.03588  loss_dice_3: 0.6246  loss_ce_4: 4.375e-09  loss_mask_4: 0.03528  loss_dice_4: 0.6254  loss_ce_5: 0  loss_mask_5: 0.0336  loss_dice_5: 0.6453  loss_ce_6: 0  loss_mask_6: 0.0343  loss_dice_6: 0.6775  loss_ce_7: 6.562e-09  loss_mask_7: 0.03746  loss_dice_7: 0.7348  loss_ce_8: 1.094e-08  loss_mask_8: 0.03421  loss_dice_8: 0.6623  time: 0.4149  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:33 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:05:33 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:05:33 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:05:33 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:05:33 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:05:33 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:05:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274232 (0.274232 s / iter per device, on 1 devices)
[32m[06/02 14:05:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210997 s / iter per device, on 1 devices)
[32m[06/02 14:05:34 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:05:34 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:05:34 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:05:34 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:05:34 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:05:34 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:05:34 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:05:38 d2.utils.events]: [0m eta: 0:49:26  iter: 2859  total_loss: 6.473  loss_ce: 7.845e-07  loss_mask: 0.03403  loss_dice: 0.6405  loss_ce_0: 0.0001865  loss_mask_0: 0.03289  loss_dice_0: 0.5851  loss_ce_1: 2.002e-07  loss_mask_1: 0.03589  loss_dice_1: 0.621  loss_ce_2: 6.562e-09  loss_mask_2: 0.03608  loss_dice_2: 0.617  loss_ce_3: 1.094e-08  loss_mask_3: 0.03474  loss_dice_3: 0.6247  loss_ce_4: 4.375e-09  loss_mask_4: 0.03345  loss_dice_4: 0.5991  loss_ce_5: 0  loss_mask_5: 0.034  loss_dice_5: 0.5646  loss_ce_6: 0  loss_mask_6: 0.03612  loss_dice_6: 0.6794  loss_ce_7: 6.562e-09  loss_mask_7: 0.0354  loss_dice_7: 0.6494  loss_ce_8: 2.187e-08  loss_mask_8: 0.03477  loss_dice_8: 0.6076  time: 0.4149  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:46 d2.utils.events]: [0m eta: 0:49:18  iter: 2879  total_loss: 6.719  loss_ce: 1.145e-06  loss_mask: 0.03349  loss_dice: 0.6435  loss_ce_0: 0.0001827  loss_mask_0: 0.0383  loss_dice_0: 0.6264  loss_ce_1: 2.063e-07  loss_mask_1: 0.03044  loss_dice_1: 0.6225  loss_ce_2: 6.562e-09  loss_mask_2: 0.0344  loss_dice_2: 0.6286  loss_ce_3: 8.749e-09  loss_mask_3: 0.03339  loss_dice_3: 0.636  loss_ce_4: 4.375e-09  loss_mask_4: 0.03524  loss_dice_4: 0.6655  loss_ce_5: 0  loss_mask_5: 0.03751  loss_dice_5: 0.7308  loss_ce_6: 0  loss_mask_6: 0.03477  loss_dice_6: 0.6499  loss_ce_7: 6.562e-09  loss_mask_7: 0.03165  loss_dice_7: 0.5979  loss_ce_8: 3.609e-08  loss_mask_8: 0.03679  loss_dice_8: 0.6662  time: 0.4149  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:05:55 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:05:55 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:05:55 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:05:55 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:05:55 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:05:55 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:05:56 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277361 (0.277361 s / iter per device, on 1 devices)
[32m[06/02 14:05:56 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212971 s / iter per device, on 1 devices)
[32m[06/02 14:05:56 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:05:56 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:05:56 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:05:56 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:05:56 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:05:56 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:05:56 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:05:56 d2.utils.events]: [0m eta: 0:49:10  iter: 2899  total_loss: 6.538  loss_ce: 1.005e-06  loss_mask: 0.03565  loss_dice: 0.583  loss_ce_0: 0.0001792  loss_mask_0: 0.03474  loss_dice_0: 0.6502  loss_ce_1: 1.813e-07  loss_mask_1: 0.03335  loss_dice_1: 0.6599  loss_ce_2: 4.375e-09  loss_mask_2: 0.03103  loss_dice_2: 0.5964  loss_ce_3: 6.562e-09  loss_mask_3: 0.03536  loss_dice_3: 0.6449  loss_ce_4: 4.375e-09  loss_mask_4: 0.03341  loss_dice_4: 0.5801  loss_ce_5: 0  loss_mask_5: 0.03244  loss_dice_5: 0.6077  loss_ce_6: 0  loss_mask_6: 0.03442  loss_dice_6: 0.5527  loss_ce_7: 6.562e-09  loss_mask_7: 0.0363  loss_dice_7: 0.6242  loss_ce_8: 3.5e-08  loss_mask_8: 0.03733  loss_dice_8: 0.6199  time: 0.4149  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:04 d2.utils.events]: [0m eta: 0:49:01  iter: 2919  total_loss: 7.124  loss_ce: 8.752e-07  loss_mask: 0.0366  loss_dice: 0.6883  loss_ce_0: 0.0001755  loss_mask_0: 0.03597  loss_dice_0: 0.7291  loss_ce_1: 1.969e-07  loss_mask_1: 0.034  loss_dice_1: 0.6794  loss_ce_2: 6.562e-09  loss_mask_2: 0.03833  loss_dice_2: 0.6545  loss_ce_3: 4.375e-09  loss_mask_3: 0.03393  loss_dice_3: 0.6803  loss_ce_4: 4.375e-09  loss_mask_4: 0.03287  loss_dice_4: 0.6662  loss_ce_5: 0  loss_mask_5: 0.03496  loss_dice_5: 0.6605  loss_ce_6: 0  loss_mask_6: 0.03488  loss_dice_6: 0.661  loss_ce_7: 6.562e-09  loss_mask_7: 0.03933  loss_dice_7: 0.6439  loss_ce_8: 2.406e-08  loss_mask_8: 0.03715  loss_dice_8: 0.6722  time: 0.4149  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:12 d2.utils.events]: [0m eta: 0:48:53  iter: 2939  total_loss: 6.869  loss_ce: 1.787e-06  loss_mask: 0.03503  loss_dice: 0.6626  loss_ce_0: 0.000172  loss_mask_0: 0.02867  loss_dice_0: 0.6138  loss_ce_1: 1.88e-07  loss_mask_1: 0.03668  loss_dice_1: 0.6076  loss_ce_2: 4.375e-09  loss_mask_2: 0.036  loss_dice_2: 0.6495  loss_ce_3: 4.375e-09  loss_mask_3: 0.03994  loss_dice_3: 0.7068  loss_ce_4: 4.375e-09  loss_mask_4: 0.03459  loss_dice_4: 0.6862  loss_ce_5: 0  loss_mask_5: 0.03509  loss_dice_5: 0.6344  loss_ce_6: 0  loss_mask_6: 0.03609  loss_dice_6: 0.6211  loss_ce_7: 6.562e-09  loss_mask_7: 0.034  loss_dice_7: 0.6012  loss_ce_8: 2.953e-08  loss_mask_8: 0.03947  loss_dice_8: 0.6384  time: 0.4149  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:16 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:06:16 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:06:16 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:06:16 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:06:16 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:06:16 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:06:17 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.262495 (0.262495 s / iter per device, on 1 devices)
[32m[06/02 14:06:17 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.198591 s / iter per device, on 1 devices)
[32m[06/02 14:06:17 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:06:17 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:06:17 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:06:17 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:06:17 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:06:17 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:06:17 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:06:21 d2.utils.events]: [0m eta: 0:48:45  iter: 2959  total_loss: 6.82  loss_ce: 3.042e-06  loss_mask: 0.0356  loss_dice: 0.6507  loss_ce_0: 0.0001682  loss_mask_0: 0.0327  loss_dice_0: 0.6328  loss_ce_1: 1.872e-07  loss_mask_1: 0.03577  loss_dice_1: 0.6325  loss_ce_2: 4.375e-09  loss_mask_2: 0.0336  loss_dice_2: 0.5987  loss_ce_3: 4.375e-09  loss_mask_3: 0.03226  loss_dice_3: 0.6556  loss_ce_4: 6.562e-09  loss_mask_4: 0.03517  loss_dice_4: 0.6288  loss_ce_5: 0  loss_mask_5: 0.03132  loss_dice_5: 0.6409  loss_ce_6: 0  loss_mask_6: 0.03557  loss_dice_6: 0.6711  loss_ce_7: 8.749e-09  loss_mask_7: 0.03428  loss_dice_7: 0.6365  loss_ce_8: 2.625e-08  loss_mask_8: 0.03483  loss_dice_8: 0.6521  time: 0.4149  data_time: 0.0070  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:30 d2.utils.events]: [0m eta: 0:48:36  iter: 2979  total_loss: 6.471  loss_ce: 2.408e-06  loss_mask: 0.03733  loss_dice: 0.6437  loss_ce_0: 0.0001652  loss_mask_0: 0.03579  loss_dice_0: 0.5963  loss_ce_1: 1.647e-07  loss_mask_1: 0.03213  loss_dice_1: 0.6422  loss_ce_2: 4.375e-09  loss_mask_2: 0.0311  loss_dice_2: 0.6075  loss_ce_3: 4.375e-09  loss_mask_3: 0.03606  loss_dice_3: 0.5739  loss_ce_4: 8.749e-09  loss_mask_4: 0.03315  loss_dice_4: 0.5849  loss_ce_5: 0  loss_mask_5: 0.03851  loss_dice_5: 0.6539  loss_ce_6: 0  loss_mask_6: 0.03616  loss_dice_6: 0.581  loss_ce_7: 6.562e-09  loss_mask_7: 0.03416  loss_dice_7: 0.6173  loss_ce_8: 2.187e-08  loss_mask_8: 0.03323  loss_dice_8: 0.6191  time: 0.4149  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:38 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0002999.pth
[32m[06/02 14:06:41 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:06:41 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:06:41 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:06:41 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:06:41 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:06:41 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:06:42 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.275684 (0.275684 s / iter per device, on 1 devices)
[32m[06/02 14:06:42 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210223 s / iter per device, on 1 devices)
[32m[06/02 14:06:42 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:06:42 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:06:42 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:06:42 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:06:42 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:06:42 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:06:42 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:06:42 d2.utils.events]: [0m eta: 0:48:28  iter: 2999  total_loss: 6.526  loss_ce: 2.004e-06  loss_mask: 0.02909  loss_dice: 0.5571  loss_ce_0: 0.0001623  loss_mask_0: 0.0369  loss_dice_0: 0.6553  loss_ce_1: 1.331e-07  loss_mask_1: 0.03506  loss_dice_1: 0.5816  loss_ce_2: 4.375e-09  loss_mask_2: 0.03165  loss_dice_2: 0.5915  loss_ce_3: 4.375e-09  loss_mask_3: 0.03345  loss_dice_3: 0.6025  loss_ce_4: 8.749e-09  loss_mask_4: 0.03444  loss_dice_4: 0.6592  loss_ce_5: 0  loss_mask_5: 0.03518  loss_dice_5: 0.63  loss_ce_6: 0  loss_mask_6: 0.03879  loss_dice_6: 0.5683  loss_ce_7: 6.562e-09  loss_mask_7: 0.03468  loss_dice_7: 0.6708  loss_ce_8: 2.297e-08  loss_mask_8: 0.03599  loss_dice_8: 0.6791  time: 0.4149  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:50 d2.utils.events]: [0m eta: 0:48:20  iter: 3019  total_loss: 6.624  loss_ce: 2.359e-06  loss_mask: 0.04031  loss_dice: 0.6051  loss_ce_0: 0.0001581  loss_mask_0: 0.03351  loss_dice_0: 0.5979  loss_ce_1: 1.294e-07  loss_mask_1: 0.04082  loss_dice_1: 0.6158  loss_ce_2: 4.375e-09  loss_mask_2: 0.0337  loss_dice_2: 0.595  loss_ce_3: 6.562e-09  loss_mask_3: 0.03702  loss_dice_3: 0.6691  loss_ce_4: 1.094e-08  loss_mask_4: 0.03744  loss_dice_4: 0.6399  loss_ce_5: 0  loss_mask_5: 0.03563  loss_dice_5: 0.6572  loss_ce_6: 0  loss_mask_6: 0.03399  loss_dice_6: 0.6416  loss_ce_7: 6.562e-09  loss_mask_7: 0.03293  loss_dice_7: 0.5778  loss_ce_8: 3.718e-08  loss_mask_8: 0.03375  loss_dice_8: 0.5712  time: 0.4149  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:06:59 d2.utils.events]: [0m eta: 0:48:12  iter: 3039  total_loss: 6.291  loss_ce: 2.933e-06  loss_mask: 0.03472  loss_dice: 0.5535  loss_ce_0: 0.0001539  loss_mask_0: 0.03086  loss_dice_0: 0.632  loss_ce_1: 1.422e-07  loss_mask_1: 0.03256  loss_dice_1: 0.5509  loss_ce_2: 4.375e-09  loss_mask_2: 0.03352  loss_dice_2: 0.6083  loss_ce_3: 8.749e-09  loss_mask_3: 0.03475  loss_dice_3: 0.6188  loss_ce_4: 1.312e-08  loss_mask_4: 0.03117  loss_dice_4: 0.5961  loss_ce_5: 0  loss_mask_5: 0.0314  loss_dice_5: 0.6053  loss_ce_6: 0  loss_mask_6: 0.03542  loss_dice_6: 0.616  loss_ce_7: 1.75e-08  loss_mask_7: 0.03586  loss_dice_7: 0.5776  loss_ce_8: 6.015e-08  loss_mask_8: 0.03536  loss_dice_8: 0.625  time: 0.4150  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:03 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:07:03 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:07:03 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:07:03 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:07:03 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:07:03 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:07:04 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268196 (0.268196 s / iter per device, on 1 devices)
[32m[06/02 14:07:04 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.204598 s / iter per device, on 1 devices)
[32m[06/02 14:07:04 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:07:04 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:07:04 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:07:04 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:07:04 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:07:04 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:07:04 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:07:08 d2.utils.events]: [0m eta: 0:48:04  iter: 3059  total_loss: 6.739  loss_ce: 7.525e-06  loss_mask: 0.03502  loss_dice: 0.6372  loss_ce_0: 0.0001501  loss_mask_0: 0.04025  loss_dice_0: 0.6405  loss_ce_1: 1.424e-07  loss_mask_1: 0.03187  loss_dice_1: 0.595  loss_ce_2: 6.562e-09  loss_mask_2: 0.03359  loss_dice_2: 0.6314  loss_ce_3: 8.749e-09  loss_mask_3: 0.03644  loss_dice_3: 0.6548  loss_ce_4: 2.406e-08  loss_mask_4: 0.03669  loss_dice_4: 0.6313  loss_ce_5: 0  loss_mask_5: 0.03553  loss_dice_5: 0.6573  loss_ce_6: 0  loss_mask_6: 0.0348  loss_dice_6: 0.645  loss_ce_7: 1.859e-08  loss_mask_7: 0.03745  loss_dice_7: 0.6723  loss_ce_8: 1.345e-07  loss_mask_8: 0.0351  loss_dice_8: 0.6316  time: 0.4150  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:16 d2.utils.events]: [0m eta: 0:47:55  iter: 3079  total_loss: 6.569  loss_ce: 3.76e-06  loss_mask: 0.03194  loss_dice: 0.6126  loss_ce_0: 0.0001469  loss_mask_0: 0.03175  loss_dice_0: 0.5879  loss_ce_1: 1.681e-07  loss_mask_1: 0.03038  loss_dice_1: 0.5885  loss_ce_2: 6.562e-09  loss_mask_2: 0.03656  loss_dice_2: 0.642  loss_ce_3: 8.749e-09  loss_mask_3: 0.03415  loss_dice_3: 0.6379  loss_ce_4: 4.047e-08  loss_mask_4: 0.03441  loss_dice_4: 0.6146  loss_ce_5: 0  loss_mask_5: 0.03274  loss_dice_5: 0.6175  loss_ce_6: 0  loss_mask_6: 0.03402  loss_dice_6: 0.5937  loss_ce_7: 6.562e-09  loss_mask_7: 0.03294  loss_dice_7: 0.6495  loss_ce_8: 6.343e-08  loss_mask_8: 0.03172  loss_dice_8: 0.5764  time: 0.4150  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:25 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:07:25 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:07:25 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:07:25 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:07:25 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:07:25 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:07:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273439 (0.273439 s / iter per device, on 1 devices)
[32m[06/02 14:07:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212790 s / iter per device, on 1 devices)
[32m[06/02 14:07:25 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:07:25 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:07:25 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:07:25 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:07:25 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:07:25 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:07:25 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:07:25 d2.utils.events]: [0m eta: 0:47:47  iter: 3099  total_loss: 6.932  loss_ce: 1.596e-06  loss_mask: 0.03586  loss_dice: 0.6069  loss_ce_0: 0.0001445  loss_mask_0: 0.03569  loss_dice_0: 0.6491  loss_ce_1: 1.138e-07  loss_mask_1: 0.03294  loss_dice_1: 0.5826  loss_ce_2: 6.562e-09  loss_mask_2: 0.03281  loss_dice_2: 0.6568  loss_ce_3: 8.749e-09  loss_mask_3: 0.03624  loss_dice_3: 0.656  loss_ce_4: 4.921e-08  loss_mask_4: 0.03399  loss_dice_4: 0.641  loss_ce_5: 0  loss_mask_5: 0.03691  loss_dice_5: 0.6524  loss_ce_6: 0  loss_mask_6: 0.03595  loss_dice_6: 0.6713  loss_ce_7: 4.375e-09  loss_mask_7: 0.03513  loss_dice_7: 0.6303  loss_ce_8: 4.375e-08  loss_mask_8: 0.03475  loss_dice_8: 0.6062  time: 0.4150  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:34 d2.utils.events]: [0m eta: 0:47:39  iter: 3119  total_loss: 6.259  loss_ce: 1.369e-06  loss_mask: 0.03274  loss_dice: 0.553  loss_ce_0: 0.000142  loss_mask_0: 0.03136  loss_dice_0: 0.5818  loss_ce_1: 1.113e-07  loss_mask_1: 0.03499  loss_dice_1: 0.6186  loss_ce_2: 4.375e-09  loss_mask_2: 0.0332  loss_dice_2: 0.5975  loss_ce_3: 8.749e-09  loss_mask_3: 0.03312  loss_dice_3: 0.6203  loss_ce_4: 4.593e-08  loss_mask_4: 0.03092  loss_dice_4: 0.5795  loss_ce_5: 0  loss_mask_5: 0.03461  loss_dice_5: 0.5802  loss_ce_6: 0  loss_mask_6: 0.03267  loss_dice_6: 0.5558  loss_ce_7: 4.375e-09  loss_mask_7: 0.03421  loss_dice_7: 0.5923  loss_ce_8: 3.172e-08  loss_mask_8: 0.0313  loss_dice_8: 0.5988  time: 0.4150  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:42 d2.utils.events]: [0m eta: 0:47:30  iter: 3139  total_loss: 6.697  loss_ce: 1.462e-06  loss_mask: 0.03438  loss_dice: 0.6104  loss_ce_0: 0.000139  loss_mask_0: 0.0373  loss_dice_0: 0.6634  loss_ce_1: 1.11e-07  loss_mask_1: 0.03098  loss_dice_1: 0.6304  loss_ce_2: 4.375e-09  loss_mask_2: 0.03429  loss_dice_2: 0.6286  loss_ce_3: 8.749e-09  loss_mask_3: 0.03013  loss_dice_3: 0.5801  loss_ce_4: 3.937e-08  loss_mask_4: 0.03621  loss_dice_4: 0.6097  loss_ce_5: 0  loss_mask_5: 0.03334  loss_dice_5: 0.6251  loss_ce_6: 2.187e-09  loss_mask_6: 0.03435  loss_dice_6: 0.6141  loss_ce_7: 6.562e-09  loss_mask_7: 0.03564  loss_dice_7: 0.6464  loss_ce_8: 2.844e-08  loss_mask_8: 0.03666  loss_dice_8: 0.6079  time: 0.4150  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:46 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:07:46 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:07:46 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:07:46 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:07:46 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:07:46 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:07:47 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274600 (0.274600 s / iter per device, on 1 devices)
[32m[06/02 14:07:47 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215434 s / iter per device, on 1 devices)
[32m[06/02 14:07:47 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:07:47 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:07:47 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:07:47 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:07:47 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:07:47 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:07:47 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:07:51 d2.utils.events]: [0m eta: 0:47:22  iter: 3159  total_loss: 6.778  loss_ce: 1.317e-06  loss_mask: 0.03242  loss_dice: 0.5746  loss_ce_0: 0.0001367  loss_mask_0: 0.03486  loss_dice_0: 0.6345  loss_ce_1: 1.113e-07  loss_mask_1: 0.03444  loss_dice_1: 0.5982  loss_ce_2: 4.375e-09  loss_mask_2: 0.0326  loss_dice_2: 0.6508  loss_ce_3: 8.749e-09  loss_mask_3: 0.03397  loss_dice_3: 0.6261  loss_ce_4: 3.5e-08  loss_mask_4: 0.03302  loss_dice_4: 0.5957  loss_ce_5: 0  loss_mask_5: 0.03373  loss_dice_5: 0.642  loss_ce_6: 0  loss_mask_6: 0.03407  loss_dice_6: 0.6461  loss_ce_7: 8.749e-09  loss_mask_7: 0.03597  loss_dice_7: 0.6418  loss_ce_8: 2.844e-08  loss_mask_8: 0.03378  loss_dice_8: 0.6148  time: 0.4150  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:07:59 d2.utils.events]: [0m eta: 0:47:14  iter: 3179  total_loss: 6.252  loss_ce: 9.049e-07  loss_mask: 0.03552  loss_dice: 0.591  loss_ce_0: 0.0001341  loss_mask_0: 0.03223  loss_dice_0: 0.554  loss_ce_1: 1.627e-07  loss_mask_1: 0.03217  loss_dice_1: 0.602  loss_ce_2: 2.187e-09  loss_mask_2: 0.0331  loss_dice_2: 0.5985  loss_ce_3: 8.749e-09  loss_mask_3: 0.03411  loss_dice_3: 0.5797  loss_ce_4: 5.25e-08  loss_mask_4: 0.03382  loss_dice_4: 0.6315  loss_ce_5: 0  loss_mask_5: 0.03247  loss_dice_5: 0.5598  loss_ce_6: 0  loss_mask_6: 0.03265  loss_dice_6: 0.609  loss_ce_7: 9.843e-09  loss_mask_7: 0.03211  loss_dice_7: 0.5457  loss_ce_8: 2.078e-08  loss_mask_8: 0.03418  loss_dice_8: 0.5972  time: 0.4150  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:08:08 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:08:08 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:08:08 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:08:08 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:08:08 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:08:08 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:08:08 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277682 (0.277682 s / iter per device, on 1 devices)
[32m[06/02 14:08:08 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.218497 s / iter per device, on 1 devices)
[32m[06/02 14:08:08 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:08:08 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:08:09 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:08:09 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:08:09 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:08:09 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:08:09 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:08:09 d2.utils.events]: [0m eta: 0:47:05  iter: 3199  total_loss: 6.307  loss_ce: 6.524e-07  loss_mask: 0.03187  loss_dice: 0.5994  loss_ce_0: 0.0001315  loss_mask_0: 0.03597  loss_dice_0: 0.5992  loss_ce_1: 1.895e-07  loss_mask_1: 0.03229  loss_dice_1: 0.6017  loss_ce_2: 2.187e-09  loss_mask_2: 0.03579  loss_dice_2: 0.6223  loss_ce_3: 8.749e-09  loss_mask_3: 0.03343  loss_dice_3: 0.5821  loss_ce_4: 2.953e-08  loss_mask_4: 0.03016  loss_dice_4: 0.6433  loss_ce_5: 0  loss_mask_5: 0.03235  loss_dice_5: 0.5893  loss_ce_6: 0  loss_mask_6: 0.03324  loss_dice_6: 0.5607  loss_ce_7: 1.094e-08  loss_mask_7: 0.03705  loss_dice_7: 0.5739  loss_ce_8: 9.843e-09  loss_mask_8: 0.0294  loss_dice_8: 0.5571  time: 0.4150  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:08:17 d2.utils.events]: [0m eta: 0:46:57  iter: 3219  total_loss: 6.553  loss_ce: 4.038e-07  loss_mask: 0.03313  loss_dice: 0.6303  loss_ce_0: 0.0001288  loss_mask_0: 0.0354  loss_dice_0: 0.6612  loss_ce_1: 2.185e-07  loss_mask_1: 0.03146  loss_dice_1: 0.5793  loss_ce_2: 2.187e-09  loss_mask_2: 0.03286  loss_dice_2: 0.6003  loss_ce_3: 8.749e-09  loss_mask_3: 0.0352  loss_dice_3: 0.6353  loss_ce_4: 1.75e-08  loss_mask_4: 0.03377  loss_dice_4: 0.6099  loss_ce_5: 0  loss_mask_5: 0.03467  loss_dice_5: 0.6067  loss_ce_6: 0  loss_mask_6: 0.03367  loss_dice_6: 0.634  loss_ce_7: 6.562e-09  loss_mask_7: 0.03342  loss_dice_7: 0.6355  loss_ce_8: 6.562e-09  loss_mask_8: 0.03221  loss_dice_8: 0.5772  time: 0.4150  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:08:25 d2.utils.events]: [0m eta: 0:46:49  iter: 3239  total_loss: 6.497  loss_ce: 2.899e-07  loss_mask: 0.03338  loss_dice: 0.5721  loss_ce_0: 0.000126  loss_mask_0: 0.03458  loss_dice_0: 0.6258  loss_ce_1: 1.333e-07  loss_mask_1: 0.0357  loss_dice_1: 0.6643  loss_ce_2: 2.187e-09  loss_mask_2: 0.03541  loss_dice_2: 0.607  loss_ce_3: 6.562e-09  loss_mask_3: 0.03351  loss_dice_3: 0.5865  loss_ce_4: 1.312e-08  loss_mask_4: 0.03546  loss_dice_4: 0.6312  loss_ce_5: 0  loss_mask_5: 0.03311  loss_dice_5: 0.6153  loss_ce_6: 0  loss_mask_6: 0.03289  loss_dice_6: 0.6001  loss_ce_7: 4.375e-09  loss_mask_7: 0.03414  loss_dice_7: 0.6155  loss_ce_8: 4.375e-09  loss_mask_8: 0.03565  loss_dice_8: 0.563  time: 0.4150  data_time: 0.0080  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:08:30 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:08:30 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:08:30 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:08:30 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:08:30 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:08:30 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:08:30 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.284578 (0.284578 s / iter per device, on 1 devices)
[32m[06/02 14:08:30 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.216356 s / iter per device, on 1 devices)
[32m[06/02 14:08:30 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:08:30 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:08:30 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:08:30 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:08:30 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:08:30 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:08:30 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:08:35 d2.utils.events]: [0m eta: 0:46:40  iter: 3259  total_loss: 6.265  loss_ce: 2.397e-07  loss_mask: 0.03277  loss_dice: 0.5525  loss_ce_0: 0.0001236  loss_mask_0: 0.03348  loss_dice_0: 0.5829  loss_ce_1: 1.368e-07  loss_mask_1: 0.0334  loss_dice_1: 0.57  loss_ce_2: 2.187e-09  loss_mask_2: 0.03333  loss_dice_2: 0.5705  loss_ce_3: 6.562e-09  loss_mask_3: 0.0293  loss_dice_3: 0.5852  loss_ce_4: 1.312e-08  loss_mask_4: 0.03175  loss_dice_4: 0.5766  loss_ce_5: 0  loss_mask_5: 0.03527  loss_dice_5: 0.594  loss_ce_6: 0  loss_mask_6: 0.03295  loss_dice_6: 0.56  loss_ce_7: 4.375e-09  loss_mask_7: 0.03313  loss_dice_7: 0.582  loss_ce_8: 2.187e-09  loss_mask_8: 0.03516  loss_dice_8: 0.5722  time: 0.4150  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:08:43 d2.utils.events]: [0m eta: 0:46:32  iter: 3279  total_loss: 6.389  loss_ce: 1.464e-07  loss_mask: 0.03344  loss_dice: 0.6123  loss_ce_0: 0.0001216  loss_mask_0: 0.03219  loss_dice_0: 0.6321  loss_ce_1: 1.519e-07  loss_mask_1: 0.03364  loss_dice_1: 0.6754  loss_ce_2: 2.187e-09  loss_mask_2: 0.03099  loss_dice_2: 0.6006  loss_ce_3: 6.562e-09  loss_mask_3: 0.03239  loss_dice_3: 0.6031  loss_ce_4: 1.312e-08  loss_mask_4: 0.0312  loss_dice_4: 0.6024  loss_ce_5: 0  loss_mask_5: 0.03676  loss_dice_5: 0.5935  loss_ce_6: 0  loss_mask_6: 0.03214  loss_dice_6: 0.5784  loss_ce_7: 4.375e-09  loss_mask_7: 0.03291  loss_dice_7: 0.6333  loss_ce_8: 2.187e-09  loss_mask_8: 0.03592  loss_dice_8: 0.6006  time: 0.4151  data_time: 0.0084  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:08:51 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:08:51 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:08:51 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:08:51 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:08:51 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:08:51 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:08:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266398 (0.266398 s / iter per device, on 1 devices)
[32m[06/02 14:08:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.203216 s / iter per device, on 1 devices)
[32m[06/02 14:08:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:08:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:08:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:08:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:08:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:08:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:08:52 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:08:52 d2.utils.events]: [0m eta: 0:46:24  iter: 3299  total_loss: 6.378  loss_ce: 7.032e-08  loss_mask: 0.03013  loss_dice: 0.6004  loss_ce_0: 0.0001195  loss_mask_0: 0.03308  loss_dice_0: 0.5681  loss_ce_1: 1.714e-07  loss_mask_1: 0.03143  loss_dice_1: 0.6438  loss_ce_2: 2.187e-09  loss_mask_2: 0.03315  loss_dice_2: 0.6096  loss_ce_3: 4.375e-09  loss_mask_3: 0.03349  loss_dice_3: 0.6229  loss_ce_4: 1.094e-08  loss_mask_4: 0.03247  loss_dice_4: 0.5605  loss_ce_5: 0  loss_mask_5: 0.03173  loss_dice_5: 0.5774  loss_ce_6: 0  loss_mask_6: 0.03282  loss_dice_6: 0.5831  loss_ce_7: 2.187e-09  loss_mask_7: 0.03284  loss_dice_7: 0.6043  loss_ce_8: 1.094e-09  loss_mask_8: 0.02841  loss_dice_8: 0.5575  time: 0.4151  data_time: 0.0081  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:00 d2.utils.events]: [0m eta: 0:46:15  iter: 3319  total_loss: 6.646  loss_ce: 1.849e-07  loss_mask: 0.03451  loss_dice: 0.6382  loss_ce_0: 0.0001173  loss_mask_0: 0.03424  loss_dice_0: 0.6425  loss_ce_1: 1.513e-07  loss_mask_1: 0.03195  loss_dice_1: 0.6143  loss_ce_2: 2.187e-09  loss_mask_2: 0.03624  loss_dice_2: 0.6053  loss_ce_3: 4.375e-09  loss_mask_3: 0.03093  loss_dice_3: 0.6179  loss_ce_4: 7.656e-09  loss_mask_4: 0.03347  loss_dice_4: 0.6814  loss_ce_5: 0  loss_mask_5: 0.03387  loss_dice_5: 0.6287  loss_ce_6: 0  loss_mask_6: 0.03281  loss_dice_6: 0.6489  loss_ce_7: 2.187e-09  loss_mask_7: 0.03286  loss_dice_7: 0.582  loss_ce_8: 2.187e-09  loss_mask_8: 0.03157  loss_dice_8: 0.6069  time: 0.4151  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:09 d2.utils.events]: [0m eta: 0:46:07  iter: 3339  total_loss: 6.777  loss_ce: 4.745e-07  loss_mask: 0.03236  loss_dice: 0.6066  loss_ce_0: 0.0001153  loss_mask_0: 0.03233  loss_dice_0: 0.606  loss_ce_1: 1.588e-07  loss_mask_1: 0.03493  loss_dice_1: 0.6184  loss_ce_2: 2.187e-09  loss_mask_2: 0.03728  loss_dice_2: 0.6001  loss_ce_3: 6.562e-09  loss_mask_3: 0.03721  loss_dice_3: 0.6665  loss_ce_4: 6.562e-09  loss_mask_4: 0.03096  loss_dice_4: 0.5699  loss_ce_5: 0  loss_mask_5: 0.03551  loss_dice_5: 0.7168  loss_ce_6: 0  loss_mask_6: 0.03201  loss_dice_6: 0.6222  loss_ce_7: 4.375e-09  loss_mask_7: 0.03794  loss_dice_7: 0.6224  loss_ce_8: 5.468e-09  loss_mask_8: 0.03967  loss_dice_8: 0.6793  time: 0.4151  data_time: 0.0063  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:13 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:09:13 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:09:13 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:09:13 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:09:13 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:09:13 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:09:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277527 (0.277527 s / iter per device, on 1 devices)
[32m[06/02 14:09:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215313 s / iter per device, on 1 devices)
[32m[06/02 14:09:14 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:09:14 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:09:14 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:09:14 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:09:14 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:09:14 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:09:14 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:09:18 d2.utils.events]: [0m eta: 0:45:58  iter: 3359  total_loss: 6.417  loss_ce: 7.901e-07  loss_mask: 0.03256  loss_dice: 0.5712  loss_ce_0: 0.0001127  loss_mask_0: 0.03442  loss_dice_0: 0.6071  loss_ce_1: 1.574e-07  loss_mask_1: 0.03396  loss_dice_1: 0.6292  loss_ce_2: 2.187e-09  loss_mask_2: 0.03145  loss_dice_2: 0.5786  loss_ce_3: 8.749e-09  loss_mask_3: 0.03445  loss_dice_3: 0.574  loss_ce_4: 6.562e-09  loss_mask_4: 0.03161  loss_dice_4: 0.5624  loss_ce_5: 0  loss_mask_5: 0.0316  loss_dice_5: 0.6141  loss_ce_6: 0  loss_mask_6: 0.03254  loss_dice_6: 0.5982  loss_ce_7: 4.375e-09  loss_mask_7: 0.03385  loss_dice_7: 0.6121  loss_ce_8: 4.375e-09  loss_mask_8: 0.03226  loss_dice_8: 0.5999  time: 0.4151  data_time: 0.0070  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:26 d2.utils.events]: [0m eta: 0:45:50  iter: 3379  total_loss: 6.423  loss_ce: 1.273e-06  loss_mask: 0.03333  loss_dice: 0.621  loss_ce_0: 0.0001106  loss_mask_0: 0.03102  loss_dice_0: 0.5857  loss_ce_1: 1.628e-07  loss_mask_1: 0.03145  loss_dice_1: 0.6025  loss_ce_2: 2.187e-09  loss_mask_2: 0.03292  loss_dice_2: 0.6074  loss_ce_3: 6.562e-09  loss_mask_3: 0.03579  loss_dice_3: 0.6178  loss_ce_4: 8.749e-09  loss_mask_4: 0.03695  loss_dice_4: 0.6292  loss_ce_5: 0  loss_mask_5: 0.03697  loss_dice_5: 0.6406  loss_ce_6: 0  loss_mask_6: 0.03499  loss_dice_6: 0.6112  loss_ce_7: 4.375e-09  loss_mask_7: 0.03393  loss_dice_7: 0.6392  loss_ce_8: 6.562e-09  loss_mask_8: 0.03711  loss_dice_8: 0.6525  time: 0.4151  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:35 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:09:35 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:09:35 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:09:35 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:09:35 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:09:35 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:09:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.267765 (0.267765 s / iter per device, on 1 devices)
[32m[06/02 14:09:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208248 s / iter per device, on 1 devices)
[32m[06/02 14:09:35 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:09:35 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:09:35 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:09:35 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:09:35 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:09:35 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:09:35 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:09:35 d2.utils.events]: [0m eta: 0:45:42  iter: 3399  total_loss: 6.839  loss_ce: 7.912e-07  loss_mask: 0.03302  loss_dice: 0.6532  loss_ce_0: 0.0001084  loss_mask_0: 0.03238  loss_dice_0: 0.6685  loss_ce_1: 1.388e-07  loss_mask_1: 0.03966  loss_dice_1: 0.6799  loss_ce_2: 2.187e-09  loss_mask_2: 0.03547  loss_dice_2: 0.6443  loss_ce_3: 6.562e-09  loss_mask_3: 0.03567  loss_dice_3: 0.6987  loss_ce_4: 8.749e-09  loss_mask_4: 0.03168  loss_dice_4: 0.6737  loss_ce_5: 0  loss_mask_5: 0.03458  loss_dice_5: 0.6456  loss_ce_6: 0  loss_mask_6: 0.03155  loss_dice_6: 0.6147  loss_ce_7: 4.375e-09  loss_mask_7: 0.03104  loss_dice_7: 0.6662  loss_ce_8: 6.562e-09  loss_mask_8: 0.03403  loss_dice_8: 0.6552  time: 0.4150  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:44 d2.utils.events]: [0m eta: 0:45:33  iter: 3419  total_loss: 6.705  loss_ce: 7.752e-07  loss_mask: 0.03472  loss_dice: 0.6363  loss_ce_0: 0.0001064  loss_mask_0: 0.03491  loss_dice_0: 0.6144  loss_ce_1: 1.378e-07  loss_mask_1: 0.03156  loss_dice_1: 0.5821  loss_ce_2: 2.187e-09  loss_mask_2: 0.03561  loss_dice_2: 0.6171  loss_ce_3: 8.749e-09  loss_mask_3: 0.03452  loss_dice_3: 0.6246  loss_ce_4: 8.749e-09  loss_mask_4: 0.03285  loss_dice_4: 0.6585  loss_ce_5: 2.187e-09  loss_mask_5: 0.0347  loss_dice_5: 0.6093  loss_ce_6: 0  loss_mask_6: 0.03201  loss_dice_6: 0.6715  loss_ce_7: 4.375e-09  loss_mask_7: 0.03366  loss_dice_7: 0.6228  loss_ce_8: 3.281e-09  loss_mask_8: 0.03417  loss_dice_8: 0.6564  time: 0.4150  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:52 d2.utils.events]: [0m eta: 0:45:24  iter: 3439  total_loss: 6.409  loss_ce: 3.655e-07  loss_mask: 0.03032  loss_dice: 0.6042  loss_ce_0: 0.0001046  loss_mask_0: 0.03314  loss_dice_0: 0.6065  loss_ce_1: 1.464e-07  loss_mask_1: 0.03068  loss_dice_1: 0.5861  loss_ce_2: 2.187e-09  loss_mask_2: 0.03096  loss_dice_2: 0.6162  loss_ce_3: 8.749e-09  loss_mask_3: 0.03468  loss_dice_3: 0.5882  loss_ce_4: 1.094e-08  loss_mask_4: 0.03428  loss_dice_4: 0.6321  loss_ce_5: 0  loss_mask_5: 0.0352  loss_dice_5: 0.5924  loss_ce_6: 0  loss_mask_6: 0.03511  loss_dice_6: 0.643  loss_ce_7: 2.187e-09  loss_mask_7: 0.03416  loss_dice_7: 0.5832  loss_ce_8: 2.187e-09  loss_mask_8: 0.03213  loss_dice_8: 0.6355  time: 0.4150  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:09:56 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:09:56 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:09:56 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:09:56 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:09:56 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:09:56 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:09:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.256500 (0.256500 s / iter per device, on 1 devices)
[32m[06/02 14:09:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.198433 s / iter per device, on 1 devices)
[32m[06/02 14:09:57 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:09:57 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:09:57 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:09:57 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:09:57 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:09:57 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:09:57 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:10:01 d2.utils.events]: [0m eta: 0:45:15  iter: 3459  total_loss: 6.815  loss_ce: 1.895e-07  loss_mask: 0.03523  loss_dice: 0.5654  loss_ce_0: 0.0001027  loss_mask_0: 0.03655  loss_dice_0: 0.6094  loss_ce_1: 1.698e-07  loss_mask_1: 0.0337  loss_dice_1: 0.6104  loss_ce_2: 2.187e-09  loss_mask_2: 0.03272  loss_dice_2: 0.6286  loss_ce_3: 4.375e-09  loss_mask_3: 0.0346  loss_dice_3: 0.6483  loss_ce_4: 1.094e-08  loss_mask_4: 0.03286  loss_dice_4: 0.6391  loss_ce_5: 0  loss_mask_5: 0.03701  loss_dice_5: 0.6425  loss_ce_6: 0  loss_mask_6: 0.03345  loss_dice_6: 0.7084  loss_ce_7: 0  loss_mask_7: 0.0327  loss_dice_7: 0.627  loss_ce_8: 2.187e-09  loss_mask_8: 0.03517  loss_dice_8: 0.6009  time: 0.4150  data_time: 0.0069  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:10:09 d2.utils.events]: [0m eta: 0:45:07  iter: 3479  total_loss: 6.485  loss_ce: 1.245e-07  loss_mask: 0.03445  loss_dice: 0.6329  loss_ce_0: 0.0001012  loss_mask_0: 0.0354  loss_dice_0: 0.6429  loss_ce_1: 1.516e-07  loss_mask_1: 0.03181  loss_dice_1: 0.6012  loss_ce_2: 2.187e-09  loss_mask_2: 0.03418  loss_dice_2: 0.6521  loss_ce_3: 4.375e-09  loss_mask_3: 0.03257  loss_dice_3: 0.5639  loss_ce_4: 9.843e-09  loss_mask_4: 0.03381  loss_dice_4: 0.636  loss_ce_5: 0  loss_mask_5: 0.03321  loss_dice_5: 0.5866  loss_ce_6: 0  loss_mask_6: 0.03202  loss_dice_6: 0.6332  loss_ce_7: 0  loss_mask_7: 0.03406  loss_dice_7: 0.6092  loss_ce_8: 2.187e-09  loss_mask_8: 0.03245  loss_dice_8: 0.5781  time: 0.4150  data_time: 0.0066  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:10:17 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:10:17 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:10:17 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:10:17 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:10:17 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:10:17 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:10:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.270951 (0.270951 s / iter per device, on 1 devices)
[32m[06/02 14:10:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.209766 s / iter per device, on 1 devices)
[32m[06/02 14:10:18 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:10:18 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:10:18 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:10:18 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:10:18 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:10:18 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:10:18 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:10:18 d2.utils.events]: [0m eta: 0:44:58  iter: 3499  total_loss: 6.573  loss_ce: 8.531e-08  loss_mask: 0.03339  loss_dice: 0.6199  loss_ce_0: 9.918e-05  loss_mask_0: 0.03115  loss_dice_0: 0.6458  loss_ce_1: 1.376e-07  loss_mask_1: 0.03405  loss_dice_1: 0.6007  loss_ce_2: 2.187e-09  loss_mask_2: 0.03368  loss_dice_2: 0.6202  loss_ce_3: 4.375e-09  loss_mask_3: 0.03562  loss_dice_3: 0.6317  loss_ce_4: 1.203e-08  loss_mask_4: 0.03668  loss_dice_4: 0.6922  loss_ce_5: 0  loss_mask_5: 0.03488  loss_dice_5: 0.6503  loss_ce_6: 0  loss_mask_6: 0.0341  loss_dice_6: 0.6557  loss_ce_7: 0  loss_mask_7: 0.03095  loss_dice_7: 0.6086  loss_ce_8: 2.187e-09  loss_mask_8: 0.03473  loss_dice_8: 0.6074  time: 0.4149  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:10:27 d2.utils.events]: [0m eta: 0:44:50  iter: 3519  total_loss: 6.678  loss_ce: 1.767e-07  loss_mask: 0.03394  loss_dice: 0.5913  loss_ce_0: 9.745e-05  loss_mask_0: 0.03113  loss_dice_0: 0.6041  loss_ce_1: 1.448e-07  loss_mask_1: 0.03599  loss_dice_1: 0.5921  loss_ce_2: 2.187e-09  loss_mask_2: 0.03501  loss_dice_2: 0.692  loss_ce_3: 4.375e-09  loss_mask_3: 0.03353  loss_dice_3: 0.6582  loss_ce_4: 1.094e-08  loss_mask_4: 0.03359  loss_dice_4: 0.6793  loss_ce_5: 2.187e-09  loss_mask_5: 0.03714  loss_dice_5: 0.6844  loss_ce_6: 0  loss_mask_6: 0.03246  loss_dice_6: 0.6302  loss_ce_7: 0  loss_mask_7: 0.03377  loss_dice_7: 0.626  loss_ce_8: 4.375e-09  loss_mask_8: 0.03546  loss_dice_8: 0.6174  time: 0.4149  data_time: 0.0064  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:10:35 d2.utils.events]: [0m eta: 0:44:42  iter: 3539  total_loss: 6.589  loss_ce: 3.726e-07  loss_mask: 0.03024  loss_dice: 0.5908  loss_ce_0: 9.583e-05  loss_mask_0: 0.03294  loss_dice_0: 0.6381  loss_ce_1: 1.394e-07  loss_mask_1: 0.03638  loss_dice_1: 0.6224  loss_ce_2: 2.187e-09  loss_mask_2: 0.0365  loss_dice_2: 0.6931  loss_ce_3: 2.187e-09  loss_mask_3: 0.03293  loss_dice_3: 0.6302  loss_ce_4: 6.562e-09  loss_mask_4: 0.03556  loss_dice_4: 0.6511  loss_ce_5: 2.187e-09  loss_mask_5: 0.03358  loss_dice_5: 0.6537  loss_ce_6: 0  loss_mask_6: 0.02952  loss_dice_6: 0.6015  loss_ce_7: 1.094e-09  loss_mask_7: 0.0346  loss_dice_7: 0.6294  loss_ce_8: 5.468e-09  loss_mask_8: 0.03579  loss_dice_8: 0.6698  time: 0.4149  data_time: 0.0079  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:10:39 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:10:39 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:10:39 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:10:39 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:10:39 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:10:39 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:10:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.299576 (0.299576 s / iter per device, on 1 devices)
[32m[06/02 14:10:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.237988 s / iter per device, on 1 devices)
[32m[06/02 14:10:40 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:10:40 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:10:40 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:10:40 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:10:40 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:10:40 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:10:40 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:10:44 d2.utils.events]: [0m eta: 0:44:33  iter: 3559  total_loss: 6.438  loss_ce: 7.697e-07  loss_mask: 0.03716  loss_dice: 0.6398  loss_ce_0: 9.42e-05  loss_mask_0: 0.03683  loss_dice_0: 0.5925  loss_ce_1: 1.432e-07  loss_mask_1: 0.03339  loss_dice_1: 0.6053  loss_ce_2: 2.187e-09  loss_mask_2: 0.03153  loss_dice_2: 0.5888  loss_ce_3: 2.187e-09  loss_mask_3: 0.03525  loss_dice_3: 0.5773  loss_ce_4: 6.562e-09  loss_mask_4: 0.03339  loss_dice_4: 0.5828  loss_ce_5: 2.187e-09  loss_mask_5: 0.0328  loss_dice_5: 0.579  loss_ce_6: 0  loss_mask_6: 0.03236  loss_dice_6: 0.6157  loss_ce_7: 0  loss_mask_7: 0.03106  loss_dice_7: 0.6188  loss_ce_8: 5.468e-09  loss_mask_8: 0.03271  loss_dice_8: 0.6336  time: 0.4149  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:10:52 d2.utils.events]: [0m eta: 0:44:25  iter: 3579  total_loss: 6.453  loss_ce: 2.488e-07  loss_mask: 0.03304  loss_dice: 0.6029  loss_ce_0: 9.274e-05  loss_mask_0: 0.03293  loss_dice_0: 0.6245  loss_ce_1: 1.42e-07  loss_mask_1: 0.0333  loss_dice_1: 0.6421  loss_ce_2: 2.187e-09  loss_mask_2: 0.03327  loss_dice_2: 0.6646  loss_ce_3: 2.187e-09  loss_mask_3: 0.0368  loss_dice_3: 0.5969  loss_ce_4: 6.562e-09  loss_mask_4: 0.03157  loss_dice_4: 0.5981  loss_ce_5: 2.187e-09  loss_mask_5: 0.03093  loss_dice_5: 0.5945  loss_ce_6: 0  loss_mask_6: 0.03396  loss_dice_6: 0.6078  loss_ce_7: 0  loss_mask_7: 0.03418  loss_dice_7: 0.582  loss_ce_8: 2.187e-09  loss_mask_8: 0.03323  loss_dice_8: 0.6085  time: 0.4149  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:01 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:11:01 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:11:01 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:11:01 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:11:01 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:11:01 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:11:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.256817 (0.256817 s / iter per device, on 1 devices)
[32m[06/02 14:11:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.197172 s / iter per device, on 1 devices)
[32m[06/02 14:11:01 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:11:01 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:11:01 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:11:01 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:11:01 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:11:01 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:11:01 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:11:02 d2.utils.events]: [0m eta: 0:44:17  iter: 3599  total_loss: 6.555  loss_ce: 2.954e-07  loss_mask: 0.03256  loss_dice: 0.606  loss_ce_0: 9.131e-05  loss_mask_0: 0.03288  loss_dice_0: 0.6112  loss_ce_1: 1.443e-07  loss_mask_1: 0.02925  loss_dice_1: 0.6071  loss_ce_2: 2.187e-09  loss_mask_2: 0.0347  loss_dice_2: 0.634  loss_ce_3: 2.187e-09  loss_mask_3: 0.03416  loss_dice_3: 0.5991  loss_ce_4: 6.562e-09  loss_mask_4: 0.03391  loss_dice_4: 0.6069  loss_ce_5: 2.187e-09  loss_mask_5: 0.0337  loss_dice_5: 0.5968  loss_ce_6: 0  loss_mask_6: 0.03493  loss_dice_6: 0.6089  loss_ce_7: 0  loss_mask_7: 0.03566  loss_dice_7: 0.5856  loss_ce_8: 2.187e-09  loss_mask_8: 0.03372  loss_dice_8: 0.5632  time: 0.4149  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:10 d2.utils.events]: [0m eta: 0:44:08  iter: 3619  total_loss: 6.256  loss_ce: 6.354e-07  loss_mask: 0.031  loss_dice: 0.5799  loss_ce_0: 8.96e-05  loss_mask_0: 0.03269  loss_dice_0: 0.5986  loss_ce_1: 2.254e-07  loss_mask_1: 0.03595  loss_dice_1: 0.5776  loss_ce_2: 2.625e-09  loss_mask_2: 0.03203  loss_dice_2: 0.5622  loss_ce_3: 2.406e-09  loss_mask_3: 0.0362  loss_dice_3: 0.5787  loss_ce_4: 8.749e-09  loss_mask_4: 0.03568  loss_dice_4: 0.6338  loss_ce_5: 2.187e-09  loss_mask_5: 0.03239  loss_dice_5: 0.5627  loss_ce_6: 0  loss_mask_6: 0.03268  loss_dice_6: 0.6021  loss_ce_7: 2.187e-09  loss_mask_7: 0.03449  loss_dice_7: 0.5603  loss_ce_8: 2.187e-09  loss_mask_8: 0.03397  loss_dice_8: 0.5869  time: 0.4149  data_time: 0.0078  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:18 d2.utils.events]: [0m eta: 0:44:00  iter: 3639  total_loss: 6.405  loss_ce: 8.769e-07  loss_mask: 0.03231  loss_dice: 0.585  loss_ce_0: 8.777e-05  loss_mask_0: 0.03288  loss_dice_0: 0.567  loss_ce_1: 2.62e-07  loss_mask_1: 0.03469  loss_dice_1: 0.6094  loss_ce_2: 2.625e-09  loss_mask_2: 0.03469  loss_dice_2: 0.6174  loss_ce_3: 2.844e-09  loss_mask_3: 0.03176  loss_dice_3: 0.6121  loss_ce_4: 1.094e-08  loss_mask_4: 0.0366  loss_dice_4: 0.6021  loss_ce_5: 2.187e-09  loss_mask_5: 0.03206  loss_dice_5: 0.6051  loss_ce_6: 0  loss_mask_6: 0.03091  loss_dice_6: 0.6221  loss_ce_7: 2.187e-09  loss_mask_7: 0.03304  loss_dice_7: 0.6297  loss_ce_8: 2.187e-09  loss_mask_8: 0.03293  loss_dice_8: 0.5867  time: 0.4149  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:22 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:11:22 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:11:22 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:11:22 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:11:22 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:11:22 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:11:23 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271464 (0.271464 s / iter per device, on 1 devices)
[32m[06/02 14:11:23 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205581 s / iter per device, on 1 devices)
[32m[06/02 14:11:23 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:11:23 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:11:23 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:11:23 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:11:23 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:11:23 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:11:23 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:11:27 d2.utils.events]: [0m eta: 0:43:52  iter: 3659  total_loss: 6.556  loss_ce: 5.415e-07  loss_mask: 0.0377  loss_dice: 0.6018  loss_ce_0: 8.648e-05  loss_mask_0: 0.02888  loss_dice_0: 0.6021  loss_ce_1: 2.153e-07  loss_mask_1: 0.03289  loss_dice_1: 0.5686  loss_ce_2: 2.734e-09  loss_mask_2: 0.03194  loss_dice_2: 0.5772  loss_ce_3: 2.625e-09  loss_mask_3: 0.03185  loss_dice_3: 0.6176  loss_ce_4: 1.312e-08  loss_mask_4: 0.03295  loss_dice_4: 0.6265  loss_ce_5: 2.187e-09  loss_mask_5: 0.03351  loss_dice_5: 0.5872  loss_ce_6: 0  loss_mask_6: 0.03665  loss_dice_6: 0.6095  loss_ce_7: 2.187e-09  loss_mask_7: 0.03463  loss_dice_7: 0.6049  loss_ce_8: 2.187e-09  loss_mask_8: 0.0346  loss_dice_8: 0.5945  time: 0.4149  data_time: 0.0074  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:36 d2.utils.events]: [0m eta: 0:43:43  iter: 3679  total_loss: 6.717  loss_ce: 2.431e-07  loss_mask: 0.03191  loss_dice: 0.608  loss_ce_0: 8.527e-05  loss_mask_0: 0.03663  loss_dice_0: 0.6236  loss_ce_1: 1.204e-07  loss_mask_1: 0.03638  loss_dice_1: 0.6488  loss_ce_2: 2.187e-09  loss_mask_2: 0.0353  loss_dice_2: 0.6285  loss_ce_3: 2.187e-09  loss_mask_3: 0.03613  loss_dice_3: 0.5889  loss_ce_4: 1.312e-08  loss_mask_4: 0.03708  loss_dice_4: 0.652  loss_ce_5: 2.187e-09  loss_mask_5: 0.03412  loss_dice_5: 0.6307  loss_ce_6: 0  loss_mask_6: 0.03391  loss_dice_6: 0.5979  loss_ce_7: 2.187e-09  loss_mask_7: 0.03283  loss_dice_7: 0.6328  loss_ce_8: 0  loss_mask_8: 0.03426  loss_dice_8: 0.6595  time: 0.4149  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:44 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:11:44 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:11:44 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:11:44 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:11:44 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:11:44 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:11:45 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271872 (0.271872 s / iter per device, on 1 devices)
[32m[06/02 14:11:45 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206435 s / iter per device, on 1 devices)
[32m[06/02 14:11:45 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:11:45 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:11:45 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 90.000 | 100.000 | 100.000 | 90.000 |  nan  |  nan  | 90.000 | 90.000 |
[32m[06/02 14:11:45 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:11:45 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:11:45 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:11:45 d2.evaluation.testing]: [0mcopypaste: 90.0000,100.0000,100.0000,90.0000,nan,nan,90.0000,90.0000
[32m[06/02 14:11:45 d2.utils.events]: [0m eta: 0:43:35  iter: 3699  total_loss: 6.333  loss_ce: 2.641e-07  loss_mask: 0.03494  loss_dice: 0.6157  loss_ce_0: 8.393e-05  loss_mask_0: 0.03422  loss_dice_0: 0.6089  loss_ce_1: 1.122e-07  loss_mask_1: 0.03225  loss_dice_1: 0.546  loss_ce_2: 2.187e-09  loss_mask_2: 0.03239  loss_dice_2: 0.5859  loss_ce_3: 2.187e-09  loss_mask_3: 0.03084  loss_dice_3: 0.5546  loss_ce_4: 1.422e-08  loss_mask_4: 0.03323  loss_dice_4: 0.6177  loss_ce_5: 2.187e-09  loss_mask_5: 0.03336  loss_dice_5: 0.6065  loss_ce_6: 0  loss_mask_6: 0.03698  loss_dice_6: 0.6054  loss_ce_7: 2.187e-09  loss_mask_7: 0.03418  loss_dice_7: 0.6019  loss_ce_8: 0  loss_mask_8: 0.0339  loss_dice_8: 0.6013  time: 0.4149  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:11:53 d2.utils.events]: [0m eta: 0:43:27  iter: 3719  total_loss: 6.211  loss_ce: 1.817e-07  loss_mask: 0.03191  loss_dice: 0.581  loss_ce_0: 8.249e-05  loss_mask_0: 0.03399  loss_dice_0: 0.5713  loss_ce_1: 1.041e-07  loss_mask_1: 0.03224  loss_dice_1: 0.5228  loss_ce_2: 2.187e-09  loss_mask_2: 0.03312  loss_dice_2: 0.6005  loss_ce_3: 2.187e-09  loss_mask_3: 0.03219  loss_dice_3: 0.5471  loss_ce_4: 1.094e-08  loss_mask_4: 0.03513  loss_dice_4: 0.5706  loss_ce_5: 2.187e-09  loss_mask_5: 0.03106  loss_dice_5: 0.5876  loss_ce_6: 0  loss_mask_6: 0.03315  loss_dice_6: 0.6238  loss_ce_7: 2.187e-09  loss_mask_7: 0.03654  loss_dice_7: 0.6288  loss_ce_8: 0  loss_mask_8: 0.03231  loss_dice_8: 0.5725  time: 0.4150  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:02 d2.utils.events]: [0m eta: 0:43:18  iter: 3739  total_loss: 6.574  loss_ce: 1.969e-07  loss_mask: 0.03334  loss_dice: 0.5943  loss_ce_0: 8.128e-05  loss_mask_0: 0.03382  loss_dice_0: 0.554  loss_ce_1: 1.011e-07  loss_mask_1: 0.03647  loss_dice_1: 0.6101  loss_ce_2: 2.187e-09  loss_mask_2: 0.03405  loss_dice_2: 0.6577  loss_ce_3: 2.187e-09  loss_mask_3: 0.03368  loss_dice_3: 0.621  loss_ce_4: 8.749e-09  loss_mask_4: 0.03447  loss_dice_4: 0.5561  loss_ce_5: 2.187e-09  loss_mask_5: 0.03543  loss_dice_5: 0.6202  loss_ce_6: 0  loss_mask_6: 0.03573  loss_dice_6: 0.6706  loss_ce_7: 2.187e-09  loss_mask_7: 0.03823  loss_dice_7: 0.6674  loss_ce_8: 0  loss_mask_8: 0.03209  loss_dice_8: 0.6244  time: 0.4149  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:06 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:12:06 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:12:06 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:12:06 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:12:06 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:12:06 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:12:06 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.261272 (0.261272 s / iter per device, on 1 devices)
[32m[06/02 14:12:06 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.199655 s / iter per device, on 1 devices)
[32m[06/02 14:12:06 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:12:06 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:12:06 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:12:06 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:12:06 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:12:06 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:12:06 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:12:11 d2.utils.events]: [0m eta: 0:43:10  iter: 3759  total_loss: 6.762  loss_ce: 2.545e-07  loss_mask: 0.03282  loss_dice: 0.6229  loss_ce_0: 7.999e-05  loss_mask_0: 0.0347  loss_dice_0: 0.6385  loss_ce_1: 1.103e-07  loss_mask_1: 0.03257  loss_dice_1: 0.6407  loss_ce_2: 2.187e-09  loss_mask_2: 0.03359  loss_dice_2: 0.6639  loss_ce_3: 2.187e-09  loss_mask_3: 0.0347  loss_dice_3: 0.6592  loss_ce_4: 1.094e-08  loss_mask_4: 0.03652  loss_dice_4: 0.6165  loss_ce_5: 2.187e-09  loss_mask_5: 0.03339  loss_dice_5: 0.664  loss_ce_6: 0  loss_mask_6: 0.03228  loss_dice_6: 0.5983  loss_ce_7: 2.187e-09  loss_mask_7: 0.03662  loss_dice_7: 0.6497  loss_ce_8: 0  loss_mask_8: 0.03304  loss_dice_8: 0.634  time: 0.4149  data_time: 0.0083  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:19 d2.utils.events]: [0m eta: 0:43:01  iter: 3779  total_loss: 6.636  loss_ce: 5.509e-07  loss_mask: 0.03761  loss_dice: 0.6812  loss_ce_0: 7.888e-05  loss_mask_0: 0.0356  loss_dice_0: 0.6835  loss_ce_1: 1.727e-07  loss_mask_1: 0.03563  loss_dice_1: 0.6595  loss_ce_2: 2.187e-09  loss_mask_2: 0.03009  loss_dice_2: 0.6166  loss_ce_3: 2.187e-09  loss_mask_3: 0.03734  loss_dice_3: 0.7  loss_ce_4: 1.094e-08  loss_mask_4: 0.0303  loss_dice_4: 0.6076  loss_ce_5: 2.187e-09  loss_mask_5: 0.036  loss_dice_5: 0.6626  loss_ce_6: 0  loss_mask_6: 0.03393  loss_dice_6: 0.6459  loss_ce_7: 2.187e-09  loss_mask_7: 0.03335  loss_dice_7: 0.647  loss_ce_8: 0  loss_mask_8: 0.02954  loss_dice_8: 0.6352  time: 0.4149  data_time: 0.0077  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:27 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:12:27 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:12:27 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:12:27 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:12:27 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:12:27 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:12:28 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.263710 (0.263710 s / iter per device, on 1 devices)
[32m[06/02 14:12:28 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.203102 s / iter per device, on 1 devices)
[32m[06/02 14:12:28 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:12:28 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:12:28 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:12:28 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:12:28 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:12:28 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:12:28 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:12:28 d2.utils.events]: [0m eta: 0:42:52  iter: 3799  total_loss: 6.884  loss_ce: 4.099e-07  loss_mask: 0.0363  loss_dice: 0.6363  loss_ce_0: 7.768e-05  loss_mask_0: 0.03468  loss_dice_0: 0.6218  loss_ce_1: 1.894e-07  loss_mask_1: 0.03566  loss_dice_1: 0.6982  loss_ce_2: 2.187e-09  loss_mask_2: 0.03841  loss_dice_2: 0.7161  loss_ce_3: 2.187e-09  loss_mask_3: 0.03417  loss_dice_3: 0.6417  loss_ce_4: 8.749e-09  loss_mask_4: 0.03544  loss_dice_4: 0.6791  loss_ce_5: 2.187e-09  loss_mask_5: 0.03189  loss_dice_5: 0.673  loss_ce_6: 0  loss_mask_6: 0.03462  loss_dice_6: 0.6199  loss_ce_7: 2.187e-09  loss_mask_7: 0.03288  loss_dice_7: 0.5699  loss_ce_8: 0  loss_mask_8: 0.03486  loss_dice_8: 0.6171  time: 0.4149  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:36 d2.utils.events]: [0m eta: 0:42:44  iter: 3819  total_loss: 6.209  loss_ce: 6.526e-07  loss_mask: 0.03134  loss_dice: 0.5647  loss_ce_0: 7.658e-05  loss_mask_0: 0.03392  loss_dice_0: 0.6081  loss_ce_1: 1.439e-07  loss_mask_1: 0.03053  loss_dice_1: 0.5851  loss_ce_2: 2.187e-09  loss_mask_2: 0.03377  loss_dice_2: 0.5926  loss_ce_3: 2.187e-09  loss_mask_3: 0.03507  loss_dice_3: 0.6026  loss_ce_4: 8.749e-09  loss_mask_4: 0.03438  loss_dice_4: 0.5607  loss_ce_5: 2.187e-09  loss_mask_5: 0.03241  loss_dice_5: 0.6203  loss_ce_6: 0  loss_mask_6: 0.03348  loss_dice_6: 0.5528  loss_ce_7: 2.187e-09  loss_mask_7: 0.03174  loss_dice_7: 0.5613  loss_ce_8: 0  loss_mask_8: 0.03123  loss_dice_8: 0.5788  time: 0.4148  data_time: 0.0070  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:44 d2.utils.events]: [0m eta: 0:42:35  iter: 3839  total_loss: 6.141  loss_ce: 5.745e-07  loss_mask: 0.03318  loss_dice: 0.5973  loss_ce_0: 7.511e-05  loss_mask_0: 0.03206  loss_dice_0: 0.5933  loss_ce_1: 1.909e-07  loss_mask_1: 0.03447  loss_dice_1: 0.5588  loss_ce_2: 2.187e-09  loss_mask_2: 0.03512  loss_dice_2: 0.608  loss_ce_3: 2.187e-09  loss_mask_3: 0.03357  loss_dice_3: 0.6046  loss_ce_4: 6.562e-09  loss_mask_4: 0.03496  loss_dice_4: 0.6155  loss_ce_5: 2.187e-09  loss_mask_5: 0.03333  loss_dice_5: 0.6201  loss_ce_6: 0  loss_mask_6: 0.03542  loss_dice_6: 0.6144  loss_ce_7: 2.187e-09  loss_mask_7: 0.03168  loss_dice_7: 0.5721  loss_ce_8: 0  loss_mask_8: 0.03795  loss_dice_8: 0.5874  time: 0.4148  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:12:48 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:12:48 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:12:48 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:12:48 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:12:48 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:12:48 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:12:49 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.290743 (0.290743 s / iter per device, on 1 devices)
[32m[06/02 14:12:49 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.225534 s / iter per device, on 1 devices)
[32m[06/02 14:12:49 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:12:49 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:12:49 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:12:49 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:12:49 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:12:49 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:12:49 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:12:53 d2.utils.events]: [0m eta: 0:42:26  iter: 3859  total_loss: 6.529  loss_ce: 3.28e-07  loss_mask: 0.0313  loss_dice: 0.6065  loss_ce_0: 7.373e-05  loss_mask_0: 0.03215  loss_dice_0: 0.5908  loss_ce_1: 1.841e-07  loss_mask_1: 0.02952  loss_dice_1: 0.5824  loss_ce_2: 2.187e-09  loss_mask_2: 0.03455  loss_dice_2: 0.6257  loss_ce_3: 2.406e-09  loss_mask_3: 0.03069  loss_dice_3: 0.6026  loss_ce_4: 4.375e-09  loss_mask_4: 0.03435  loss_dice_4: 0.5679  loss_ce_5: 2.187e-09  loss_mask_5: 0.03567  loss_dice_5: 0.6585  loss_ce_6: 0  loss_mask_6: 0.03179  loss_dice_6: 0.5658  loss_ce_7: 2.187e-09  loss_mask_7: 0.03386  loss_dice_7: 0.6151  loss_ce_8: 0  loss_mask_8: 0.03289  loss_dice_8: 0.5833  time: 0.4148  data_time: 0.0069  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:02 d2.utils.events]: [0m eta: 0:42:18  iter: 3879  total_loss: 6.399  loss_ce: 1.239e-07  loss_mask: 0.03572  loss_dice: 0.616  loss_ce_0: 7.262e-05  loss_mask_0: 0.03399  loss_dice_0: 0.6448  loss_ce_1: 1.25e-07  loss_mask_1: 0.03425  loss_dice_1: 0.5853  loss_ce_2: 2.187e-09  loss_mask_2: 0.03913  loss_dice_2: 0.6278  loss_ce_3: 2.187e-09  loss_mask_3: 0.0316  loss_dice_3: 0.6002  loss_ce_4: 4.375e-09  loss_mask_4: 0.03612  loss_dice_4: 0.6332  loss_ce_5: 2.187e-09  loss_mask_5: 0.03276  loss_dice_5: 0.6116  loss_ce_6: 0  loss_mask_6: 0.03611  loss_dice_6: 0.6153  loss_ce_7: 2.187e-09  loss_mask_7: 0.03384  loss_dice_7: 0.5589  loss_ce_8: 0  loss_mask_8: 0.03795  loss_dice_8: 0.6875  time: 0.4148  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:10 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:13:10 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:13:10 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:13:10 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:13:10 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:13:10 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:13:11 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.254552 (0.254552 s / iter per device, on 1 devices)
[32m[06/02 14:13:11 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.195716 s / iter per device, on 1 devices)
[32m[06/02 14:13:11 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:13:11 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:13:11 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:13:11 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:13:11 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:13:11 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:13:11 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:13:11 d2.utils.events]: [0m eta: 0:42:10  iter: 3899  total_loss: 6.207  loss_ce: 1.306e-07  loss_mask: 0.028  loss_dice: 0.5292  loss_ce_0: 7.18e-05  loss_mask_0: 0.0321  loss_dice_0: 0.5984  loss_ce_1: 9.908e-08  loss_mask_1: 0.03827  loss_dice_1: 0.6164  loss_ce_2: 2.187e-09  loss_mask_2: 0.03138  loss_dice_2: 0.5886  loss_ce_3: 2.187e-09  loss_mask_3: 0.03268  loss_dice_3: 0.5801  loss_ce_4: 4.375e-09  loss_mask_4: 0.02986  loss_dice_4: 0.5654  loss_ce_5: 2.187e-09  loss_mask_5: 0.03319  loss_dice_5: 0.5331  loss_ce_6: 0  loss_mask_6: 0.0374  loss_dice_6: 0.6192  loss_ce_7: 2.187e-09  loss_mask_7: 0.03224  loss_dice_7: 0.5838  loss_ce_8: 0  loss_mask_8: 0.03266  loss_dice_8: 0.5648  time: 0.4148  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:19 d2.utils.events]: [0m eta: 0:42:02  iter: 3919  total_loss: 6.697  loss_ce: 9.209e-08  loss_mask: 0.03526  loss_dice: 0.6022  loss_ce_0: 7.062e-05  loss_mask_0: 0.03521  loss_dice_0: 0.621  loss_ce_1: 1.097e-07  loss_mask_1: 0.03284  loss_dice_1: 0.6285  loss_ce_2: 2.187e-09  loss_mask_2: 0.03189  loss_dice_2: 0.6282  loss_ce_3: 2.187e-09  loss_mask_3: 0.03531  loss_dice_3: 0.6791  loss_ce_4: 4.375e-09  loss_mask_4: 0.03428  loss_dice_4: 0.612  loss_ce_5: 2.187e-09  loss_mask_5: 0.03346  loss_dice_5: 0.667  loss_ce_6: 0  loss_mask_6: 0.03272  loss_dice_6: 0.684  loss_ce_7: 0  loss_mask_7: 0.03088  loss_dice_7: 0.6155  loss_ce_8: 0  loss_mask_8: 0.03708  loss_dice_8: 0.6474  time: 0.4148  data_time: 0.0075  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:27 d2.utils.events]: [0m eta: 0:41:53  iter: 3939  total_loss: 6.369  loss_ce: 1.521e-07  loss_mask: 0.03563  loss_dice: 0.6149  loss_ce_0: 6.985e-05  loss_mask_0: 0.03206  loss_dice_0: 0.6206  loss_ce_1: 7.437e-08  loss_mask_1: 0.0354  loss_dice_1: 0.5836  loss_ce_2: 2.187e-09  loss_mask_2: 0.03179  loss_dice_2: 0.5663  loss_ce_3: 2.187e-09  loss_mask_3: 0.03234  loss_dice_3: 0.5775  loss_ce_4: 6.562e-09  loss_mask_4: 0.03385  loss_dice_4: 0.5515  loss_ce_5: 2.187e-09  loss_mask_5: 0.03415  loss_dice_5: 0.603  loss_ce_6: 0  loss_mask_6: 0.03599  loss_dice_6: 0.5975  loss_ce_7: 0  loss_mask_7: 0.03219  loss_dice_7: 0.5696  loss_ce_8: 0  loss_mask_8: 0.03118  loss_dice_8: 0.6053  time: 0.4147  data_time: 0.0072  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:31 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:13:31 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:13:31 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:13:31 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:13:31 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:13:31 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:13:32 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278943 (0.278943 s / iter per device, on 1 devices)
[32m[06/02 14:13:32 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.220461 s / iter per device, on 1 devices)
[32m[06/02 14:13:32 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:13:32 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:13:32 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:13:32 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:13:32 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:13:32 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:13:32 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:13:36 d2.utils.events]: [0m eta: 0:41:44  iter: 3959  total_loss: 6.363  loss_ce: 1.18e-07  loss_mask: 0.03217  loss_dice: 0.5718  loss_ce_0: 6.868e-05  loss_mask_0: 0.0319  loss_dice_0: 0.6233  loss_ce_1: 6.551e-08  loss_mask_1: 0.03303  loss_dice_1: 0.5747  loss_ce_2: 2.187e-09  loss_mask_2: 0.03388  loss_dice_2: 0.5859  loss_ce_3: 2.187e-09  loss_mask_3: 0.03557  loss_dice_3: 0.6388  loss_ce_4: 6.562e-09  loss_mask_4: 0.03672  loss_dice_4: 0.5905  loss_ce_5: 2.187e-09  loss_mask_5: 0.03606  loss_dice_5: 0.6266  loss_ce_6: 0  loss_mask_6: 0.03105  loss_dice_6: 0.5775  loss_ce_7: 0  loss_mask_7: 0.03699  loss_dice_7: 0.6655  loss_ce_8: 0  loss_mask_8: 0.03379  loss_dice_8: 0.5725  time: 0.4147  data_time: 0.0071  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:44 d2.utils.events]: [0m eta: 0:41:35  iter: 3979  total_loss: 6.461  loss_ce: 6.343e-08  loss_mask: 0.03386  loss_dice: 0.6143  loss_ce_0: 6.722e-05  loss_mask_0: 0.03292  loss_dice_0: 0.5376  loss_ce_1: 6.496e-08  loss_mask_1: 0.03138  loss_dice_1: 0.5876  loss_ce_2: 2.187e-09  loss_mask_2: 0.03323  loss_dice_2: 0.6496  loss_ce_3: 2.187e-09  loss_mask_3: 0.03523  loss_dice_3: 0.5965  loss_ce_4: 4.375e-09  loss_mask_4: 0.03593  loss_dice_4: 0.6135  loss_ce_5: 2.187e-09  loss_mask_5: 0.03341  loss_dice_5: 0.6101  loss_ce_6: 0  loss_mask_6: 0.03185  loss_dice_6: 0.5582  loss_ce_7: 0  loss_mask_7: 0.03542  loss_dice_7: 0.6268  loss_ce_8: 0  loss_mask_8: 0.03439  loss_dice_8: 0.6257  time: 0.4147  data_time: 0.0073  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:13:53 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0003999.pth
[32m[06/02 14:13:56 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:13:56 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:13:56 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:13:56 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:13:56 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:13:56 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:13:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.284332 (0.284332 s / iter per device, on 1 devices)
[32m[06/02 14:13:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.219680 s / iter per device, on 1 devices)
[32m[06/02 14:13:57 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:13:57 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.925
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:13:57 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP   |  AP50   |  AP75   |  APs   |  APm  |  APl  |  AR1   |  AR10  |
|:------:|:-------:|:-------:|:------:|:-----:|:-----:|:------:|:------:|
| 92.525 | 100.000 | 100.000 | 92.525 |  nan  |  nan  | 95.000 | 95.000 |
[32m[06/02 14:13:57 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:13:57 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:13:57 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:13:57 d2.evaluation.testing]: [0mcopypaste: 92.5248,100.0000,100.0000,92.5248,nan,nan,95.0000,95.0000
[32m[06/02 14:13:57 d2.utils.events]: [0m eta: 0:41:26  iter: 3999  total_loss: 6.061  loss_ce: 5.545e-08  loss_mask: 0.03326  loss_dice: 0.5719  loss_ce_0: 6.649e-05  loss_mask_0: 0.03252  loss_dice_0: 0.5574  loss_ce_1: 6.671e-08  loss_mask_1: 0.03172  loss_dice_1: 0.5404  loss_ce_2: 2.187e-09  loss_mask_2: 0.03558  loss_dice_2: 0.6214  loss_ce_3: 2.187e-09  loss_mask_3: 0.031  loss_dice_3: 0.5678  loss_ce_4: 2.187e-09  loss_mask_4: 0.0316  loss_dice_4: 0.6122  loss_ce_5: 2.187e-09  loss_mask_5: 0.03591  loss_dice_5: 0.5847  loss_ce_6: 0  loss_mask_6: 0.0331  loss_dice_6: 0.5719  loss_ce_7: 0  loss_mask_7: 0.02964  loss_dice_7: 0.5248  loss_ce_8: 0  loss_mask_8: 0.0348  loss_dice_8: 0.6284  time: 0.4147  data_time: 0.0076  lr: 0.0005  max_mem: 5953M
[32m[06/02 14:14:05 d2.utils.events]: [0m eta: 0:41:17  iter: 4019  total_loss: 5.999  loss_ce: 6.004e-08  loss_mask: 0.03056  loss_dice: 0.5648  loss_ce_0: 6.571e-05  loss_mask_0: 0.03448  loss_dice_0: 0.5845  loss_ce_1: 6.606e-08  loss_mask_1: 0.03057  loss_dice_1: 0.5612  loss_ce_2: 2.187e-09  loss_mask_2: 0.0312  loss_dice_2: 0.5661  loss_ce_3: 2.187e-09  loss_mask_3: 0.0294  loss_dice_3: 0.5466  loss_ce_4: 2.187e-09  loss_mask_4: 0.03064  loss_dice_4: 0.5179  loss_ce_5: 0  loss_mask_5: 0.03141  loss_dice_5: 0.5738  loss_ce_6: 0  loss_mask_6: 0.03075  loss_dice_6: 0.5248  loss_ce_7: 0  loss_mask_7: 0.03095  loss_dice_7: 0.5477  loss_ce_8: 0  loss_mask_8: 0.02964  loss_dice_8: 0.5682  time: 0.4146  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:14:13 d2.utils.events]: [0m eta: 0:41:08  iter: 4039  total_loss: 5.997  loss_ce: 5.14e-08  loss_mask: 0.0329  loss_dice: 0.5634  loss_ce_0: 6.555e-05  loss_mask_0: 0.0311  loss_dice_0: 0.5221  loss_ce_1: 6.431e-08  loss_mask_1: 0.03261  loss_dice_1: 0.5615  loss_ce_2: 2.187e-09  loss_mask_2: 0.0301  loss_dice_2: 0.573  loss_ce_3: 2.187e-09  loss_mask_3: 0.02881  loss_dice_3: 0.5579  loss_ce_4: 2.187e-09  loss_mask_4: 0.03173  loss_dice_4: 0.5833  loss_ce_5: 1.094e-09  loss_mask_5: 0.03138  loss_dice_5: 0.5366  loss_ce_6: 0  loss_mask_6: 0.03441  loss_dice_6: 0.5574  loss_ce_7: 0  loss_mask_7: 0.03299  loss_dice_7: 0.5552  loss_ce_8: 0  loss_mask_8: 0.03274  loss_dice_8: 0.551  time: 0.4146  data_time: 0.0081  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:14:18 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:14:18 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:14:18 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:14:18 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:14:18 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:14:18 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:14:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266518 (0.266518 s / iter per device, on 1 devices)
[32m[06/02 14:14:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.201549 s / iter per device, on 1 devices)
[32m[06/02 14:14:18 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:14:18 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:14:18 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:14:18 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:14:18 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:14:18 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:14:18 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:14:22 d2.utils.events]: [0m eta: 0:40:59  iter: 4059  total_loss: 5.991  loss_ce: 5.107e-08  loss_mask: 0.03565  loss_dice: 0.5805  loss_ce_0: 6.547e-05  loss_mask_0: 0.03205  loss_dice_0: 0.5663  loss_ce_1: 6.453e-08  loss_mask_1: 0.0317  loss_dice_1: 0.5868  loss_ce_2: 2.187e-09  loss_mask_2: 0.03102  loss_dice_2: 0.5629  loss_ce_3: 2.187e-09  loss_mask_3: 0.03124  loss_dice_3: 0.5497  loss_ce_4: 2.187e-09  loss_mask_4: 0.02956  loss_dice_4: 0.573  loss_ce_5: 2.187e-09  loss_mask_5: 0.03235  loss_dice_5: 0.5708  loss_ce_6: 0  loss_mask_6: 0.033  loss_dice_6: 0.5694  loss_ce_7: 0  loss_mask_7: 0.03208  loss_dice_7: 0.5357  loss_ce_8: 0  loss_mask_8: 0.03313  loss_dice_8: 0.5715  time: 0.4146  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:14:31 d2.utils.events]: [0m eta: 0:40:50  iter: 4079  total_loss: 5.846  loss_ce: 5.096e-08  loss_mask: 0.03314  loss_dice: 0.5861  loss_ce_0: 6.531e-05  loss_mask_0: 0.03287  loss_dice_0: 0.5975  loss_ce_1: 6.431e-08  loss_mask_1: 0.03093  loss_dice_1: 0.5552  loss_ce_2: 2.187e-09  loss_mask_2: 0.03324  loss_dice_2: 0.5868  loss_ce_3: 2.187e-09  loss_mask_3: 0.03468  loss_dice_3: 0.5762  loss_ce_4: 2.187e-09  loss_mask_4: 0.03051  loss_dice_4: 0.5333  loss_ce_5: 2.187e-09  loss_mask_5: 0.03404  loss_dice_5: 0.5682  loss_ce_6: 0  loss_mask_6: 0.0307  loss_dice_6: 0.4996  loss_ce_7: 0  loss_mask_7: 0.03176  loss_dice_7: 0.5483  loss_ce_8: 0  loss_mask_8: 0.03456  loss_dice_8: 0.5517  time: 0.4146  data_time: 0.0085  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:14:39 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:14:39 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:14:39 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:14:39 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:14:39 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:14:39 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:14:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.295113 (0.295113 s / iter per device, on 1 devices)
[32m[06/02 14:14:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.233770 s / iter per device, on 1 devices)
[32m[06/02 14:14:40 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:14:40 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:14:40 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:14:40 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:14:40 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:14:40 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:14:40 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:14:40 d2.utils.events]: [0m eta: 0:40:41  iter: 4099  total_loss: 6.097  loss_ce: 4.714e-08  loss_mask: 0.03309  loss_dice: 0.555  loss_ce_0: 6.511e-05  loss_mask_0: 0.03139  loss_dice_0: 0.5163  loss_ce_1: 6.256e-08  loss_mask_1: 0.03186  loss_dice_1: 0.5551  loss_ce_2: 2.187e-09  loss_mask_2: 0.03336  loss_dice_2: 0.5656  loss_ce_3: 2.187e-09  loss_mask_3: 0.0338  loss_dice_3: 0.5662  loss_ce_4: 2.187e-09  loss_mask_4: 0.0327  loss_dice_4: 0.5741  loss_ce_5: 2.187e-09  loss_mask_5: 0.03231  loss_dice_5: 0.5807  loss_ce_6: 0  loss_mask_6: 0.03172  loss_dice_6: 0.5786  loss_ce_7: 0  loss_mask_7: 0.03143  loss_dice_7: 0.5712  loss_ce_8: 0  loss_mask_8: 0.03092  loss_dice_8: 0.5708  time: 0.4146  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:14:48 d2.utils.events]: [0m eta: 0:40:33  iter: 4119  total_loss: 5.789  loss_ce: 5.337e-08  loss_mask: 0.03298  loss_dice: 0.5775  loss_ce_0: 6.496e-05  loss_mask_0: 0.03391  loss_dice_0: 0.5689  loss_ce_1: 6.19e-08  loss_mask_1: 0.03407  loss_dice_1: 0.5627  loss_ce_2: 2.187e-09  loss_mask_2: 0.03264  loss_dice_2: 0.5693  loss_ce_3: 2.187e-09  loss_mask_3: 0.03166  loss_dice_3: 0.575  loss_ce_4: 2.187e-09  loss_mask_4: 0.03075  loss_dice_4: 0.5504  loss_ce_5: 2.187e-09  loss_mask_5: 0.03295  loss_dice_5: 0.5359  loss_ce_6: 0  loss_mask_6: 0.03227  loss_dice_6: 0.5537  loss_ce_7: 0  loss_mask_7: 0.02983  loss_dice_7: 0.5437  loss_ce_8: 0  loss_mask_8: 0.03241  loss_dice_8: 0.5578  time: 0.4145  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:14:56 d2.utils.events]: [0m eta: 0:40:22  iter: 4139  total_loss: 6.021  loss_ce: 5.195e-08  loss_mask: 0.03189  loss_dice: 0.532  loss_ce_0: 6.486e-05  loss_mask_0: 0.03195  loss_dice_0: 0.5557  loss_ce_1: 6.365e-08  loss_mask_1: 0.03275  loss_dice_1: 0.5807  loss_ce_2: 2.187e-09  loss_mask_2: 0.03186  loss_dice_2: 0.5932  loss_ce_3: 2.187e-09  loss_mask_3: 0.03176  loss_dice_3: 0.5604  loss_ce_4: 2.187e-09  loss_mask_4: 0.03155  loss_dice_4: 0.5724  loss_ce_5: 2.187e-09  loss_mask_5: 0.03118  loss_dice_5: 0.5263  loss_ce_6: 0  loss_mask_6: 0.03323  loss_dice_6: 0.5992  loss_ce_7: 0  loss_mask_7: 0.03249  loss_dice_7: 0.5661  loss_ce_8: 0  loss_mask_8: 0.03117  loss_dice_8: 0.5591  time: 0.4145  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:00 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:15:00 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:15:00 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:15:00 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:15:00 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:15:00 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:15:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273319 (0.273319 s / iter per device, on 1 devices)
[32m[06/02 14:15:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212689 s / iter per device, on 1 devices)
[32m[06/02 14:15:01 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:15:01 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:15:01 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:15:01 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:15:01 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:15:01 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:15:01 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:15:05 d2.utils.events]: [0m eta: 0:40:14  iter: 4159  total_loss: 5.695  loss_ce: 6.135e-08  loss_mask: 0.03005  loss_dice: 0.5109  loss_ce_0: 6.476e-05  loss_mask_0: 0.03137  loss_dice_0: 0.523  loss_ce_1: 6.343e-08  loss_mask_1: 0.03451  loss_dice_1: 0.5727  loss_ce_2: 2.187e-09  loss_mask_2: 0.03177  loss_dice_2: 0.5393  loss_ce_3: 2.187e-09  loss_mask_3: 0.03193  loss_dice_3: 0.5609  loss_ce_4: 2.187e-09  loss_mask_4: 0.0311  loss_dice_4: 0.5349  loss_ce_5: 2.187e-09  loss_mask_5: 0.03238  loss_dice_5: 0.5621  loss_ce_6: 0  loss_mask_6: 0.03086  loss_dice_6: 0.4929  loss_ce_7: 0  loss_mask_7: 0.03229  loss_dice_7: 0.5568  loss_ce_8: 0  loss_mask_8: 0.03292  loss_dice_8: 0.5888  time: 0.4145  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:14 d2.utils.events]: [0m eta: 0:40:05  iter: 4179  total_loss: 5.711  loss_ce: 5.25e-08  loss_mask: 0.03342  loss_dice: 0.5395  loss_ce_0: 6.465e-05  loss_mask_0: 0.03269  loss_dice_0: 0.4866  loss_ce_1: 6.912e-08  loss_mask_1: 0.02952  loss_dice_1: 0.5573  loss_ce_2: 2.187e-09  loss_mask_2: 0.03127  loss_dice_2: 0.5227  loss_ce_3: 2.187e-09  loss_mask_3: 0.03033  loss_dice_3: 0.534  loss_ce_4: 2.187e-09  loss_mask_4: 0.0345  loss_dice_4: 0.5825  loss_ce_5: 2.187e-09  loss_mask_5: 0.03161  loss_dice_5: 0.5324  loss_ce_6: 0  loss_mask_6: 0.03131  loss_dice_6: 0.5381  loss_ce_7: 0  loss_mask_7: 0.03513  loss_dice_7: 0.5963  loss_ce_8: 0  loss_mask_8: 0.03282  loss_dice_8: 0.5476  time: 0.4145  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:22 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:15:22 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:15:22 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:15:22 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:15:22 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:15:22 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:15:23 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259222 (0.259222 s / iter per device, on 1 devices)
[32m[06/02 14:15:23 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.196813 s / iter per device, on 1 devices)
[32m[06/02 14:15:23 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:15:23 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:15:23 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:15:23 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:15:23 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:15:23 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:15:23 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:15:23 d2.utils.events]: [0m eta: 0:39:57  iter: 4199  total_loss: 5.934  loss_ce: 5.95e-08  loss_mask: 0.02923  loss_dice: 0.5562  loss_ce_0: 6.444e-05  loss_mask_0: 0.03412  loss_dice_0: 0.5805  loss_ce_1: 6.431e-08  loss_mask_1: 0.03394  loss_dice_1: 0.5598  loss_ce_2: 2.187e-09  loss_mask_2: 0.03608  loss_dice_2: 0.6077  loss_ce_3: 2.187e-09  loss_mask_3: 0.03297  loss_dice_3: 0.5693  loss_ce_4: 2.187e-09  loss_mask_4: 0.03094  loss_dice_4: 0.5487  loss_ce_5: 0  loss_mask_5: 0.0354  loss_dice_5: 0.5943  loss_ce_6: 0  loss_mask_6: 0.03269  loss_dice_6: 0.5828  loss_ce_7: 0  loss_mask_7: 0.03481  loss_dice_7: 0.583  loss_ce_8: 0  loss_mask_8: 0.0296  loss_dice_8: 0.5621  time: 0.4145  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:31 d2.utils.events]: [0m eta: 0:39:47  iter: 4219  total_loss: 6.055  loss_ce: 5.632e-08  loss_mask: 0.03002  loss_dice: 0.546  loss_ce_0: 6.425e-05  loss_mask_0: 0.03615  loss_dice_0: 0.5875  loss_ce_1: 7.109e-08  loss_mask_1: 0.0319  loss_dice_1: 0.5577  loss_ce_2: 2.187e-09  loss_mask_2: 0.03176  loss_dice_2: 0.5528  loss_ce_3: 2.187e-09  loss_mask_3: 0.0342  loss_dice_3: 0.5807  loss_ce_4: 2.187e-09  loss_mask_4: 0.03104  loss_dice_4: 0.5495  loss_ce_5: 1.094e-09  loss_mask_5: 0.03373  loss_dice_5: 0.6019  loss_ce_6: 0  loss_mask_6: 0.03133  loss_dice_6: 0.5643  loss_ce_7: 0  loss_mask_7: 0.03165  loss_dice_7: 0.5394  loss_ce_8: 0  loss_mask_8: 0.03261  loss_dice_8: 0.5523  time: 0.4145  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:39 d2.utils.events]: [0m eta: 0:39:38  iter: 4239  total_loss: 5.92  loss_ce: 5.381e-08  loss_mask: 0.03381  loss_dice: 0.5975  loss_ce_0: 6.412e-05  loss_mask_0: 0.03429  loss_dice_0: 0.5699  loss_ce_1: 6.507e-08  loss_mask_1: 0.03198  loss_dice_1: 0.5607  loss_ce_2: 2.187e-09  loss_mask_2: 0.03208  loss_dice_2: 0.5697  loss_ce_3: 2.187e-09  loss_mask_3: 0.03076  loss_dice_3: 0.5781  loss_ce_4: 2.187e-09  loss_mask_4: 0.03235  loss_dice_4: 0.5257  loss_ce_5: 0  loss_mask_5: 0.03421  loss_dice_5: 0.5354  loss_ce_6: 0  loss_mask_6: 0.02824  loss_dice_6: 0.4833  loss_ce_7: 0  loss_mask_7: 0.0334  loss_dice_7: 0.5423  loss_ce_8: 0  loss_mask_8: 0.03182  loss_dice_8: 0.5748  time: 0.4145  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:43 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:15:43 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:15:43 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:15:43 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:15:43 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:15:43 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:15:44 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271316 (0.271316 s / iter per device, on 1 devices)
[32m[06/02 14:15:44 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213153 s / iter per device, on 1 devices)
[32m[06/02 14:15:44 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:15:44 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:15:44 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:15:44 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:15:44 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:15:44 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:15:44 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:15:48 d2.utils.events]: [0m eta: 0:39:28  iter: 4259  total_loss: 5.888  loss_ce: 5.064e-08  loss_mask: 0.03127  loss_dice: 0.6099  loss_ce_0: 6.394e-05  loss_mask_0: 0.03243  loss_dice_0: 0.5611  loss_ce_1: 6.649e-08  loss_mask_1: 0.03012  loss_dice_1: 0.5069  loss_ce_2: 2.187e-09  loss_mask_2: 0.03038  loss_dice_2: 0.5408  loss_ce_3: 2.187e-09  loss_mask_3: 0.03461  loss_dice_3: 0.5979  loss_ce_4: 2.187e-09  loss_mask_4: 0.03099  loss_dice_4: 0.5725  loss_ce_5: 0  loss_mask_5: 0.02998  loss_dice_5: 0.5824  loss_ce_6: 0  loss_mask_6: 0.03107  loss_dice_6: 0.5444  loss_ce_7: 0  loss_mask_7: 0.03294  loss_dice_7: 0.5564  loss_ce_8: 0  loss_mask_8: 0.03041  loss_dice_8: 0.5448  time: 0.4144  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:15:57 d2.utils.events]: [0m eta: 0:39:19  iter: 4279  total_loss: 5.979  loss_ce: 4.495e-08  loss_mask: 0.032  loss_dice: 0.58  loss_ce_0: 6.375e-05  loss_mask_0: 0.03425  loss_dice_0: 0.5902  loss_ce_1: 6.737e-08  loss_mask_1: 0.03333  loss_dice_1: 0.5949  loss_ce_2: 2.187e-09  loss_mask_2: 0.03362  loss_dice_2: 0.541  loss_ce_3: 2.187e-09  loss_mask_3: 0.03104  loss_dice_3: 0.6044  loss_ce_4: 2.187e-09  loss_mask_4: 0.03253  loss_dice_4: 0.5289  loss_ce_5: 0  loss_mask_5: 0.03271  loss_dice_5: 0.5834  loss_ce_6: 0  loss_mask_6: 0.03213  loss_dice_6: 0.5485  loss_ce_7: 0  loss_mask_7: 0.0324  loss_dice_7: 0.5428  loss_ce_8: 0  loss_mask_8: 0.03228  loss_dice_8: 0.6058  time: 0.4144  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:05 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:16:05 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:16:05 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:16:05 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:16:05 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:16:05 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:16:06 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.275822 (0.275822 s / iter per device, on 1 devices)
[32m[06/02 14:16:06 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215239 s / iter per device, on 1 devices)
[32m[06/02 14:16:06 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:16:06 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:16:06 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:16:06 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:16:06 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:16:06 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:16:06 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:16:06 d2.utils.events]: [0m eta: 0:39:10  iter: 4299  total_loss: 5.877  loss_ce: 4.911e-08  loss_mask: 0.02939  loss_dice: 0.5627  loss_ce_0: 6.363e-05  loss_mask_0: 0.03412  loss_dice_0: 0.5358  loss_ce_1: 7.393e-08  loss_mask_1: 0.032  loss_dice_1: 0.5172  loss_ce_2: 2.187e-09  loss_mask_2: 0.03161  loss_dice_2: 0.5307  loss_ce_3: 2.187e-09  loss_mask_3: 0.02959  loss_dice_3: 0.5388  loss_ce_4: 2.187e-09  loss_mask_4: 0.03468  loss_dice_4: 0.6189  loss_ce_5: 0  loss_mask_5: 0.03319  loss_dice_5: 0.5492  loss_ce_6: 0  loss_mask_6: 0.02908  loss_dice_6: 0.5406  loss_ce_7: 0  loss_mask_7: 0.03215  loss_dice_7: 0.5256  loss_ce_8: 0  loss_mask_8: 0.03049  loss_dice_8: 0.5646  time: 0.4144  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:14 d2.utils.events]: [0m eta: 0:39:00  iter: 4319  total_loss: 5.862  loss_ce: 4.867e-08  loss_mask: 0.02923  loss_dice: 0.5554  loss_ce_0: 6.35e-05  loss_mask_0: 0.03348  loss_dice_0: 0.6117  loss_ce_1: 7.426e-08  loss_mask_1: 0.02813  loss_dice_1: 0.5399  loss_ce_2: 2.187e-09  loss_mask_2: 0.03006  loss_dice_2: 0.5533  loss_ce_3: 2.187e-09  loss_mask_3: 0.0309  loss_dice_3: 0.5622  loss_ce_4: 2.187e-09  loss_mask_4: 0.03255  loss_dice_4: 0.5594  loss_ce_5: 2.187e-09  loss_mask_5: 0.03157  loss_dice_5: 0.5655  loss_ce_6: 0  loss_mask_6: 0.03259  loss_dice_6: 0.5298  loss_ce_7: 0  loss_mask_7: 0.031  loss_dice_7: 0.5534  loss_ce_8: 0  loss_mask_8: 0.03186  loss_dice_8: 0.518  time: 0.4144  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:22 d2.utils.events]: [0m eta: 0:38:51  iter: 4339  total_loss: 5.866  loss_ce: 5.304e-08  loss_mask: 0.02845  loss_dice: 0.5266  loss_ce_0: 6.342e-05  loss_mask_0: 0.03327  loss_dice_0: 0.5606  loss_ce_1: 6.781e-08  loss_mask_1: 0.03282  loss_dice_1: 0.5661  loss_ce_2: 2.187e-09  loss_mask_2: 0.03138  loss_dice_2: 0.5705  loss_ce_3: 2.187e-09  loss_mask_3: 0.03173  loss_dice_3: 0.5658  loss_ce_4: 2.187e-09  loss_mask_4: 0.03406  loss_dice_4: 0.5617  loss_ce_5: 2.187e-09  loss_mask_5: 0.03176  loss_dice_5: 0.5238  loss_ce_6: 0  loss_mask_6: 0.02879  loss_dice_6: 0.536  loss_ce_7: 0  loss_mask_7: 0.03416  loss_dice_7: 0.5643  loss_ce_8: 0  loss_mask_8: 0.03277  loss_dice_8: 0.5231  time: 0.4144  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:26 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:16:26 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:16:26 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:16:26 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:16:26 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:16:26 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:16:27 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.291930 (0.291930 s / iter per device, on 1 devices)
[32m[06/02 14:16:27 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.233115 s / iter per device, on 1 devices)
[32m[06/02 14:16:27 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:16:27 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:16:27 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:16:27 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:16:27 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:16:27 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:16:27 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:16:31 d2.utils.events]: [0m eta: 0:38:41  iter: 4359  total_loss: 5.808  loss_ce: 4.856e-08  loss_mask: 0.03305  loss_dice: 0.5375  loss_ce_0: 6.313e-05  loss_mask_0: 0.02995  loss_dice_0: 0.5215  loss_ce_1: 8.202e-08  loss_mask_1: 0.03207  loss_dice_1: 0.5569  loss_ce_2: 2.187e-09  loss_mask_2: 0.02852  loss_dice_2: 0.4943  loss_ce_3: 2.187e-09  loss_mask_3: 0.02949  loss_dice_3: 0.5475  loss_ce_4: 2.187e-09  loss_mask_4: 0.03272  loss_dice_4: 0.6037  loss_ce_5: 2.187e-09  loss_mask_5: 0.02872  loss_dice_5: 0.5212  loss_ce_6: 0  loss_mask_6: 0.03314  loss_dice_6: 0.5197  loss_ce_7: 0  loss_mask_7: 0.0308  loss_dice_7: 0.51  loss_ce_8: 0  loss_mask_8: 0.03156  loss_dice_8: 0.5225  time: 0.4144  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:40 d2.utils.events]: [0m eta: 0:38:32  iter: 4379  total_loss: 5.715  loss_ce: 4.779e-08  loss_mask: 0.02967  loss_dice: 0.5316  loss_ce_0: 6.305e-05  loss_mask_0: 0.03041  loss_dice_0: 0.5475  loss_ce_1: 6.77e-08  loss_mask_1: 0.03049  loss_dice_1: 0.5631  loss_ce_2: 2.187e-09  loss_mask_2: 0.03452  loss_dice_2: 0.5654  loss_ce_3: 2.187e-09  loss_mask_3: 0.02904  loss_dice_3: 0.5719  loss_ce_4: 2.187e-09  loss_mask_4: 0.03299  loss_dice_4: 0.5826  loss_ce_5: 0  loss_mask_5: 0.02669  loss_dice_5: 0.4991  loss_ce_6: 0  loss_mask_6: 0.02885  loss_dice_6: 0.5342  loss_ce_7: 0  loss_mask_7: 0.03067  loss_dice_7: 0.5071  loss_ce_8: 0  loss_mask_8: 0.03441  loss_dice_8: 0.549  time: 0.4143  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:48 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:16:48 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:16:48 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:16:48 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:16:48 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:16:48 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:16:48 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264328 (0.264328 s / iter per device, on 1 devices)
[32m[06/02 14:16:48 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.204184 s / iter per device, on 1 devices)
[32m[06/02 14:16:48 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:16:48 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:16:49 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:16:49 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:16:49 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:16:49 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:16:49 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:16:49 d2.utils.events]: [0m eta: 0:38:23  iter: 4399  total_loss: 5.827  loss_ce: 4.692e-08  loss_mask: 0.03164  loss_dice: 0.5725  loss_ce_0: 6.295e-05  loss_mask_0: 0.02947  loss_dice_0: 0.5012  loss_ce_1: 8.093e-08  loss_mask_1: 0.03218  loss_dice_1: 0.5442  loss_ce_2: 2.187e-09  loss_mask_2: 0.02847  loss_dice_2: 0.4885  loss_ce_3: 2.187e-09  loss_mask_3: 0.03206  loss_dice_3: 0.5461  loss_ce_4: 2.187e-09  loss_mask_4: 0.03293  loss_dice_4: 0.5535  loss_ce_5: 0  loss_mask_5: 0.03339  loss_dice_5: 0.5349  loss_ce_6: 0  loss_mask_6: 0.03269  loss_dice_6: 0.564  loss_ce_7: 0  loss_mask_7: 0.03191  loss_dice_7: 0.5189  loss_ce_8: 0  loss_mask_8: 0.02918  loss_dice_8: 0.5166  time: 0.4143  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:16:57 d2.utils.events]: [0m eta: 0:38:16  iter: 4419  total_loss: 5.753  loss_ce: 4.593e-08  loss_mask: 0.02991  loss_dice: 0.5225  loss_ce_0: 6.268e-05  loss_mask_0: 0.03086  loss_dice_0: 0.5138  loss_ce_1: 7.951e-08  loss_mask_1: 0.03055  loss_dice_1: 0.5724  loss_ce_2: 2.187e-09  loss_mask_2: 0.03116  loss_dice_2: 0.5411  loss_ce_3: 2.187e-09  loss_mask_3: 0.03091  loss_dice_3: 0.5351  loss_ce_4: 2.187e-09  loss_mask_4: 0.03182  loss_dice_4: 0.5458  loss_ce_5: 0  loss_mask_5: 0.03336  loss_dice_5: 0.5461  loss_ce_6: 0  loss_mask_6: 0.03284  loss_dice_6: 0.5513  loss_ce_7: 0  loss_mask_7: 0.03116  loss_dice_7: 0.5249  loss_ce_8: 0  loss_mask_8: 0.03516  loss_dice_8: 0.5227  time: 0.4143  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:05 d2.utils.events]: [0m eta: 0:38:08  iter: 4439  total_loss: 5.736  loss_ce: 4.451e-08  loss_mask: 0.03162  loss_dice: 0.5729  loss_ce_0: 6.25e-05  loss_mask_0: 0.03039  loss_dice_0: 0.5182  loss_ce_1: 7.984e-08  loss_mask_1: 0.02917  loss_dice_1: 0.5302  loss_ce_2: 2.187e-09  loss_mask_2: 0.03201  loss_dice_2: 0.5273  loss_ce_3: 2.187e-09  loss_mask_3: 0.03068  loss_dice_3: 0.5503  loss_ce_4: 2.187e-09  loss_mask_4: 0.03453  loss_dice_4: 0.5435  loss_ce_5: 0  loss_mask_5: 0.03295  loss_dice_5: 0.5626  loss_ce_6: 0  loss_mask_6: 0.03196  loss_dice_6: 0.5873  loss_ce_7: 0  loss_mask_7: 0.0336  loss_dice_7: 0.5367  loss_ce_8: 0  loss_mask_8: 0.03096  loss_dice_8: 0.5796  time: 0.4143  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:09 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:17:09 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:17:09 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:17:09 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:17:09 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:17:09 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:17:10 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.281282 (0.281282 s / iter per device, on 1 devices)
[32m[06/02 14:17:10 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.219817 s / iter per device, on 1 devices)
[32m[06/02 14:17:10 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:17:10 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:17:10 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:17:10 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:17:10 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:17:10 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:17:10 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:17:14 d2.utils.events]: [0m eta: 0:38:00  iter: 4459  total_loss: 5.842  loss_ce: 4.67e-08  loss_mask: 0.03036  loss_dice: 0.5322  loss_ce_0: 6.23e-05  loss_mask_0: 0.03251  loss_dice_0: 0.5524  loss_ce_1: 6.715e-08  loss_mask_1: 0.03274  loss_dice_1: 0.5279  loss_ce_2: 2.187e-09  loss_mask_2: 0.032  loss_dice_2: 0.4983  loss_ce_3: 2.187e-09  loss_mask_3: 0.02939  loss_dice_3: 0.5269  loss_ce_4: 2.187e-09  loss_mask_4: 0.03359  loss_dice_4: 0.5402  loss_ce_5: 0  loss_mask_5: 0.0311  loss_dice_5: 0.5707  loss_ce_6: 0  loss_mask_6: 0.02905  loss_dice_6: 0.5266  loss_ce_7: 0  loss_mask_7: 0.03132  loss_dice_7: 0.5543  loss_ce_8: 0  loss_mask_8: 0.0337  loss_dice_8: 0.5933  time: 0.4143  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:23 d2.utils.events]: [0m eta: 0:37:53  iter: 4479  total_loss: 5.922  loss_ce: 4.834e-08  loss_mask: 0.03314  loss_dice: 0.5683  loss_ce_0: 6.216e-05  loss_mask_0: 0.03278  loss_dice_0: 0.5604  loss_ce_1: 6.617e-08  loss_mask_1: 0.02897  loss_dice_1: 0.541  loss_ce_2: 2.187e-09  loss_mask_2: 0.0318  loss_dice_2: 0.556  loss_ce_3: 2.187e-09  loss_mask_3: 0.03126  loss_dice_3: 0.5812  loss_ce_4: 2.187e-09  loss_mask_4: 0.02962  loss_dice_4: 0.5713  loss_ce_5: 0  loss_mask_5: 0.03125  loss_dice_5: 0.5554  loss_ce_6: 0  loss_mask_6: 0.0259  loss_dice_6: 0.5423  loss_ce_7: 0  loss_mask_7: 0.03108  loss_dice_7: 0.5568  loss_ce_8: 0  loss_mask_8: 0.03073  loss_dice_8: 0.5015  time: 0.4143  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:31 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:17:31 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:17:31 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:17:31 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:17:31 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:17:31 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:17:32 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268865 (0.268865 s / iter per device, on 1 devices)
[32m[06/02 14:17:32 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.204789 s / iter per device, on 1 devices)
[32m[06/02 14:17:32 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:17:32 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:17:32 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:17:32 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:17:32 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:17:32 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:17:32 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:17:32 d2.utils.events]: [0m eta: 0:37:46  iter: 4499  total_loss: 5.747  loss_ce: 4.276e-08  loss_mask: 0.0325  loss_dice: 0.5446  loss_ce_0: 6.208e-05  loss_mask_0: 0.03181  loss_dice_0: 0.5415  loss_ce_1: 6.409e-08  loss_mask_1: 0.02724  loss_dice_1: 0.461  loss_ce_2: 2.187e-09  loss_mask_2: 0.03102  loss_dice_2: 0.5341  loss_ce_3: 2.187e-09  loss_mask_3: 0.03014  loss_dice_3: 0.5598  loss_ce_4: 2.187e-09  loss_mask_4: 0.02962  loss_dice_4: 0.5592  loss_ce_5: 0  loss_mask_5: 0.03037  loss_dice_5: 0.5235  loss_ce_6: 0  loss_mask_6: 0.03232  loss_dice_6: 0.545  loss_ce_7: 0  loss_mask_7: 0.03078  loss_dice_7: 0.5563  loss_ce_8: 0  loss_mask_8: 0.03096  loss_dice_8: 0.5529  time: 0.4143  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:40 d2.utils.events]: [0m eta: 0:37:36  iter: 4519  total_loss: 5.823  loss_ce: 4.101e-08  loss_mask: 0.03076  loss_dice: 0.5032  loss_ce_0: 6.194e-05  loss_mask_0: 0.03316  loss_dice_0: 0.5727  loss_ce_1: 6.234e-08  loss_mask_1: 0.03151  loss_dice_1: 0.5263  loss_ce_2: 2.187e-09  loss_mask_2: 0.03392  loss_dice_2: 0.5681  loss_ce_3: 2.187e-09  loss_mask_3: 0.03108  loss_dice_3: 0.5298  loss_ce_4: 2.187e-09  loss_mask_4: 0.03092  loss_dice_4: 0.5474  loss_ce_5: 0  loss_mask_5: 0.03517  loss_dice_5: 0.5765  loss_ce_6: 0  loss_mask_6: 0.03124  loss_dice_6: 0.5466  loss_ce_7: 0  loss_mask_7: 0.03062  loss_dice_7: 0.5674  loss_ce_8: 0  loss_mask_8: 0.0305  loss_dice_8: 0.5502  time: 0.4143  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:48 d2.utils.events]: [0m eta: 0:37:27  iter: 4539  total_loss: 5.886  loss_ce: 4.003e-08  loss_mask: 0.02669  loss_dice: 0.5521  loss_ce_0: 6.176e-05  loss_mask_0: 0.03278  loss_dice_0: 0.5271  loss_ce_1: 7.098e-08  loss_mask_1: 0.03146  loss_dice_1: 0.5836  loss_ce_2: 2.187e-09  loss_mask_2: 0.03011  loss_dice_2: 0.5437  loss_ce_3: 2.187e-09  loss_mask_3: 0.03406  loss_dice_3: 0.6229  loss_ce_4: 2.187e-09  loss_mask_4: 0.0316  loss_dice_4: 0.5183  loss_ce_5: 0  loss_mask_5: 0.03112  loss_dice_5: 0.5353  loss_ce_6: 0  loss_mask_6: 0.03146  loss_dice_6: 0.5295  loss_ce_7: 0  loss_mask_7: 0.02967  loss_dice_7: 0.5637  loss_ce_8: 0  loss_mask_8: 0.03303  loss_dice_8: 0.5398  time: 0.4143  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:17:52 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:17:52 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:17:52 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:17:52 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:17:52 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:17:52 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:17:53 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.263991 (0.263991 s / iter per device, on 1 devices)
[32m[06/02 14:17:53 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.204236 s / iter per device, on 1 devices)
[32m[06/02 14:17:53 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:17:53 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:17:53 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:17:53 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:17:53 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:17:53 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:17:53 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:17:57 d2.utils.events]: [0m eta: 0:37:18  iter: 4559  total_loss: 5.662  loss_ce: 4.222e-08  loss_mask: 0.02945  loss_dice: 0.5567  loss_ce_0: 6.164e-05  loss_mask_0: 0.02887  loss_dice_0: 0.4993  loss_ce_1: 7.098e-08  loss_mask_1: 0.03243  loss_dice_1: 0.5435  loss_ce_2: 2.187e-09  loss_mask_2: 0.03106  loss_dice_2: 0.5669  loss_ce_3: 2.187e-09  loss_mask_3: 0.0312  loss_dice_3: 0.5921  loss_ce_4: 2.187e-09  loss_mask_4: 0.02922  loss_dice_4: 0.5119  loss_ce_5: 2.187e-09  loss_mask_5: 0.03004  loss_dice_5: 0.5552  loss_ce_6: 0  loss_mask_6: 0.03058  loss_dice_6: 0.5386  loss_ce_7: 0  loss_mask_7: 0.03209  loss_dice_7: 0.5267  loss_ce_8: 0  loss_mask_8: 0.03021  loss_dice_8: 0.5191  time: 0.4143  data_time: 0.0058  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:06 d2.utils.events]: [0m eta: 0:37:09  iter: 4579  total_loss: 5.955  loss_ce: 3.609e-08  loss_mask: 0.03212  loss_dice: 0.52  loss_ce_0: 6.146e-05  loss_mask_0: 0.03173  loss_dice_0: 0.5271  loss_ce_1: 6.31e-08  loss_mask_1: 0.0329  loss_dice_1: 0.581  loss_ce_2: 2.187e-09  loss_mask_2: 0.03273  loss_dice_2: 0.5652  loss_ce_3: 2.187e-09  loss_mask_3: 0.03255  loss_dice_3: 0.5529  loss_ce_4: 2.187e-09  loss_mask_4: 0.03081  loss_dice_4: 0.5555  loss_ce_5: 0  loss_mask_5: 0.03058  loss_dice_5: 0.5618  loss_ce_6: 0  loss_mask_6: 0.02912  loss_dice_6: 0.5161  loss_ce_7: 0  loss_mask_7: 0.03146  loss_dice_7: 0.5479  loss_ce_8: 0  loss_mask_8: 0.03361  loss_dice_8: 0.5772  time: 0.4143  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:14 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:18:14 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:18:14 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:18:14 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:18:14 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:18:14 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:18:15 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268497 (0.268497 s / iter per device, on 1 devices)
[32m[06/02 14:18:15 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208123 s / iter per device, on 1 devices)
[32m[06/02 14:18:15 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:18:15 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:18:15 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:18:15 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:18:15 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:18:15 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:18:15 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:18:15 d2.utils.events]: [0m eta: 0:37:01  iter: 4599  total_loss: 5.792  loss_ce: 3.543e-08  loss_mask: 0.03165  loss_dice: 0.5617  loss_ce_0: 6.125e-05  loss_mask_0: 0.03335  loss_dice_0: 0.5515  loss_ce_1: 7.623e-08  loss_mask_1: 0.03083  loss_dice_1: 0.5574  loss_ce_2: 2.187e-09  loss_mask_2: 0.03108  loss_dice_2: 0.5065  loss_ce_3: 2.187e-09  loss_mask_3: 0.03381  loss_dice_3: 0.5337  loss_ce_4: 2.187e-09  loss_mask_4: 0.02916  loss_dice_4: 0.5176  loss_ce_5: 0  loss_mask_5: 0.03242  loss_dice_5: 0.6111  loss_ce_6: 0  loss_mask_6: 0.03149  loss_dice_6: 0.5105  loss_ce_7: 0  loss_mask_7: 0.03149  loss_dice_7: 0.5372  loss_ce_8: 0  loss_mask_8: 0.03088  loss_dice_8: 0.5064  time: 0.4143  data_time: 0.0090  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:23 d2.utils.events]: [0m eta: 0:36:52  iter: 4619  total_loss: 5.785  loss_ce: 3.828e-08  loss_mask: 0.03113  loss_dice: 0.5395  loss_ce_0: 6.114e-05  loss_mask_0: 0.03127  loss_dice_0: 0.5298  loss_ce_1: 7.579e-08  loss_mask_1: 0.03408  loss_dice_1: 0.5489  loss_ce_2: 2.187e-09  loss_mask_2: 0.03335  loss_dice_2: 0.5589  loss_ce_3: 2.187e-09  loss_mask_3: 0.03233  loss_dice_3: 0.5366  loss_ce_4: 2.187e-09  loss_mask_4: 0.03474  loss_dice_4: 0.5825  loss_ce_5: 0  loss_mask_5: 0.03039  loss_dice_5: 0.5423  loss_ce_6: 0  loss_mask_6: 0.03317  loss_dice_6: 0.5352  loss_ce_7: 0  loss_mask_7: 0.02941  loss_dice_7: 0.5566  loss_ce_8: 0  loss_mask_8: 0.03399  loss_dice_8: 0.5654  time: 0.4143  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:31 d2.utils.events]: [0m eta: 0:36:44  iter: 4639  total_loss: 5.796  loss_ce: 4.079e-08  loss_mask: 0.03174  loss_dice: 0.561  loss_ce_0: 6.097e-05  loss_mask_0: 0.03142  loss_dice_0: 0.5231  loss_ce_1: 6.299e-08  loss_mask_1: 0.03364  loss_dice_1: 0.577  loss_ce_2: 2.187e-09  loss_mask_2: 0.02999  loss_dice_2: 0.5294  loss_ce_3: 2.187e-09  loss_mask_3: 0.03178  loss_dice_3: 0.5381  loss_ce_4: 2.187e-09  loss_mask_4: 0.03142  loss_dice_4: 0.5433  loss_ce_5: 0  loss_mask_5: 0.03125  loss_dice_5: 0.5559  loss_ce_6: 0  loss_mask_6: 0.03153  loss_dice_6: 0.5091  loss_ce_7: 0  loss_mask_7: 0.03411  loss_dice_7: 0.5876  loss_ce_8: 0  loss_mask_8: 0.03129  loss_dice_8: 0.5387  time: 0.4142  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:35 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:18:35 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:18:35 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:18:35 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:18:35 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:18:35 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:18:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.290155 (0.290155 s / iter per device, on 1 devices)
[32m[06/02 14:18:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.228128 s / iter per device, on 1 devices)
[32m[06/02 14:18:36 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:18:36 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:18:36 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:18:36 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:18:36 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:18:36 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:18:36 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:18:40 d2.utils.events]: [0m eta: 0:36:35  iter: 4659  total_loss: 5.693  loss_ce: 3.795e-08  loss_mask: 0.02968  loss_dice: 0.4903  loss_ce_0: 6.078e-05  loss_mask_0: 0.03197  loss_dice_0: 0.5758  loss_ce_1: 7.087e-08  loss_mask_1: 0.02956  loss_dice_1: 0.5324  loss_ce_2: 2.187e-09  loss_mask_2: 0.02999  loss_dice_2: 0.5387  loss_ce_3: 2.187e-09  loss_mask_3: 0.03459  loss_dice_3: 0.6  loss_ce_4: 2.187e-09  loss_mask_4: 0.03108  loss_dice_4: 0.5242  loss_ce_5: 1.094e-09  loss_mask_5: 0.02839  loss_dice_5: 0.5124  loss_ce_6: 0  loss_mask_6: 0.02801  loss_dice_6: 0.4861  loss_ce_7: 0  loss_mask_7: 0.02992  loss_dice_7: 0.5118  loss_ce_8: 0  loss_mask_8: 0.03095  loss_dice_8: 0.5619  time: 0.4142  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:49 d2.utils.events]: [0m eta: 0:36:27  iter: 4679  total_loss: 5.648  loss_ce: 3.97e-08  loss_mask: 0.03261  loss_dice: 0.5583  loss_ce_0: 6.062e-05  loss_mask_0: 0.03372  loss_dice_0: 0.5463  loss_ce_1: 6.135e-08  loss_mask_1: 0.03248  loss_dice_1: 0.5586  loss_ce_2: 2.187e-09  loss_mask_2: 0.02904  loss_dice_2: 0.5108  loss_ce_3: 2.187e-09  loss_mask_3: 0.0328  loss_dice_3: 0.5224  loss_ce_4: 2.187e-09  loss_mask_4: 0.03095  loss_dice_4: 0.5229  loss_ce_5: 0  loss_mask_5: 0.02979  loss_dice_5: 0.5428  loss_ce_6: 0  loss_mask_6: 0.03222  loss_dice_6: 0.5362  loss_ce_7: 0  loss_mask_7: 0.03334  loss_dice_7: 0.5578  loss_ce_8: 0  loss_mask_8: 0.03431  loss_dice_8: 0.5566  time: 0.4142  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:18:57 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:18:57 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:18:57 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:18:57 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:18:57 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:18:57 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:18:58 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264755 (0.264755 s / iter per device, on 1 devices)
[32m[06/02 14:18:58 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205561 s / iter per device, on 1 devices)
[32m[06/02 14:18:58 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:18:58 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:18:58 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:18:58 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:18:58 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:18:58 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:18:58 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:18:58 d2.utils.events]: [0m eta: 0:36:18  iter: 4699  total_loss: 5.688  loss_ce: 4.036e-08  loss_mask: 0.02946  loss_dice: 0.507  loss_ce_0: 6.05e-05  loss_mask_0: 0.0302  loss_dice_0: 0.5537  loss_ce_1: 6.813e-08  loss_mask_1: 0.02979  loss_dice_1: 0.5289  loss_ce_2: 2.187e-09  loss_mask_2: 0.03127  loss_dice_2: 0.5456  loss_ce_3: 2.187e-09  loss_mask_3: 0.03104  loss_dice_3: 0.5364  loss_ce_4: 2.187e-09  loss_mask_4: 0.03014  loss_dice_4: 0.551  loss_ce_5: 0  loss_mask_5: 0.03118  loss_dice_5: 0.5096  loss_ce_6: 0  loss_mask_6: 0.03525  loss_dice_6: 0.5883  loss_ce_7: 0  loss_mask_7: 0.03082  loss_dice_7: 0.5631  loss_ce_8: 0  loss_mask_8: 0.0317  loss_dice_8: 0.5428  time: 0.4142  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:06 d2.utils.events]: [0m eta: 0:36:09  iter: 4719  total_loss: 5.671  loss_ce: 4.036e-08  loss_mask: 0.03148  loss_dice: 0.534  loss_ce_0: 6.029e-05  loss_mask_0: 0.03147  loss_dice_0: 0.5228  loss_ce_1: 6.978e-08  loss_mask_1: 0.03081  loss_dice_1: 0.5527  loss_ce_2: 2.187e-09  loss_mask_2: 0.03182  loss_dice_2: 0.5191  loss_ce_3: 2.187e-09  loss_mask_3: 0.03023  loss_dice_3: 0.4897  loss_ce_4: 2.187e-09  loss_mask_4: 0.0307  loss_dice_4: 0.5601  loss_ce_5: 0  loss_mask_5: 0.03019  loss_dice_5: 0.5657  loss_ce_6: 0  loss_mask_6: 0.02897  loss_dice_6: 0.4793  loss_ce_7: 0  loss_mask_7: 0.02878  loss_dice_7: 0.5235  loss_ce_8: 0  loss_mask_8: 0.0313  loss_dice_8: 0.5512  time: 0.4142  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:14 d2.utils.events]: [0m eta: 0:36:01  iter: 4739  total_loss: 5.862  loss_ce: 3.948e-08  loss_mask: 0.0335  loss_dice: 0.5389  loss_ce_0: 6.003e-05  loss_mask_0: 0.03441  loss_dice_0: 0.5233  loss_ce_1: 7.787e-08  loss_mask_1: 0.03183  loss_dice_1: 0.5509  loss_ce_2: 2.187e-09  loss_mask_2: 0.03165  loss_dice_2: 0.5662  loss_ce_3: 2.187e-09  loss_mask_3: 0.03062  loss_dice_3: 0.5589  loss_ce_4: 2.187e-09  loss_mask_4: 0.03165  loss_dice_4: 0.567  loss_ce_5: 0  loss_mask_5: 0.02943  loss_dice_5: 0.5159  loss_ce_6: 0  loss_mask_6: 0.03142  loss_dice_6: 0.5341  loss_ce_7: 0  loss_mask_7: 0.03656  loss_dice_7: 0.588  loss_ce_8: 0  loss_mask_8: 0.03343  loss_dice_8: 0.5593  time: 0.4142  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:18 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:19:18 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:19:18 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:19:18 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:19:18 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:19:18 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:19:19 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259654 (0.259654 s / iter per device, on 1 devices)
[32m[06/02 14:19:19 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.201093 s / iter per device, on 1 devices)
[32m[06/02 14:19:19 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:19:19 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:19:19 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:19:19 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:19:19 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:19:19 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:19:19 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:19:23 d2.utils.events]: [0m eta: 0:35:52  iter: 4759  total_loss: 6.053  loss_ce: 4.101e-08  loss_mask: 0.03209  loss_dice: 0.5665  loss_ce_0: 5.988e-05  loss_mask_0: 0.03334  loss_dice_0: 0.58  loss_ce_1: 6.059e-08  loss_mask_1: 0.02817  loss_dice_1: 0.527  loss_ce_2: 2.187e-09  loss_mask_2: 0.03412  loss_dice_2: 0.5891  loss_ce_3: 2.187e-09  loss_mask_3: 0.0343  loss_dice_3: 0.5793  loss_ce_4: 2.187e-09  loss_mask_4: 0.03117  loss_dice_4: 0.5675  loss_ce_5: 0  loss_mask_5: 0.03144  loss_dice_5: 0.5921  loss_ce_6: 0  loss_mask_6: 0.03115  loss_dice_6: 0.5853  loss_ce_7: 0  loss_mask_7: 0.0304  loss_dice_7: 0.5204  loss_ce_8: 0  loss_mask_8: 0.02911  loss_dice_8: 0.495  time: 0.4142  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:31 d2.utils.events]: [0m eta: 0:35:44  iter: 4779  total_loss: 5.867  loss_ce: 4.309e-08  loss_mask: 0.03115  loss_dice: 0.5497  loss_ce_0: 5.982e-05  loss_mask_0: 0.03392  loss_dice_0: 0.5347  loss_ce_1: 5.949e-08  loss_mask_1: 0.03436  loss_dice_1: 0.5825  loss_ce_2: 2.187e-09  loss_mask_2: 0.03032  loss_dice_2: 0.528  loss_ce_3: 0  loss_mask_3: 0.03278  loss_dice_3: 0.5735  loss_ce_4: 2.187e-09  loss_mask_4: 0.02974  loss_dice_4: 0.5107  loss_ce_5: 0  loss_mask_5: 0.0316  loss_dice_5: 0.5531  loss_ce_6: 0  loss_mask_6: 0.03542  loss_dice_6: 0.6098  loss_ce_7: 0  loss_mask_7: 0.03093  loss_dice_7: 0.5415  loss_ce_8: 0  loss_mask_8: 0.0336  loss_dice_8: 0.5593  time: 0.4141  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:40 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:19:40 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:19:40 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:19:40 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:19:40 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:19:40 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:19:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268424 (0.268424 s / iter per device, on 1 devices)
[32m[06/02 14:19:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.204661 s / iter per device, on 1 devices)
[32m[06/02 14:19:40 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:19:40 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:19:40 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:19:40 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:19:40 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:19:40 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:19:40 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:19:40 d2.utils.events]: [0m eta: 0:35:36  iter: 4799  total_loss: 5.864  loss_ce: 4.025e-08  loss_mask: 0.03265  loss_dice: 0.5359  loss_ce_0: 5.967e-05  loss_mask_0: 0.03092  loss_dice_0: 0.5239  loss_ce_1: 6.967e-08  loss_mask_1: 0.03305  loss_dice_1: 0.5783  loss_ce_2: 2.187e-09  loss_mask_2: 0.03204  loss_dice_2: 0.6069  loss_ce_3: 0  loss_mask_3: 0.03194  loss_dice_3: 0.5739  loss_ce_4: 2.187e-09  loss_mask_4: 0.03419  loss_dice_4: 0.582  loss_ce_5: 0  loss_mask_5: 0.03044  loss_dice_5: 0.5559  loss_ce_6: 0  loss_mask_6: 0.03384  loss_dice_6: 0.5527  loss_ce_7: 0  loss_mask_7: 0.03109  loss_dice_7: 0.5551  loss_ce_8: 0  loss_mask_8: 0.02989  loss_dice_8: 0.5375  time: 0.4141  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:49 d2.utils.events]: [0m eta: 0:35:28  iter: 4819  total_loss: 5.873  loss_ce: 3.795e-08  loss_mask: 0.03294  loss_dice: 0.5126  loss_ce_0: 5.949e-05  loss_mask_0: 0.02758  loss_dice_0: 0.5236  loss_ce_1: 8.06e-08  loss_mask_1: 0.03188  loss_dice_1: 0.5592  loss_ce_2: 2.187e-09  loss_mask_2: 0.03203  loss_dice_2: 0.566  loss_ce_3: 0  loss_mask_3: 0.03272  loss_dice_3: 0.5857  loss_ce_4: 2.187e-09  loss_mask_4: 0.03195  loss_dice_4: 0.5256  loss_ce_5: 0  loss_mask_5: 0.03137  loss_dice_5: 0.5921  loss_ce_6: 0  loss_mask_6: 0.03169  loss_dice_6: 0.555  loss_ce_7: 0  loss_mask_7: 0.03162  loss_dice_7: 0.5744  loss_ce_8: 0  loss_mask_8: 0.03132  loss_dice_8: 0.547  time: 0.4141  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:19:57 d2.utils.events]: [0m eta: 0:35:20  iter: 4839  total_loss: 5.902  loss_ce: 3.915e-08  loss_mask: 0.03309  loss_dice: 0.5514  loss_ce_0: 5.939e-05  loss_mask_0: 0.0308  loss_dice_0: 0.5723  loss_ce_1: 8.093e-08  loss_mask_1: 0.02919  loss_dice_1: 0.5221  loss_ce_2: 2.187e-09  loss_mask_2: 0.03072  loss_dice_2: 0.5854  loss_ce_3: 0  loss_mask_3: 0.03288  loss_dice_3: 0.5669  loss_ce_4: 2.187e-09  loss_mask_4: 0.03034  loss_dice_4: 0.5289  loss_ce_5: 0  loss_mask_5: 0.0294  loss_dice_5: 0.5479  loss_ce_6: 0  loss_mask_6: 0.03223  loss_dice_6: 0.5442  loss_ce_7: 0  loss_mask_7: 0.03227  loss_dice_7: 0.5409  loss_ce_8: 0  loss_mask_8: 0.03165  loss_dice_8: 0.5853  time: 0.4141  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:01 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:20:01 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:20:01 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:20:01 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:20:01 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:20:01 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:20:02 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.250380 (0.250380 s / iter per device, on 1 devices)
[32m[06/02 14:20:02 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.190819 s / iter per device, on 1 devices)
[32m[06/02 14:20:02 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:20:02 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:20:02 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:20:02 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:20:02 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:20:02 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:20:02 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:20:06 d2.utils.events]: [0m eta: 0:35:11  iter: 4859  total_loss: 5.808  loss_ce: 4.189e-08  loss_mask: 0.03001  loss_dice: 0.5134  loss_ce_0: 5.92e-05  loss_mask_0: 0.02974  loss_dice_0: 0.5201  loss_ce_1: 6.256e-08  loss_mask_1: 0.03242  loss_dice_1: 0.5414  loss_ce_2: 2.187e-09  loss_mask_2: 0.02962  loss_dice_2: 0.5342  loss_ce_3: 0  loss_mask_3: 0.0312  loss_dice_3: 0.5647  loss_ce_4: 2.187e-09  loss_mask_4: 0.03204  loss_dice_4: 0.5479  loss_ce_5: 0  loss_mask_5: 0.03304  loss_dice_5: 0.5595  loss_ce_6: 0  loss_mask_6: 0.032  loss_dice_6: 0.5257  loss_ce_7: 0  loss_mask_7: 0.03248  loss_dice_7: 0.5326  loss_ce_8: 0  loss_mask_8: 0.02934  loss_dice_8: 0.5212  time: 0.4140  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:14 d2.utils.events]: [0m eta: 0:35:02  iter: 4879  total_loss: 5.62  loss_ce: 4.036e-08  loss_mask: 0.02975  loss_dice: 0.5191  loss_ce_0: 5.903e-05  loss_mask_0: 0.03007  loss_dice_0: 0.5017  loss_ce_1: 6.157e-08  loss_mask_1: 0.03217  loss_dice_1: 0.5446  loss_ce_2: 2.187e-09  loss_mask_2: 0.03379  loss_dice_2: 0.5664  loss_ce_3: 0  loss_mask_3: 0.03021  loss_dice_3: 0.5127  loss_ce_4: 2.187e-09  loss_mask_4: 0.03095  loss_dice_4: 0.5489  loss_ce_5: 0  loss_mask_5: 0.03063  loss_dice_5: 0.5637  loss_ce_6: 0  loss_mask_6: 0.02906  loss_dice_6: 0.5091  loss_ce_7: 0  loss_mask_7: 0.03383  loss_dice_7: 0.5521  loss_ce_8: 0  loss_mask_8: 0.0344  loss_dice_8: 0.5416  time: 0.4140  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:22 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:20:22 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:20:22 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:20:22 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:20:22 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:20:22 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:20:22 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259560 (0.259560 s / iter per device, on 1 devices)
[32m[06/02 14:20:22 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.195415 s / iter per device, on 1 devices)
[32m[06/02 14:20:22 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:20:22 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:20:22 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:20:22 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:20:22 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:20:22 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:20:22 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:20:22 d2.utils.events]: [0m eta: 0:34:53  iter: 4899  total_loss: 5.823  loss_ce: 3.959e-08  loss_mask: 0.03193  loss_dice: 0.5481  loss_ce_0: 5.88e-05  loss_mask_0: 0.03219  loss_dice_0: 0.5544  loss_ce_1: 6.081e-08  loss_mask_1: 0.02997  loss_dice_1: 0.5072  loss_ce_2: 2.187e-09  loss_mask_2: 0.03074  loss_dice_2: 0.5254  loss_ce_3: 0  loss_mask_3: 0.03169  loss_dice_3: 0.5879  loss_ce_4: 2.187e-09  loss_mask_4: 0.02984  loss_dice_4: 0.5257  loss_ce_5: 0  loss_mask_5: 0.03118  loss_dice_5: 0.552  loss_ce_6: 0  loss_mask_6: 0.03182  loss_dice_6: 0.5906  loss_ce_7: 0  loss_mask_7: 0.03282  loss_dice_7: 0.5278  loss_ce_8: 0  loss_mask_8: 0.03248  loss_dice_8: 0.583  time: 0.4139  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:31 d2.utils.events]: [0m eta: 0:34:45  iter: 4919  total_loss: 5.792  loss_ce: 3.664e-08  loss_mask: 0.03453  loss_dice: 0.5698  loss_ce_0: 5.858e-05  loss_mask_0: 0.03392  loss_dice_0: 0.5361  loss_ce_1: 5.971e-08  loss_mask_1: 0.03275  loss_dice_1: 0.5344  loss_ce_2: 2.187e-09  loss_mask_2: 0.03093  loss_dice_2: 0.4984  loss_ce_3: 0  loss_mask_3: 0.03128  loss_dice_3: 0.542  loss_ce_4: 1.094e-09  loss_mask_4: 0.03348  loss_dice_4: 0.5587  loss_ce_5: 0  loss_mask_5: 0.02932  loss_dice_5: 0.5324  loss_ce_6: 0  loss_mask_6: 0.02975  loss_dice_6: 0.5201  loss_ce_7: 0  loss_mask_7: 0.03151  loss_dice_7: 0.5423  loss_ce_8: 0  loss_mask_8: 0.03572  loss_dice_8: 0.5769  time: 0.4139  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:39 d2.utils.events]: [0m eta: 0:34:37  iter: 4939  total_loss: 5.665  loss_ce: 4.232e-08  loss_mask: 0.02806  loss_dice: 0.4706  loss_ce_0: 5.843e-05  loss_mask_0: 0.03279  loss_dice_0: 0.5695  loss_ce_1: 5.917e-08  loss_mask_1: 0.03123  loss_dice_1: 0.5607  loss_ce_2: 2.187e-09  loss_mask_2: 0.02992  loss_dice_2: 0.5266  loss_ce_3: 0  loss_mask_3: 0.02803  loss_dice_3: 0.512  loss_ce_4: 2.187e-09  loss_mask_4: 0.02893  loss_dice_4: 0.5022  loss_ce_5: 0  loss_mask_5: 0.02839  loss_dice_5: 0.5202  loss_ce_6: 0  loss_mask_6: 0.032  loss_dice_6: 0.5394  loss_ce_7: 0  loss_mask_7: 0.02932  loss_dice_7: 0.5243  loss_ce_8: 0  loss_mask_8: 0.03103  loss_dice_8: 0.5111  time: 0.4139  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:43 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:20:43 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:20:43 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:20:43 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:20:43 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:20:43 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:20:44 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264074 (0.264074 s / iter per device, on 1 devices)
[32m[06/02 14:20:44 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206664 s / iter per device, on 1 devices)
[32m[06/02 14:20:44 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:20:44 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:20:44 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:20:44 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:20:44 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:20:44 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:20:44 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:20:48 d2.utils.events]: [0m eta: 0:34:28  iter: 4959  total_loss: 5.805  loss_ce: 3.948e-08  loss_mask: 0.0321  loss_dice: 0.5199  loss_ce_0: 5.826e-05  loss_mask_0: 0.03102  loss_dice_0: 0.5532  loss_ce_1: 6.704e-08  loss_mask_1: 0.03382  loss_dice_1: 0.5272  loss_ce_2: 2.187e-09  loss_mask_2: 0.03463  loss_dice_2: 0.5768  loss_ce_3: 0  loss_mask_3: 0.03021  loss_dice_3: 0.5424  loss_ce_4: 1.094e-09  loss_mask_4: 0.03438  loss_dice_4: 0.5464  loss_ce_5: 0  loss_mask_5: 0.03334  loss_dice_5: 0.5701  loss_ce_6: 0  loss_mask_6: 0.03298  loss_dice_6: 0.5399  loss_ce_7: 0  loss_mask_7: 0.03117  loss_dice_7: 0.5388  loss_ce_8: 0  loss_mask_8: 0.03179  loss_dice_8: 0.5317  time: 0.4139  data_time: 0.0062  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:20:56 d2.utils.events]: [0m eta: 0:34:20  iter: 4979  total_loss: 5.846  loss_ce: 3.937e-08  loss_mask: 0.03251  loss_dice: 0.5117  loss_ce_0: 5.801e-05  loss_mask_0: 0.03022  loss_dice_0: 0.5378  loss_ce_1: 5.895e-08  loss_mask_1: 0.03353  loss_dice_1: 0.5438  loss_ce_2: 2.187e-09  loss_mask_2: 0.03188  loss_dice_2: 0.549  loss_ce_3: 0  loss_mask_3: 0.02893  loss_dice_3: 0.5154  loss_ce_4: 2.187e-09  loss_mask_4: 0.0333  loss_dice_4: 0.517  loss_ce_5: 0  loss_mask_5: 0.03183  loss_dice_5: 0.5597  loss_ce_6: 0  loss_mask_6: 0.03292  loss_dice_6: 0.5321  loss_ce_7: 0  loss_mask_7: 0.03383  loss_dice_7: 0.5505  loss_ce_8: 0  loss_mask_8: 0.02846  loss_dice_8: 0.5278  time: 0.4138  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:04 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0004999.pth
[32m[06/02 14:21:07 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:21:07 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:21:07 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:21:07 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:21:07 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:21:07 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:21:07 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.258854 (0.258854 s / iter per device, on 1 devices)
[32m[06/02 14:21:07 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.193439 s / iter per device, on 1 devices)
[32m[06/02 14:21:07 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:21:07 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:21:08 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:21:08 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:21:08 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:21:08 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:21:08 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:21:08 d2.utils.events]: [0m eta: 0:34:11  iter: 4999  total_loss: 5.918  loss_ce: 4.2e-08  loss_mask: 0.03225  loss_dice: 0.5359  loss_ce_0: 5.78e-05  loss_mask_0: 0.033  loss_dice_0: 0.5445  loss_ce_1: 6.059e-08  loss_mask_1: 0.03382  loss_dice_1: 0.5563  loss_ce_2: 2.187e-09  loss_mask_2: 0.03261  loss_dice_2: 0.5462  loss_ce_3: 0  loss_mask_3: 0.03362  loss_dice_3: 0.5173  loss_ce_4: 2.187e-09  loss_mask_4: 0.03297  loss_dice_4: 0.5994  loss_ce_5: 0  loss_mask_5: 0.03504  loss_dice_5: 0.541  loss_ce_6: 0  loss_mask_6: 0.03253  loss_dice_6: 0.5157  loss_ce_7: 0  loss_mask_7: 0.03323  loss_dice_7: 0.5398  loss_ce_8: 0  loss_mask_8: 0.03376  loss_dice_8: 0.6045  time: 0.4137  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:16 d2.utils.events]: [0m eta: 0:34:03  iter: 5019  total_loss: 5.949  loss_ce: 4.178e-08  loss_mask: 0.03175  loss_dice: 0.5456  loss_ce_0: 5.754e-05  loss_mask_0: 0.03024  loss_dice_0: 0.5765  loss_ce_1: 5.949e-08  loss_mask_1: 0.0318  loss_dice_1: 0.5616  loss_ce_2: 2.187e-09  loss_mask_2: 0.03233  loss_dice_2: 0.5812  loss_ce_3: 0  loss_mask_3: 0.02937  loss_dice_3: 0.5477  loss_ce_4: 2.187e-09  loss_mask_4: 0.03085  loss_dice_4: 0.5195  loss_ce_5: 0  loss_mask_5: 0.02769  loss_dice_5: 0.537  loss_ce_6: 0  loss_mask_6: 0.03249  loss_dice_6: 0.5461  loss_ce_7: 0  loss_mask_7: 0.03285  loss_dice_7: 0.5776  loss_ce_8: 0  loss_mask_8: 0.0297  loss_dice_8: 0.5039  time: 0.4137  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:24 d2.utils.events]: [0m eta: 0:33:54  iter: 5039  total_loss: 5.565  loss_ce: 4.156e-08  loss_mask: 0.03286  loss_dice: 0.5577  loss_ce_0: 5.738e-05  loss_mask_0: 0.02901  loss_dice_0: 0.5124  loss_ce_1: 6.485e-08  loss_mask_1: 0.03066  loss_dice_1: 0.5328  loss_ce_2: 2.187e-09  loss_mask_2: 0.02921  loss_dice_2: 0.5234  loss_ce_3: 0  loss_mask_3: 0.0287  loss_dice_3: 0.5169  loss_ce_4: 2.187e-09  loss_mask_4: 0.0297  loss_dice_4: 0.5374  loss_ce_5: 0  loss_mask_5: 0.03092  loss_dice_5: 0.5365  loss_ce_6: 0  loss_mask_6: 0.02845  loss_dice_6: 0.5047  loss_ce_7: 0  loss_mask_7: 0.03049  loss_dice_7: 0.4905  loss_ce_8: 0  loss_mask_8: 0.02911  loss_dice_8: 0.5128  time: 0.4136  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:28 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:21:28 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:21:28 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:21:28 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:21:28 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:21:28 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:21:28 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259633 (0.259633 s / iter per device, on 1 devices)
[32m[06/02 14:21:28 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.197426 s / iter per device, on 1 devices)
[32m[06/02 14:21:28 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:21:28 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:21:28 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:21:28 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:21:28 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:21:28 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:21:28 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:21:33 d2.utils.events]: [0m eta: 0:33:46  iter: 5059  total_loss: 5.665  loss_ce: 3.686e-08  loss_mask: 0.03473  loss_dice: 0.526  loss_ce_0: 5.727e-05  loss_mask_0: 0.03184  loss_dice_0: 0.5434  loss_ce_1: 6.365e-08  loss_mask_1: 0.03186  loss_dice_1: 0.5694  loss_ce_2: 2.187e-09  loss_mask_2: 0.02996  loss_dice_2: 0.5174  loss_ce_3: 0  loss_mask_3: 0.03141  loss_dice_3: 0.5279  loss_ce_4: 2.187e-09  loss_mask_4: 0.02934  loss_dice_4: 0.5779  loss_ce_5: 0  loss_mask_5: 0.03187  loss_dice_5: 0.5642  loss_ce_6: 0  loss_mask_6: 0.03049  loss_dice_6: 0.5349  loss_ce_7: 0  loss_mask_7: 0.02867  loss_dice_7: 0.5206  loss_ce_8: 0  loss_mask_8: 0.03186  loss_dice_8: 0.534  time: 0.4136  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:41 d2.utils.events]: [0m eta: 0:33:38  iter: 5079  total_loss: 5.679  loss_ce: 3.653e-08  loss_mask: 0.03257  loss_dice: 0.5732  loss_ce_0: 5.713e-05  loss_mask_0: 0.03119  loss_dice_0: 0.5506  loss_ce_1: 6.988e-08  loss_mask_1: 0.03013  loss_dice_1: 0.5349  loss_ce_2: 2.187e-09  loss_mask_2: 0.03042  loss_dice_2: 0.5545  loss_ce_3: 0  loss_mask_3: 0.03203  loss_dice_3: 0.5105  loss_ce_4: 2.187e-09  loss_mask_4: 0.03012  loss_dice_4: 0.5598  loss_ce_5: 0  loss_mask_5: 0.03018  loss_dice_5: 0.525  loss_ce_6: 0  loss_mask_6: 0.03053  loss_dice_6: 0.54  loss_ce_7: 0  loss_mask_7: 0.03467  loss_dice_7: 0.5563  loss_ce_8: 0  loss_mask_8: 0.03572  loss_dice_8: 0.5579  time: 0.4136  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:49 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:21:49 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:21:49 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:21:49 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:21:49 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:21:49 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:21:50 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280408 (0.280408 s / iter per device, on 1 devices)
[32m[06/02 14:21:50 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217448 s / iter per device, on 1 devices)
[32m[06/02 14:21:50 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:21:50 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:21:50 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:21:50 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:21:50 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:21:50 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:21:50 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:21:50 d2.utils.events]: [0m eta: 0:33:30  iter: 5099  total_loss: 5.752  loss_ce: 3.937e-08  loss_mask: 0.03173  loss_dice: 0.531  loss_ce_0: 5.696e-05  loss_mask_0: 0.02923  loss_dice_0: 0.5354  loss_ce_1: 6.693e-08  loss_mask_1: 0.03182  loss_dice_1: 0.5555  loss_ce_2: 2.187e-09  loss_mask_2: 0.03103  loss_dice_2: 0.5405  loss_ce_3: 0  loss_mask_3: 0.02922  loss_dice_3: 0.5627  loss_ce_4: 2.187e-09  loss_mask_4: 0.03105  loss_dice_4: 0.5332  loss_ce_5: 0  loss_mask_5: 0.03276  loss_dice_5: 0.576  loss_ce_6: 0  loss_mask_6: 0.03277  loss_dice_6: 0.52  loss_ce_7: 0  loss_mask_7: 0.02855  loss_dice_7: 0.4947  loss_ce_8: 0  loss_mask_8: 0.03024  loss_dice_8: 0.5448  time: 0.4136  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:21:58 d2.utils.events]: [0m eta: 0:33:22  iter: 5119  total_loss: 5.847  loss_ce: 3.959e-08  loss_mask: 0.0292  loss_dice: 0.4879  loss_ce_0: 5.671e-05  loss_mask_0: 0.02936  loss_dice_0: 0.5287  loss_ce_1: 5.435e-08  loss_mask_1: 0.0303  loss_dice_1: 0.5354  loss_ce_2: 2.187e-09  loss_mask_2: 0.0296  loss_dice_2: 0.5496  loss_ce_3: 0  loss_mask_3: 0.03074  loss_dice_3: 0.5969  loss_ce_4: 2.187e-09  loss_mask_4: 0.03301  loss_dice_4: 0.5786  loss_ce_5: 0  loss_mask_5: 0.02931  loss_dice_5: 0.5108  loss_ce_6: 0  loss_mask_6: 0.03099  loss_dice_6: 0.5247  loss_ce_7: 0  loss_mask_7: 0.03411  loss_dice_7: 0.5712  loss_ce_8: 0  loss_mask_8: 0.03249  loss_dice_8: 0.5867  time: 0.4136  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:06 d2.utils.events]: [0m eta: 0:33:14  iter: 5139  total_loss: 5.758  loss_ce: 3.74e-08  loss_mask: 0.03047  loss_dice: 0.5116  loss_ce_0: 5.66e-05  loss_mask_0: 0.0301  loss_dice_0: 0.5173  loss_ce_1: 6.835e-08  loss_mask_1: 0.0297  loss_dice_1: 0.4958  loss_ce_2: 2.187e-09  loss_mask_2: 0.0335  loss_dice_2: 0.5799  loss_ce_3: 0  loss_mask_3: 0.03229  loss_dice_3: 0.5314  loss_ce_4: 2.187e-09  loss_mask_4: 0.03476  loss_dice_4: 0.5379  loss_ce_5: 0  loss_mask_5: 0.03194  loss_dice_5: 0.6033  loss_ce_6: 0  loss_mask_6: 0.0299  loss_dice_6: 0.5439  loss_ce_7: 0  loss_mask_7: 0.029  loss_dice_7: 0.4881  loss_ce_8: 0  loss_mask_8: 0.03208  loss_dice_8: 0.5592  time: 0.4136  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:11 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:22:11 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:22:11 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:22:11 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:22:11 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:22:11 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:22:11 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271390 (0.271390 s / iter per device, on 1 devices)
[32m[06/02 14:22:11 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.209489 s / iter per device, on 1 devices)
[32m[06/02 14:22:11 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:22:11 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:22:11 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:22:11 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:22:11 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:22:11 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:22:11 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:22:16 d2.utils.events]: [0m eta: 0:33:06  iter: 5159  total_loss: 5.765  loss_ce: 4.101e-08  loss_mask: 0.02902  loss_dice: 0.5269  loss_ce_0: 5.642e-05  loss_mask_0: 0.03148  loss_dice_0: 0.5412  loss_ce_1: 5.512e-08  loss_mask_1: 0.02927  loss_dice_1: 0.5176  loss_ce_2: 2.187e-09  loss_mask_2: 0.03076  loss_dice_2: 0.5783  loss_ce_3: 0  loss_mask_3: 0.03042  loss_dice_3: 0.5256  loss_ce_4: 2.187e-09  loss_mask_4: 0.03181  loss_dice_4: 0.5841  loss_ce_5: 0  loss_mask_5: 0.03294  loss_dice_5: 0.5495  loss_ce_6: 0  loss_mask_6: 0.03132  loss_dice_6: 0.5205  loss_ce_7: 0  loss_mask_7: 0.03686  loss_dice_7: 0.5952  loss_ce_8: 0  loss_mask_8: 0.02967  loss_dice_8: 0.5237  time: 0.4136  data_time: 0.0084  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:24 d2.utils.events]: [0m eta: 0:32:57  iter: 5179  total_loss: 5.72  loss_ce: 3.642e-08  loss_mask: 0.03446  loss_dice: 0.5789  loss_ce_0: 5.62e-05  loss_mask_0: 0.03055  loss_dice_0: 0.4849  loss_ce_1: 7.174e-08  loss_mask_1: 0.03126  loss_dice_1: 0.5373  loss_ce_2: 2.187e-09  loss_mask_2: 0.03105  loss_dice_2: 0.5857  loss_ce_3: 0  loss_mask_3: 0.03337  loss_dice_3: 0.5344  loss_ce_4: 2.187e-09  loss_mask_4: 0.03369  loss_dice_4: 0.5153  loss_ce_5: 0  loss_mask_5: 0.03299  loss_dice_5: 0.5398  loss_ce_6: 0  loss_mask_6: 0.03343  loss_dice_6: 0.5695  loss_ce_7: 0  loss_mask_7: 0.02827  loss_dice_7: 0.5105  loss_ce_8: 0  loss_mask_8: 0.03353  loss_dice_8: 0.5642  time: 0.4136  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:32 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:22:32 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:22:32 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:22:32 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:22:32 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:22:32 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:22:33 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.246428 (0.246428 s / iter per device, on 1 devices)
[32m[06/02 14:22:33 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.187540 s / iter per device, on 1 devices)
[32m[06/02 14:22:33 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:22:33 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:22:33 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:22:33 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:22:33 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:22:33 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:22:33 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:22:33 d2.utils.events]: [0m eta: 0:32:49  iter: 5199  total_loss: 5.722  loss_ce: 4.145e-08  loss_mask: 0.03015  loss_dice: 0.5658  loss_ce_0: 5.603e-05  loss_mask_0: 0.03152  loss_dice_0: 0.5496  loss_ce_1: 5.709e-08  loss_mask_1: 0.03043  loss_dice_1: 0.5049  loss_ce_2: 2.187e-09  loss_mask_2: 0.03059  loss_dice_2: 0.5237  loss_ce_3: 0  loss_mask_3: 0.03176  loss_dice_3: 0.5548  loss_ce_4: 2.187e-09  loss_mask_4: 0.03387  loss_dice_4: 0.5816  loss_ce_5: 0  loss_mask_5: 0.02942  loss_dice_5: 0.5519  loss_ce_6: 0  loss_mask_6: 0.03077  loss_dice_6: 0.5248  loss_ce_7: 0  loss_mask_7: 0.02853  loss_dice_7: 0.5492  loss_ce_8: 0  loss_mask_8: 0.0276  loss_dice_8: 0.5326  time: 0.4136  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:41 d2.utils.events]: [0m eta: 0:32:41  iter: 5219  total_loss: 5.697  loss_ce: 3.708e-08  loss_mask: 0.03169  loss_dice: 0.5237  loss_ce_0: 5.586e-05  loss_mask_0: 0.02972  loss_dice_0: 0.5222  loss_ce_1: 7.327e-08  loss_mask_1: 0.03055  loss_dice_1: 0.5436  loss_ce_2: 2.187e-09  loss_mask_2: 0.03671  loss_dice_2: 0.5484  loss_ce_3: 0  loss_mask_3: 0.02837  loss_dice_3: 0.5396  loss_ce_4: 0  loss_mask_4: 0.03066  loss_dice_4: 0.5343  loss_ce_5: 0  loss_mask_5: 0.03022  loss_dice_5: 0.523  loss_ce_6: 0  loss_mask_6: 0.03267  loss_dice_6: 0.5495  loss_ce_7: 0  loss_mask_7: 0.03435  loss_dice_7: 0.5166  loss_ce_8: 0  loss_mask_8: 0.03103  loss_dice_8: 0.5409  time: 0.4135  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:49 d2.utils.events]: [0m eta: 0:32:32  iter: 5239  total_loss: 5.869  loss_ce: 3.587e-08  loss_mask: 0.03176  loss_dice: 0.5785  loss_ce_0: 5.565e-05  loss_mask_0: 0.03418  loss_dice_0: 0.5823  loss_ce_1: 5.807e-08  loss_mask_1: 0.03027  loss_dice_1: 0.5134  loss_ce_2: 2.187e-09  loss_mask_2: 0.02823  loss_dice_2: 0.5392  loss_ce_3: 0  loss_mask_3: 0.0311  loss_dice_3: 0.5462  loss_ce_4: 1.094e-09  loss_mask_4: 0.03065  loss_dice_4: 0.5066  loss_ce_5: 0  loss_mask_5: 0.03176  loss_dice_5: 0.5348  loss_ce_6: 0  loss_mask_6: 0.03324  loss_dice_6: 0.5161  loss_ce_7: 0  loss_mask_7: 0.03157  loss_dice_7: 0.5533  loss_ce_8: 0  loss_mask_8: 0.02829  loss_dice_8: 0.5414  time: 0.4135  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:22:53 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:22:53 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:22:53 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:22:53 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:22:53 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:22:53 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:22:54 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259665 (0.259665 s / iter per device, on 1 devices)
[32m[06/02 14:22:54 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.199359 s / iter per device, on 1 devices)
[32m[06/02 14:22:54 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:22:54 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:22:54 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:22:54 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:22:54 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:22:54 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:22:54 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:22:58 d2.utils.events]: [0m eta: 0:32:23  iter: 5259  total_loss: 5.787  loss_ce: 3.172e-08  loss_mask: 0.03277  loss_dice: 0.5826  loss_ce_0: 5.557e-05  loss_mask_0: 0.03023  loss_dice_0: 0.5264  loss_ce_1: 5.621e-08  loss_mask_1: 0.03164  loss_dice_1: 0.537  loss_ce_2: 2.187e-09  loss_mask_2: 0.03024  loss_dice_2: 0.5511  loss_ce_3: 0  loss_mask_3: 0.02832  loss_dice_3: 0.5188  loss_ce_4: 1.094e-09  loss_mask_4: 0.03241  loss_dice_4: 0.5893  loss_ce_5: 0  loss_mask_5: 0.02934  loss_dice_5: 0.504  loss_ce_6: 0  loss_mask_6: 0.03472  loss_dice_6: 0.5346  loss_ce_7: 0  loss_mask_7: 0.02903  loss_dice_7: 0.5236  loss_ce_8: 0  loss_mask_8: 0.03149  loss_dice_8: 0.5316  time: 0.4134  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:06 d2.utils.events]: [0m eta: 0:32:15  iter: 5279  total_loss: 5.861  loss_ce: 2.406e-08  loss_mask: 0.03102  loss_dice: 0.5233  loss_ce_0: 5.538e-05  loss_mask_0: 0.03051  loss_dice_0: 0.5615  loss_ce_1: 7.207e-08  loss_mask_1: 0.03497  loss_dice_1: 0.5446  loss_ce_2: 2.187e-09  loss_mask_2: 0.03179  loss_dice_2: 0.5589  loss_ce_3: 0  loss_mask_3: 0.03253  loss_dice_3: 0.5486  loss_ce_4: 0  loss_mask_4: 0.03006  loss_dice_4: 0.5466  loss_ce_5: 0  loss_mask_5: 0.03501  loss_dice_5: 0.6171  loss_ce_6: 0  loss_mask_6: 0.03084  loss_dice_6: 0.5381  loss_ce_7: 0  loss_mask_7: 0.03254  loss_dice_7: 0.5181  loss_ce_8: 0  loss_mask_8: 0.03283  loss_dice_8: 0.5465  time: 0.4134  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:14 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:23:14 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:23:14 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:23:14 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:23:14 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:23:14 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:23:15 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.267640 (0.267640 s / iter per device, on 1 devices)
[32m[06/02 14:23:15 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208341 s / iter per device, on 1 devices)
[32m[06/02 14:23:15 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:23:15 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:23:15 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:23:15 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:23:15 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:23:15 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:23:15 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:23:15 d2.utils.events]: [0m eta: 0:32:07  iter: 5299  total_loss: 5.676  loss_ce: 2.734e-08  loss_mask: 0.03208  loss_dice: 0.5498  loss_ce_0: 5.513e-05  loss_mask_0: 0.03034  loss_dice_0: 0.5159  loss_ce_1: 6.343e-08  loss_mask_1: 0.02975  loss_dice_1: 0.5258  loss_ce_2: 2.187e-09  loss_mask_2: 0.02941  loss_dice_2: 0.5212  loss_ce_3: 0  loss_mask_3: 0.03041  loss_dice_3: 0.5206  loss_ce_4: 0  loss_mask_4: 0.03002  loss_dice_4: 0.5602  loss_ce_5: 0  loss_mask_5: 0.02964  loss_dice_5: 0.5247  loss_ce_6: 0  loss_mask_6: 0.02855  loss_dice_6: 0.523  loss_ce_7: 0  loss_mask_7: 0.02893  loss_dice_7: 0.5054  loss_ce_8: 0  loss_mask_8: 0.02756  loss_dice_8: 0.533  time: 0.4134  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:23 d2.utils.events]: [0m eta: 0:31:58  iter: 5319  total_loss: 5.716  loss_ce: 2.942e-08  loss_mask: 0.031  loss_dice: 0.5457  loss_ce_0: 5.494e-05  loss_mask_0: 0.03125  loss_dice_0: 0.5136  loss_ce_1: 5.359e-08  loss_mask_1: 0.03019  loss_dice_1: 0.5489  loss_ce_2: 2.187e-09  loss_mask_2: 0.03322  loss_dice_2: 0.5554  loss_ce_3: 0  loss_mask_3: 0.03069  loss_dice_3: 0.5379  loss_ce_4: 0  loss_mask_4: 0.03102  loss_dice_4: 0.541  loss_ce_5: 0  loss_mask_5: 0.03084  loss_dice_5: 0.506  loss_ce_6: 0  loss_mask_6: 0.03  loss_dice_6: 0.4893  loss_ce_7: 0  loss_mask_7: 0.03161  loss_dice_7: 0.5581  loss_ce_8: 0  loss_mask_8: 0.03096  loss_dice_8: 0.5076  time: 0.4134  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:32 d2.utils.events]: [0m eta: 0:31:51  iter: 5339  total_loss: 5.654  loss_ce: 2.526e-08  loss_mask: 0.03312  loss_dice: 0.576  loss_ce_0: 5.478e-05  loss_mask_0: 0.03172  loss_dice_0: 0.5448  loss_ce_1: 5.162e-08  loss_mask_1: 0.03342  loss_dice_1: 0.5586  loss_ce_2: 2.187e-09  loss_mask_2: 0.03291  loss_dice_2: 0.5238  loss_ce_3: 0  loss_mask_3: 0.03263  loss_dice_3: 0.5172  loss_ce_4: 0  loss_mask_4: 0.03399  loss_dice_4: 0.6061  loss_ce_5: 0  loss_mask_5: 0.03109  loss_dice_5: 0.5086  loss_ce_6: 0  loss_mask_6: 0.03183  loss_dice_6: 0.52  loss_ce_7: 0  loss_mask_7: 0.03394  loss_dice_7: 0.5295  loss_ce_8: 0  loss_mask_8: 0.03282  loss_dice_8: 0.509  time: 0.4134  data_time: 0.0083  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:36 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:23:36 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:23:36 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:23:36 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:23:36 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:23:36 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:23:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274932 (0.274932 s / iter per device, on 1 devices)
[32m[06/02 14:23:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210309 s / iter per device, on 1 devices)
[32m[06/02 14:23:36 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:23:36 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:23:36 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:23:36 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:23:36 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:23:36 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:23:36 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:23:41 d2.utils.events]: [0m eta: 0:31:43  iter: 5359  total_loss: 5.73  loss_ce: 2.45e-08  loss_mask: 0.03054  loss_dice: 0.5282  loss_ce_0: 5.451e-05  loss_mask_0: 0.03115  loss_dice_0: 0.5393  loss_ce_1: 5.206e-08  loss_mask_1: 0.03247  loss_dice_1: 0.5333  loss_ce_2: 2.187e-09  loss_mask_2: 0.031  loss_dice_2: 0.533  loss_ce_3: 0  loss_mask_3: 0.02811  loss_dice_3: 0.5408  loss_ce_4: 0  loss_mask_4: 0.03203  loss_dice_4: 0.5297  loss_ce_5: 0  loss_mask_5: 0.03265  loss_dice_5: 0.5344  loss_ce_6: 0  loss_mask_6: 0.03162  loss_dice_6: 0.5346  loss_ce_7: 0  loss_mask_7: 0.03308  loss_dice_7: 0.5709  loss_ce_8: 0  loss_mask_8: 0.02942  loss_dice_8: 0.5471  time: 0.4134  data_time: 0.0055  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:49 d2.utils.events]: [0m eta: 0:31:34  iter: 5379  total_loss: 5.655  loss_ce: 2.198e-08  loss_mask: 0.03048  loss_dice: 0.5306  loss_ce_0: 5.442e-05  loss_mask_0: 0.02996  loss_dice_0: 0.566  loss_ce_1: 5.129e-08  loss_mask_1: 0.02962  loss_dice_1: 0.5248  loss_ce_2: 2.187e-09  loss_mask_2: 0.03159  loss_dice_2: 0.519  loss_ce_3: 0  loss_mask_3: 0.03139  loss_dice_3: 0.5232  loss_ce_4: 0  loss_mask_4: 0.03225  loss_dice_4: 0.54  loss_ce_5: 0  loss_mask_5: 0.02803  loss_dice_5: 0.5314  loss_ce_6: 0  loss_mask_6: 0.03083  loss_dice_6: 0.5309  loss_ce_7: 0  loss_mask_7: 0.02745  loss_dice_7: 0.4827  loss_ce_8: 0  loss_mask_8: 0.0296  loss_dice_8: 0.5328  time: 0.4133  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:23:57 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:23:57 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:23:57 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:23:57 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:23:57 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:23:57 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:23:58 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.269410 (0.269410 s / iter per device, on 1 devices)
[32m[06/02 14:23:58 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208190 s / iter per device, on 1 devices)
[32m[06/02 14:23:58 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:23:58 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:23:58 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:23:58 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:23:58 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:23:58 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:23:58 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:23:58 d2.utils.events]: [0m eta: 0:31:25  iter: 5399  total_loss: 5.896  loss_ce: 2.242e-08  loss_mask: 0.03385  loss_dice: 0.5828  loss_ce_0: 5.419e-05  loss_mask_0: 0.0307  loss_dice_0: 0.5493  loss_ce_1: 5.14e-08  loss_mask_1: 0.02979  loss_dice_1: 0.5198  loss_ce_2: 2.187e-09  loss_mask_2: 0.0314  loss_dice_2: 0.5642  loss_ce_3: 0  loss_mask_3: 0.03154  loss_dice_3: 0.5438  loss_ce_4: 0  loss_mask_4: 0.0322  loss_dice_4: 0.5271  loss_ce_5: 0  loss_mask_5: 0.03205  loss_dice_5: 0.5488  loss_ce_6: 0  loss_mask_6: 0.02992  loss_dice_6: 0.5835  loss_ce_7: 0  loss_mask_7: 0.03135  loss_dice_7: 0.5186  loss_ce_8: 0  loss_mask_8: 0.0323  loss_dice_8: 0.5437  time: 0.4133  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:06 d2.utils.events]: [0m eta: 0:31:16  iter: 5419  total_loss: 5.633  loss_ce: 2.056e-08  loss_mask: 0.03162  loss_dice: 0.5943  loss_ce_0: 5.401e-05  loss_mask_0: 0.03074  loss_dice_0: 0.5152  loss_ce_1: 5.337e-08  loss_mask_1: 0.02846  loss_dice_1: 0.504  loss_ce_2: 2.187e-09  loss_mask_2: 0.02813  loss_dice_2: 0.4987  loss_ce_3: 0  loss_mask_3: 0.03193  loss_dice_3: 0.5229  loss_ce_4: 0  loss_mask_4: 0.02959  loss_dice_4: 0.5208  loss_ce_5: 0  loss_mask_5: 0.02958  loss_dice_5: 0.5251  loss_ce_6: 0  loss_mask_6: 0.03123  loss_dice_6: 0.5717  loss_ce_7: 0  loss_mask_7: 0.02839  loss_dice_7: 0.5699  loss_ce_8: 0  loss_mask_8: 0.02986  loss_dice_8: 0.5181  time: 0.4133  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:14 d2.utils.events]: [0m eta: 0:31:07  iter: 5439  total_loss: 5.668  loss_ce: 1.815e-08  loss_mask: 0.0327  loss_dice: 0.5266  loss_ce_0: 5.384e-05  loss_mask_0: 0.033  loss_dice_0: 0.5487  loss_ce_1: 6.179e-08  loss_mask_1: 0.03087  loss_dice_1: 0.4971  loss_ce_2: 2.187e-09  loss_mask_2: 0.03455  loss_dice_2: 0.5403  loss_ce_3: 0  loss_mask_3: 0.02672  loss_dice_3: 0.4966  loss_ce_4: 0  loss_mask_4: 0.03009  loss_dice_4: 0.538  loss_ce_5: 0  loss_mask_5: 0.03429  loss_dice_5: 0.5569  loss_ce_6: 0  loss_mask_6: 0.03318  loss_dice_6: 0.5481  loss_ce_7: 0  loss_mask_7: 0.03196  loss_dice_7: 0.5322  loss_ce_8: 0  loss_mask_8: 0.03139  loss_dice_8: 0.5505  time: 0.4132  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:18 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:24:18 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:24:18 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:24:18 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:24:18 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:24:18 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:24:19 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274706 (0.274706 s / iter per device, on 1 devices)
[32m[06/02 14:24:19 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213950 s / iter per device, on 1 devices)
[32m[06/02 14:24:19 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:24:19 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:24:19 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:24:19 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:24:19 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:24:19 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:24:19 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:24:23 d2.utils.events]: [0m eta: 0:30:58  iter: 5459  total_loss: 5.857  loss_ce: 1.98e-08  loss_mask: 0.02952  loss_dice: 0.5186  loss_ce_0: 5.36e-05  loss_mask_0: 0.03427  loss_dice_0: 0.6004  loss_ce_1: 7.098e-08  loss_mask_1: 0.03005  loss_dice_1: 0.5512  loss_ce_2: 2.187e-09  loss_mask_2: 0.03287  loss_dice_2: 0.5272  loss_ce_3: 0  loss_mask_3: 0.03519  loss_dice_3: 0.5671  loss_ce_4: 0  loss_mask_4: 0.03296  loss_dice_4: 0.5955  loss_ce_5: 0  loss_mask_5: 0.03065  loss_dice_5: 0.5344  loss_ce_6: 0  loss_mask_6: 0.03234  loss_dice_6: 0.544  loss_ce_7: 0  loss_mask_7: 0.03088  loss_dice_7: 0.554  loss_ce_8: 0  loss_mask_8: 0.03261  loss_dice_8: 0.5815  time: 0.4132  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:31 d2.utils.events]: [0m eta: 0:30:50  iter: 5479  total_loss: 5.727  loss_ce: 2.308e-08  loss_mask: 0.02823  loss_dice: 0.5372  loss_ce_0: 5.34e-05  loss_mask_0: 0.02912  loss_dice_0: 0.5241  loss_ce_1: 5.643e-08  loss_mask_1: 0.0287  loss_dice_1: 0.5265  loss_ce_2: 2.187e-09  loss_mask_2: 0.03151  loss_dice_2: 0.5444  loss_ce_3: 0  loss_mask_3: 0.03163  loss_dice_3: 0.5536  loss_ce_4: 0  loss_mask_4: 0.03208  loss_dice_4: 0.5568  loss_ce_5: 0  loss_mask_5: 0.03085  loss_dice_5: 0.5542  loss_ce_6: 0  loss_mask_6: 0.03215  loss_dice_6: 0.5417  loss_ce_7: 0  loss_mask_7: 0.03229  loss_dice_7: 0.55  loss_ce_8: 0  loss_mask_8: 0.02904  loss_dice_8: 0.5255  time: 0.4132  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:39 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:24:39 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:24:39 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:24:39 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:24:39 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:24:39 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:24:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.292765 (0.292765 s / iter per device, on 1 devices)
[32m[06/02 14:24:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.230837 s / iter per device, on 1 devices)
[32m[06/02 14:24:40 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:24:40 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:24:40 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:24:40 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:24:40 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:24:40 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:24:40 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:24:40 d2.utils.events]: [0m eta: 0:30:40  iter: 5499  total_loss: 5.912  loss_ce: 2.406e-08  loss_mask: 0.03365  loss_dice: 0.5282  loss_ce_0: 5.324e-05  loss_mask_0: 0.03053  loss_dice_0: 0.5165  loss_ce_1: 5.851e-08  loss_mask_1: 0.02934  loss_dice_1: 0.5395  loss_ce_2: 2.187e-09  loss_mask_2: 0.03275  loss_dice_2: 0.5855  loss_ce_3: 0  loss_mask_3: 0.02852  loss_dice_3: 0.5566  loss_ce_4: 0  loss_mask_4: 0.02976  loss_dice_4: 0.5524  loss_ce_5: 0  loss_mask_5: 0.03519  loss_dice_5: 0.5797  loss_ce_6: 0  loss_mask_6: 0.03446  loss_dice_6: 0.5632  loss_ce_7: 0  loss_mask_7: 0.0342  loss_dice_7: 0.5601  loss_ce_8: 0  loss_mask_8: 0.03201  loss_dice_8: 0.5227  time: 0.4132  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:48 d2.utils.events]: [0m eta: 0:30:32  iter: 5519  total_loss: 5.775  loss_ce: 2.242e-08  loss_mask: 0.03384  loss_dice: 0.5529  loss_ce_0: 5.305e-05  loss_mask_0: 0.03062  loss_dice_0: 0.5598  loss_ce_1: 5.807e-08  loss_mask_1: 0.02995  loss_dice_1: 0.5625  loss_ce_2: 2.187e-09  loss_mask_2: 0.03137  loss_dice_2: 0.5919  loss_ce_3: 0  loss_mask_3: 0.03208  loss_dice_3: 0.5448  loss_ce_4: 0  loss_mask_4: 0.03311  loss_dice_4: 0.5317  loss_ce_5: 0  loss_mask_5: 0.03001  loss_dice_5: 0.5283  loss_ce_6: 0  loss_mask_6: 0.03044  loss_dice_6: 0.5042  loss_ce_7: 0  loss_mask_7: 0.03035  loss_dice_7: 0.5275  loss_ce_8: 0  loss_mask_8: 0.03064  loss_dice_8: 0.5306  time: 0.4132  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:24:56 d2.utils.events]: [0m eta: 0:30:23  iter: 5539  total_loss: 5.912  loss_ce: 2.592e-08  loss_mask: 0.03486  loss_dice: 0.5775  loss_ce_0: 5.285e-05  loss_mask_0: 0.03193  loss_dice_0: 0.5512  loss_ce_1: 5.643e-08  loss_mask_1: 0.03119  loss_dice_1: 0.5811  loss_ce_2: 2.187e-09  loss_mask_2: 0.03091  loss_dice_2: 0.5401  loss_ce_3: 0  loss_mask_3: 0.03392  loss_dice_3: 0.5368  loss_ce_4: 0  loss_mask_4: 0.03147  loss_dice_4: 0.5622  loss_ce_5: 0  loss_mask_5: 0.02921  loss_dice_5: 0.5391  loss_ce_6: 0  loss_mask_6: 0.03183  loss_dice_6: 0.5556  loss_ce_7: 0  loss_mask_7: 0.02956  loss_dice_7: 0.5268  loss_ce_8: 0  loss_mask_8: 0.0317  loss_dice_8: 0.5739  time: 0.4131  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:01 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:25:01 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:25:01 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:25:01 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:25:01 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:25:01 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:25:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.256208 (0.256208 s / iter per device, on 1 devices)
[32m[06/02 14:25:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.196943 s / iter per device, on 1 devices)
[32m[06/02 14:25:01 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:25:01 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:25:01 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:25:01 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:25:01 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:25:01 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:25:01 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:25:05 d2.utils.events]: [0m eta: 0:30:14  iter: 5559  total_loss: 5.758  loss_ce: 2.023e-08  loss_mask: 0.03069  loss_dice: 0.521  loss_ce_0: 5.256e-05  loss_mask_0: 0.03104  loss_dice_0: 0.5285  loss_ce_1: 6.988e-08  loss_mask_1: 0.03184  loss_dice_1: 0.5485  loss_ce_2: 2.187e-09  loss_mask_2: 0.031  loss_dice_2: 0.5556  loss_ce_3: 0  loss_mask_3: 0.03047  loss_dice_3: 0.5176  loss_ce_4: 0  loss_mask_4: 0.0321  loss_dice_4: 0.5414  loss_ce_5: 0  loss_mask_5: 0.03171  loss_dice_5: 0.5426  loss_ce_6: 0  loss_mask_6: 0.03144  loss_dice_6: 0.5547  loss_ce_7: 0  loss_mask_7: 0.03241  loss_dice_7: 0.5546  loss_ce_8: 0  loss_mask_8: 0.03347  loss_dice_8: 0.5694  time: 0.4131  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:14 d2.utils.events]: [0m eta: 0:30:06  iter: 5579  total_loss: 6.034  loss_ce: 2.515e-08  loss_mask: 0.03082  loss_dice: 0.6072  loss_ce_0: 5.247e-05  loss_mask_0: 0.02857  loss_dice_0: 0.5096  loss_ce_1: 5.993e-08  loss_mask_1: 0.03395  loss_dice_1: 0.5841  loss_ce_2: 2.187e-09  loss_mask_2: 0.03062  loss_dice_2: 0.5932  loss_ce_3: 0  loss_mask_3: 0.03032  loss_dice_3: 0.5527  loss_ce_4: 0  loss_mask_4: 0.0308  loss_dice_4: 0.5436  loss_ce_5: 0  loss_mask_5: 0.02929  loss_dice_5: 0.5205  loss_ce_6: 0  loss_mask_6: 0.0314  loss_dice_6: 0.5184  loss_ce_7: 0  loss_mask_7: 0.02969  loss_dice_7: 0.5395  loss_ce_8: 0  loss_mask_8: 0.03097  loss_dice_8: 0.5464  time: 0.4131  data_time: 0.0082  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:22 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:25:22 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:25:22 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:25:22 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:25:22 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:25:22 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:25:23 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271232 (0.271232 s / iter per device, on 1 devices)
[32m[06/02 14:25:23 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.207862 s / iter per device, on 1 devices)
[32m[06/02 14:25:23 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:25:23 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:25:23 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:25:23 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:25:23 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:25:23 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:25:23 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:25:23 d2.utils.events]: [0m eta: 0:29:57  iter: 5599  total_loss: 5.872  loss_ce: 2.308e-08  loss_mask: 0.03449  loss_dice: 0.5919  loss_ce_0: 5.24e-05  loss_mask_0: 0.03142  loss_dice_0: 0.5318  loss_ce_1: 6.31e-08  loss_mask_1: 0.03268  loss_dice_1: 0.5548  loss_ce_2: 2.187e-09  loss_mask_2: 0.02905  loss_dice_2: 0.5576  loss_ce_3: 0  loss_mask_3: 0.03098  loss_dice_3: 0.5128  loss_ce_4: 0  loss_mask_4: 0.02961  loss_dice_4: 0.5459  loss_ce_5: 0  loss_mask_5: 0.03463  loss_dice_5: 0.6067  loss_ce_6: 0  loss_mask_6: 0.03075  loss_dice_6: 0.5555  loss_ce_7: 0  loss_mask_7: 0.03434  loss_dice_7: 0.5711  loss_ce_8: 0  loss_mask_8: 0.0337  loss_dice_8: 0.5675  time: 0.4131  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:31 d2.utils.events]: [0m eta: 0:29:49  iter: 5619  total_loss: 5.745  loss_ce: 2.636e-08  loss_mask: 0.0304  loss_dice: 0.507  loss_ce_0: 5.218e-05  loss_mask_0: 0.03069  loss_dice_0: 0.5313  loss_ce_1: 6.474e-08  loss_mask_1: 0.03029  loss_dice_1: 0.5532  loss_ce_2: 2.187e-09  loss_mask_2: 0.03032  loss_dice_2: 0.5109  loss_ce_3: 0  loss_mask_3: 0.03065  loss_dice_3: 0.561  loss_ce_4: 0  loss_mask_4: 0.02864  loss_dice_4: 0.49  loss_ce_5: 0  loss_mask_5: 0.03024  loss_dice_5: 0.5358  loss_ce_6: 0  loss_mask_6: 0.03134  loss_dice_6: 0.5631  loss_ce_7: 0  loss_mask_7: 0.02779  loss_dice_7: 0.5299  loss_ce_8: 0  loss_mask_8: 0.0284  loss_dice_8: 0.4808  time: 0.4131  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:39 d2.utils.events]: [0m eta: 0:29:41  iter: 5639  total_loss: 5.878  loss_ce: 2.887e-08  loss_mask: 0.03322  loss_dice: 0.5678  loss_ce_0: 5.194e-05  loss_mask_0: 0.03079  loss_dice_0: 0.5508  loss_ce_1: 6.606e-08  loss_mask_1: 0.03081  loss_dice_1: 0.5828  loss_ce_2: 2.187e-09  loss_mask_2: 0.03143  loss_dice_2: 0.5773  loss_ce_3: 0  loss_mask_3: 0.03106  loss_dice_3: 0.5372  loss_ce_4: 0  loss_mask_4: 0.03042  loss_dice_4: 0.5047  loss_ce_5: 0  loss_mask_5: 0.03216  loss_dice_5: 0.5117  loss_ce_6: 0  loss_mask_6: 0.03304  loss_dice_6: 0.583  loss_ce_7: 0  loss_mask_7: 0.03329  loss_dice_7: 0.5642  loss_ce_8: 0  loss_mask_8: 0.03171  loss_dice_8: 0.5269  time: 0.4131  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:43 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:25:43 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:25:43 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:25:43 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:25:43 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:25:43 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:25:44 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.289249 (0.289249 s / iter per device, on 1 devices)
[32m[06/02 14:25:44 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.228926 s / iter per device, on 1 devices)
[32m[06/02 14:25:44 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:25:44 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:25:44 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:25:44 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:25:44 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:25:44 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:25:44 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:25:48 d2.utils.events]: [0m eta: 0:29:32  iter: 5659  total_loss: 5.828  loss_ce: 3.325e-08  loss_mask: 0.03034  loss_dice: 0.5333  loss_ce_0: 5.175e-05  loss_mask_0: 0.03131  loss_dice_0: 0.5663  loss_ce_1: 5.293e-08  loss_mask_1: 0.03087  loss_dice_1: 0.5488  loss_ce_2: 2.187e-09  loss_mask_2: 0.03111  loss_dice_2: 0.5573  loss_ce_3: 0  loss_mask_3: 0.03144  loss_dice_3: 0.5447  loss_ce_4: 0  loss_mask_4: 0.03174  loss_dice_4: 0.5435  loss_ce_5: 0  loss_mask_5: 0.03218  loss_dice_5: 0.6  loss_ce_6: 0  loss_mask_6: 0.0335  loss_dice_6: 0.5624  loss_ce_7: 0  loss_mask_7: 0.0299  loss_dice_7: 0.5232  loss_ce_8: 0  loss_mask_8: 0.03441  loss_dice_8: 0.5835  time: 0.4131  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:25:57 d2.utils.events]: [0m eta: 0:29:24  iter: 5679  total_loss: 5.738  loss_ce: 3.204e-08  loss_mask: 0.02998  loss_dice: 0.5275  loss_ce_0: 5.157e-05  loss_mask_0: 0.02858  loss_dice_0: 0.5317  loss_ce_1: 5.14e-08  loss_mask_1: 0.0325  loss_dice_1: 0.5343  loss_ce_2: 2.187e-09  loss_mask_2: 0.03043  loss_dice_2: 0.5398  loss_ce_3: 0  loss_mask_3: 0.02903  loss_dice_3: 0.5385  loss_ce_4: 0  loss_mask_4: 0.02992  loss_dice_4: 0.5623  loss_ce_5: 0  loss_mask_5: 0.03278  loss_dice_5: 0.573  loss_ce_6: 0  loss_mask_6: 0.02916  loss_dice_6: 0.5517  loss_ce_7: 0  loss_mask_7: 0.03164  loss_dice_7: 0.5555  loss_ce_8: 0  loss_mask_8: 0.02956  loss_dice_8: 0.5003  time: 0.4131  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:05 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:26:05 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:26:05 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:26:05 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:26:05 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:26:05 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:26:05 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264551 (0.264551 s / iter per device, on 1 devices)
[32m[06/02 14:26:05 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205111 s / iter per device, on 1 devices)
[32m[06/02 14:26:05 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:26:05 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:26:05 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:26:05 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:26:05 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:26:05 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:26:05 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:26:05 d2.utils.events]: [0m eta: 0:29:15  iter: 5699  total_loss: 5.923  loss_ce: 3.456e-08  loss_mask: 0.03297  loss_dice: 0.5485  loss_ce_0: 5.146e-05  loss_mask_0: 0.03312  loss_dice_0: 0.5936  loss_ce_1: 5.556e-08  loss_mask_1: 0.03169  loss_dice_1: 0.5511  loss_ce_2: 2.187e-09  loss_mask_2: 0.03338  loss_dice_2: 0.5682  loss_ce_3: 0  loss_mask_3: 0.03122  loss_dice_3: 0.5573  loss_ce_4: 0  loss_mask_4: 0.03085  loss_dice_4: 0.5368  loss_ce_5: 0  loss_mask_5: 0.0314  loss_dice_5: 0.5851  loss_ce_6: 0  loss_mask_6: 0.03291  loss_dice_6: 0.5669  loss_ce_7: 0  loss_mask_7: 0.03029  loss_dice_7: 0.5194  loss_ce_8: 0  loss_mask_8: 0.03082  loss_dice_8: 0.5472  time: 0.4131  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:14 d2.utils.events]: [0m eta: 0:29:07  iter: 5719  total_loss: 5.939  loss_ce: 3.226e-08  loss_mask: 0.03163  loss_dice: 0.5808  loss_ce_0: 5.124e-05  loss_mask_0: 0.0321  loss_dice_0: 0.5664  loss_ce_1: 4.965e-08  loss_mask_1: 0.0298  loss_dice_1: 0.5393  loss_ce_2: 2.187e-09  loss_mask_2: 0.03096  loss_dice_2: 0.5213  loss_ce_3: 0  loss_mask_3: 0.03193  loss_dice_3: 0.5668  loss_ce_4: 0  loss_mask_4: 0.0327  loss_dice_4: 0.5701  loss_ce_5: 0  loss_mask_5: 0.0332  loss_dice_5: 0.6166  loss_ce_6: 0  loss_mask_6: 0.03246  loss_dice_6: 0.5613  loss_ce_7: 0  loss_mask_7: 0.03207  loss_dice_7: 0.5279  loss_ce_8: 0  loss_mask_8: 0.03243  loss_dice_8: 0.58  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:22 d2.utils.events]: [0m eta: 0:28:59  iter: 5739  total_loss: 5.621  loss_ce: 3.161e-08  loss_mask: 0.03516  loss_dice: 0.5419  loss_ce_0: 5.11e-05  loss_mask_0: 0.03081  loss_dice_0: 0.5291  loss_ce_1: 5.72e-08  loss_mask_1: 0.03272  loss_dice_1: 0.5291  loss_ce_2: 2.187e-09  loss_mask_2: 0.03176  loss_dice_2: 0.5407  loss_ce_3: 0  loss_mask_3: 0.03333  loss_dice_3: 0.575  loss_ce_4: 0  loss_mask_4: 0.02966  loss_dice_4: 0.5183  loss_ce_5: 0  loss_mask_5: 0.03058  loss_dice_5: 0.5306  loss_ce_6: 0  loss_mask_6: 0.02918  loss_dice_6: 0.5365  loss_ce_7: 0  loss_mask_7: 0.02949  loss_dice_7: 0.5167  loss_ce_8: 0  loss_mask_8: 0.0348  loss_dice_8: 0.471  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:26 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:26:26 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:26:26 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:26:26 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:26:26 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:26:26 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:26:27 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.267875 (0.267875 s / iter per device, on 1 devices)
[32m[06/02 14:26:27 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206524 s / iter per device, on 1 devices)
[32m[06/02 14:26:27 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:26:27 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:26:27 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:26:27 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:26:27 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:26:27 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:26:27 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:26:31 d2.utils.events]: [0m eta: 0:28:51  iter: 5759  total_loss: 5.912  loss_ce: 3.883e-08  loss_mask: 0.03229  loss_dice: 0.5257  loss_ce_0: 5.087e-05  loss_mask_0: 0.03022  loss_dice_0: 0.538  loss_ce_1: 5.053e-08  loss_mask_1: 0.02977  loss_dice_1: 0.5585  loss_ce_2: 2.187e-09  loss_mask_2: 0.03248  loss_dice_2: 0.5419  loss_ce_3: 0  loss_mask_3: 0.03129  loss_dice_3: 0.5462  loss_ce_4: 0  loss_mask_4: 0.02946  loss_dice_4: 0.5254  loss_ce_5: 0  loss_mask_5: 0.03037  loss_dice_5: 0.5344  loss_ce_6: 0  loss_mask_6: 0.03321  loss_dice_6: 0.5411  loss_ce_7: 0  loss_mask_7: 0.03072  loss_dice_7: 0.5053  loss_ce_8: 0  loss_mask_8: 0.03049  loss_dice_8: 0.5577  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:39 d2.utils.events]: [0m eta: 0:28:43  iter: 5779  total_loss: 5.675  loss_ce: 3.74e-08  loss_mask: 0.03421  loss_dice: 0.5325  loss_ce_0: 5.069e-05  loss_mask_0: 0.02906  loss_dice_0: 0.5311  loss_ce_1: 5.829e-08  loss_mask_1: 0.0337  loss_dice_1: 0.5494  loss_ce_2: 2.187e-09  loss_mask_2: 0.0336  loss_dice_2: 0.5571  loss_ce_3: 0  loss_mask_3: 0.03082  loss_dice_3: 0.527  loss_ce_4: 0  loss_mask_4: 0.03087  loss_dice_4: 0.563  loss_ce_5: 0  loss_mask_5: 0.03183  loss_dice_5: 0.5579  loss_ce_6: 0  loss_mask_6: 0.03036  loss_dice_6: 0.5437  loss_ce_7: 0  loss_mask_7: 0.0322  loss_dice_7: 0.5455  loss_ce_8: 0  loss_mask_8: 0.02954  loss_dice_8: 0.5402  time: 0.4130  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:48 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:26:48 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:26:48 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:26:48 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:26:48 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:26:48 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:26:48 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.289606 (0.289606 s / iter per device, on 1 devices)
[32m[06/02 14:26:48 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.227963 s / iter per device, on 1 devices)
[32m[06/02 14:26:48 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:26:48 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:26:48 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:26:48 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:26:48 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:26:48 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:26:48 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:26:48 d2.utils.events]: [0m eta: 0:28:34  iter: 5799  total_loss: 5.886  loss_ce: 3.62e-08  loss_mask: 0.03012  loss_dice: 0.5375  loss_ce_0: 5.056e-05  loss_mask_0: 0.03158  loss_dice_0: 0.5454  loss_ce_1: 5.129e-08  loss_mask_1: 0.03004  loss_dice_1: 0.5122  loss_ce_2: 2.187e-09  loss_mask_2: 0.03495  loss_dice_2: 0.5773  loss_ce_3: 0  loss_mask_3: 0.02984  loss_dice_3: 0.5273  loss_ce_4: 0  loss_mask_4: 0.03197  loss_dice_4: 0.5394  loss_ce_5: 0  loss_mask_5: 0.03378  loss_dice_5: 0.6348  loss_ce_6: 0  loss_mask_6: 0.02986  loss_dice_6: 0.5062  loss_ce_7: 0  loss_mask_7: 0.02981  loss_dice_7: 0.5068  loss_ce_8: 0  loss_mask_8: 0.03068  loss_dice_8: 0.5868  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:26:57 d2.utils.events]: [0m eta: 0:28:26  iter: 5819  total_loss: 5.804  loss_ce: 3.314e-08  loss_mask: 0.03375  loss_dice: 0.5812  loss_ce_0: 5.034e-05  loss_mask_0: 0.03415  loss_dice_0: 0.5413  loss_ce_1: 4.987e-08  loss_mask_1: 0.03277  loss_dice_1: 0.5482  loss_ce_2: 2.187e-09  loss_mask_2: 0.03136  loss_dice_2: 0.5222  loss_ce_3: 0  loss_mask_3: 0.03103  loss_dice_3: 0.5728  loss_ce_4: 0  loss_mask_4: 0.03277  loss_dice_4: 0.5676  loss_ce_5: 0  loss_mask_5: 0.03055  loss_dice_5: 0.5331  loss_ce_6: 0  loss_mask_6: 0.03164  loss_dice_6: 0.5256  loss_ce_7: 0  loss_mask_7: 0.03206  loss_dice_7: 0.5651  loss_ce_8: 0  loss_mask_8: 0.0306  loss_dice_8: 0.5528  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:05 d2.utils.events]: [0m eta: 0:28:18  iter: 5839  total_loss: 5.848  loss_ce: 3.106e-08  loss_mask: 0.02979  loss_dice: 0.5572  loss_ce_0: 5.016e-05  loss_mask_0: 0.0293  loss_dice_0: 0.5277  loss_ce_1: 6.212e-08  loss_mask_1: 0.02805  loss_dice_1: 0.5565  loss_ce_2: 2.187e-09  loss_mask_2: 0.03109  loss_dice_2: 0.5313  loss_ce_3: 0  loss_mask_3: 0.03035  loss_dice_3: 0.5558  loss_ce_4: 0  loss_mask_4: 0.03092  loss_dice_4: 0.5224  loss_ce_5: 0  loss_mask_5: 0.03124  loss_dice_5: 0.5901  loss_ce_6: 0  loss_mask_6: 0.03063  loss_dice_6: 0.5479  loss_ce_7: 0  loss_mask_7: 0.02878  loss_dice_7: 0.5091  loss_ce_8: 0  loss_mask_8: 0.03193  loss_dice_8: 0.5465  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:09 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:27:09 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:27:09 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:27:09 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:27:09 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:27:09 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:27:10 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266755 (0.266755 s / iter per device, on 1 devices)
[32m[06/02 14:27:10 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.206553 s / iter per device, on 1 devices)
[32m[06/02 14:27:10 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:27:10 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:27:10 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:27:10 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:27:10 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:27:10 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:27:10 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:27:14 d2.utils.events]: [0m eta: 0:28:11  iter: 5859  total_loss: 5.609  loss_ce: 3.543e-08  loss_mask: 0.03068  loss_dice: 0.5169  loss_ce_0: 5.004e-05  loss_mask_0: 0.03312  loss_dice_0: 0.5668  loss_ce_1: 4.725e-08  loss_mask_1: 0.03161  loss_dice_1: 0.5252  loss_ce_2: 2.187e-09  loss_mask_2: 0.03224  loss_dice_2: 0.5333  loss_ce_3: 0  loss_mask_3: 0.02885  loss_dice_3: 0.4924  loss_ce_4: 0  loss_mask_4: 0.03261  loss_dice_4: 0.571  loss_ce_5: 0  loss_mask_5: 0.03041  loss_dice_5: 0.5205  loss_ce_6: 0  loss_mask_6: 0.02902  loss_dice_6: 0.5114  loss_ce_7: 0  loss_mask_7: 0.03203  loss_dice_7: 0.5357  loss_ce_8: 0  loss_mask_8: 0.02971  loss_dice_8: 0.5318  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:22 d2.utils.events]: [0m eta: 0:28:04  iter: 5879  total_loss: 5.806  loss_ce: 3.598e-08  loss_mask: 0.02801  loss_dice: 0.5248  loss_ce_0: 4.982e-05  loss_mask_0: 0.03177  loss_dice_0: 0.5636  loss_ce_1: 5.96e-08  loss_mask_1: 0.0306  loss_dice_1: 0.5182  loss_ce_2: 2.187e-09  loss_mask_2: 0.03123  loss_dice_2: 0.5289  loss_ce_3: 0  loss_mask_3: 0.03166  loss_dice_3: 0.6244  loss_ce_4: 0  loss_mask_4: 0.02965  loss_dice_4: 0.5186  loss_ce_5: 0  loss_mask_5: 0.03175  loss_dice_5: 0.5166  loss_ce_6: 0  loss_mask_6: 0.03215  loss_dice_6: 0.5403  loss_ce_7: 0  loss_mask_7: 0.03273  loss_dice_7: 0.5886  loss_ce_8: 0  loss_mask_8: 0.03132  loss_dice_8: 0.5319  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:30 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:27:30 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:27:30 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:27:30 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:27:30 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:27:30 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:27:31 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.281905 (0.281905 s / iter per device, on 1 devices)
[32m[06/02 14:27:31 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217480 s / iter per device, on 1 devices)
[32m[06/02 14:27:31 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:27:31 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:27:31 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:27:31 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:27:31 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:27:31 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:27:31 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:27:31 d2.utils.events]: [0m eta: 0:27:56  iter: 5899  total_loss: 5.685  loss_ce: 3.981e-08  loss_mask: 0.03216  loss_dice: 0.5214  loss_ce_0: 4.967e-05  loss_mask_0: 0.03169  loss_dice_0: 0.5599  loss_ce_1: 4.582e-08  loss_mask_1: 0.03005  loss_dice_1: 0.5207  loss_ce_2: 2.187e-09  loss_mask_2: 0.03192  loss_dice_2: 0.4972  loss_ce_3: 0  loss_mask_3: 0.03469  loss_dice_3: 0.5467  loss_ce_4: 0  loss_mask_4: 0.03351  loss_dice_4: 0.5636  loss_ce_5: 0  loss_mask_5: 0.03281  loss_dice_5: 0.5454  loss_ce_6: 0  loss_mask_6: 0.03314  loss_dice_6: 0.5276  loss_ce_7: 0  loss_mask_7: 0.03349  loss_dice_7: 0.5426  loss_ce_8: 0  loss_mask_8: 0.02961  loss_dice_8: 0.5137  time: 0.4130  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:39 d2.utils.events]: [0m eta: 0:27:48  iter: 5919  total_loss: 5.63  loss_ce: 4.429e-08  loss_mask: 0.03093  loss_dice: 0.5186  loss_ce_0: 4.949e-05  loss_mask_0: 0.0313  loss_dice_0: 0.5414  loss_ce_1: 4.429e-08  loss_mask_1: 0.03031  loss_dice_1: 0.5146  loss_ce_2: 2.187e-09  loss_mask_2: 0.03019  loss_dice_2: 0.5416  loss_ce_3: 0  loss_mask_3: 0.02961  loss_dice_3: 0.507  loss_ce_4: 0  loss_mask_4: 0.03189  loss_dice_4: 0.5663  loss_ce_5: 0  loss_mask_5: 0.03158  loss_dice_5: 0.5317  loss_ce_6: 0  loss_mask_6: 0.02998  loss_dice_6: 0.5097  loss_ce_7: 0  loss_mask_7: 0.02915  loss_dice_7: 0.4922  loss_ce_8: 0  loss_mask_8: 0.0289  loss_dice_8: 0.5096  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:48 d2.utils.events]: [0m eta: 0:27:40  iter: 5939  total_loss: 5.693  loss_ce: 3.423e-08  loss_mask: 0.03147  loss_dice: 0.5387  loss_ce_0: 4.93e-05  loss_mask_0: 0.03082  loss_dice_0: 0.5468  loss_ce_1: 4.462e-08  loss_mask_1: 0.02953  loss_dice_1: 0.5205  loss_ce_2: 2.187e-09  loss_mask_2: 0.02989  loss_dice_2: 0.5248  loss_ce_3: 0  loss_mask_3: 0.03024  loss_dice_3: 0.5752  loss_ce_4: 0  loss_mask_4: 0.03138  loss_dice_4: 0.5235  loss_ce_5: 0  loss_mask_5: 0.03092  loss_dice_5: 0.585  loss_ce_6: 0  loss_mask_6: 0.0327  loss_dice_6: 0.5375  loss_ce_7: 0  loss_mask_7: 0.03161  loss_dice_7: 0.5584  loss_ce_8: 0  loss_mask_8: 0.03399  loss_dice_8: 0.5779  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:27:52 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:27:52 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:27:52 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:27:52 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:27:52 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:27:52 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:27:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.289017 (0.289017 s / iter per device, on 1 devices)
[32m[06/02 14:27:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.227433 s / iter per device, on 1 devices)
[32m[06/02 14:27:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:27:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:27:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:27:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:27:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:27:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:27:52 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:27:57 d2.utils.events]: [0m eta: 0:27:32  iter: 5959  total_loss: 5.686  loss_ce: 3.664e-08  loss_mask: 0.02984  loss_dice: 0.5446  loss_ce_0: 4.913e-05  loss_mask_0: 0.03196  loss_dice_0: 0.5374  loss_ce_1: 4.615e-08  loss_mask_1: 0.02805  loss_dice_1: 0.5409  loss_ce_2: 2.187e-09  loss_mask_2: 0.029  loss_dice_2: 0.5459  loss_ce_3: 0  loss_mask_3: 0.03032  loss_dice_3: 0.55  loss_ce_4: 0  loss_mask_4: 0.02876  loss_dice_4: 0.5358  loss_ce_5: 0  loss_mask_5: 0.03144  loss_dice_5: 0.5267  loss_ce_6: 0  loss_mask_6: 0.03141  loss_dice_6: 0.5247  loss_ce_7: 0  loss_mask_7: 0.03076  loss_dice_7: 0.5304  loss_ce_8: 0  loss_mask_8: 0.02899  loss_dice_8: 0.5395  time: 0.4129  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:05 d2.utils.events]: [0m eta: 0:27:24  iter: 5979  total_loss: 5.772  loss_ce: 3.489e-08  loss_mask: 0.03239  loss_dice: 0.5832  loss_ce_0: 4.9e-05  loss_mask_0: 0.03043  loss_dice_0: 0.5304  loss_ce_1: 5.928e-08  loss_mask_1: 0.03253  loss_dice_1: 0.5617  loss_ce_2: 2.187e-09  loss_mask_2: 0.03328  loss_dice_2: 0.5798  loss_ce_3: 0  loss_mask_3: 0.03122  loss_dice_3: 0.5595  loss_ce_4: 0  loss_mask_4: 0.03016  loss_dice_4: 0.52  loss_ce_5: 0  loss_mask_5: 0.03216  loss_dice_5: 0.5134  loss_ce_6: 0  loss_mask_6: 0.03198  loss_dice_6: 0.5428  loss_ce_7: 0  loss_mask_7: 0.03573  loss_dice_7: 0.5581  loss_ce_8: 0  loss_mask_8: 0.03045  loss_dice_8: 0.521  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:13 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0005999.pth
[32m[06/02 14:28:16 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:28:16 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:28:16 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:28:16 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:28:16 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:28:16 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:28:17 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.283689 (0.283689 s / iter per device, on 1 devices)
[32m[06/02 14:28:17 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.221582 s / iter per device, on 1 devices)
[32m[06/02 14:28:17 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:28:17 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:28:17 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:28:17 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:28:17 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:28:17 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:28:17 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:28:17 d2.utils.events]: [0m eta: 0:27:16  iter: 5999  total_loss: 5.552  loss_ce: 3.718e-08  loss_mask: 0.03059  loss_dice: 0.4975  loss_ce_0: 4.877e-05  loss_mask_0: 0.03023  loss_dice_0: 0.5174  loss_ce_1: 4.921e-08  loss_mask_1: 0.02916  loss_dice_1: 0.5362  loss_ce_2: 2.187e-09  loss_mask_2: 0.03212  loss_dice_2: 0.5515  loss_ce_3: 0  loss_mask_3: 0.02857  loss_dice_3: 0.4965  loss_ce_4: 0  loss_mask_4: 0.02898  loss_dice_4: 0.5454  loss_ce_5: 0  loss_mask_5: 0.02984  loss_dice_5: 0.5325  loss_ce_6: 0  loss_mask_6: 0.02823  loss_dice_6: 0.5006  loss_ce_7: 0  loss_mask_7: 0.03124  loss_dice_7: 0.5198  loss_ce_8: 0  loss_mask_8: 0.02865  loss_dice_8: 0.5123  time: 0.4129  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:25 d2.utils.events]: [0m eta: 0:27:09  iter: 6019  total_loss: 5.74  loss_ce: 3.576e-08  loss_mask: 0.03185  loss_dice: 0.5177  loss_ce_0: 4.864e-05  loss_mask_0: 0.03054  loss_dice_0: 0.5973  loss_ce_1: 4.462e-08  loss_mask_1: 0.02876  loss_dice_1: 0.4993  loss_ce_2: 2.187e-09  loss_mask_2: 0.02796  loss_dice_2: 0.4963  loss_ce_3: 0  loss_mask_3: 0.03279  loss_dice_3: 0.5797  loss_ce_4: 0  loss_mask_4: 0.03031  loss_dice_4: 0.5465  loss_ce_5: 0  loss_mask_5: 0.03159  loss_dice_5: 0.5095  loss_ce_6: 0  loss_mask_6: 0.03149  loss_dice_6: 0.5283  loss_ce_7: 0  loss_mask_7: 0.02949  loss_dice_7: 0.5238  loss_ce_8: 0  loss_mask_8: 0.03009  loss_dice_8: 0.5813  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:34 d2.utils.events]: [0m eta: 0:27:01  iter: 6039  total_loss: 5.475  loss_ce: 3.204e-08  loss_mask: 0.03241  loss_dice: 0.5593  loss_ce_0: 4.855e-05  loss_mask_0: 0.03147  loss_dice_0: 0.5101  loss_ce_1: 4.473e-08  loss_mask_1: 0.02988  loss_dice_1: 0.4872  loss_ce_2: 2.187e-09  loss_mask_2: 0.02975  loss_dice_2: 0.5032  loss_ce_3: 0  loss_mask_3: 0.03139  loss_dice_3: 0.5319  loss_ce_4: 0  loss_mask_4: 0.03148  loss_dice_4: 0.5061  loss_ce_5: 0  loss_mask_5: 0.03203  loss_dice_5: 0.5077  loss_ce_6: 0  loss_mask_6: 0.02775  loss_dice_6: 0.5007  loss_ce_7: 0  loss_mask_7: 0.03282  loss_dice_7: 0.5473  loss_ce_8: 0  loss_mask_8: 0.0338  loss_dice_8: 0.4893  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:38 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:28:38 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:28:38 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:28:38 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:28:38 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:28:38 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:28:38 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.258489 (0.258489 s / iter per device, on 1 devices)
[32m[06/02 14:28:38 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.198576 s / iter per device, on 1 devices)
[32m[06/02 14:28:38 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:28:38 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:28:39 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:28:39 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:28:39 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:28:39 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:28:39 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:28:43 d2.utils.events]: [0m eta: 0:26:53  iter: 6059  total_loss: 5.782  loss_ce: 2.909e-08  loss_mask: 0.03196  loss_dice: 0.5517  loss_ce_0: 4.828e-05  loss_mask_0: 0.03342  loss_dice_0: 0.5567  loss_ce_1: 4.429e-08  loss_mask_1: 0.03016  loss_dice_1: 0.5195  loss_ce_2: 2.187e-09  loss_mask_2: 0.03049  loss_dice_2: 0.539  loss_ce_3: 0  loss_mask_3: 0.02988  loss_dice_3: 0.5208  loss_ce_4: 0  loss_mask_4: 0.03086  loss_dice_4: 0.5351  loss_ce_5: 0  loss_mask_5: 0.03545  loss_dice_5: 0.5798  loss_ce_6: 0  loss_mask_6: 0.02938  loss_dice_6: 0.4976  loss_ce_7: 0  loss_mask_7: 0.03345  loss_dice_7: 0.5942  loss_ce_8: 0  loss_mask_8: 0.02827  loss_dice_8: 0.4835  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:51 d2.utils.events]: [0m eta: 0:26:44  iter: 6079  total_loss: 5.53  loss_ce: 2.909e-08  loss_mask: 0.02795  loss_dice: 0.4846  loss_ce_0: 4.803e-05  loss_mask_0: 0.03079  loss_dice_0: 0.5419  loss_ce_1: 4.189e-08  loss_mask_1: 0.02771  loss_dice_1: 0.5024  loss_ce_2: 2.187e-09  loss_mask_2: 0.03158  loss_dice_2: 0.5351  loss_ce_3: 0  loss_mask_3: 0.03397  loss_dice_3: 0.552  loss_ce_4: 0  loss_mask_4: 0.03072  loss_dice_4: 0.5289  loss_ce_5: 0  loss_mask_5: 0.03206  loss_dice_5: 0.5223  loss_ce_6: 0  loss_mask_6: 0.03147  loss_dice_6: 0.5439  loss_ce_7: 0  loss_mask_7: 0.03114  loss_dice_7: 0.4943  loss_ce_8: 0  loss_mask_8: 0.02803  loss_dice_8: 0.5159  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:28:59 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:28:59 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:28:59 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:28:59 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:28:59 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:28:59 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:29:00 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264738 (0.264738 s / iter per device, on 1 devices)
[32m[06/02 14:29:00 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.202006 s / iter per device, on 1 devices)
[32m[06/02 14:29:00 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:29:00 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:29:00 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:29:00 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:29:00 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:29:00 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:29:00 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:29:00 d2.utils.events]: [0m eta: 0:26:36  iter: 6099  total_loss: 5.53  loss_ce: 1.925e-08  loss_mask: 0.02791  loss_dice: 0.4818  loss_ce_0: 4.787e-05  loss_mask_0: 0.02812  loss_dice_0: 0.5324  loss_ce_1: 4.243e-08  loss_mask_1: 0.03077  loss_dice_1: 0.5553  loss_ce_2: 2.187e-09  loss_mask_2: 0.02943  loss_dice_2: 0.5785  loss_ce_3: 0  loss_mask_3: 0.0289  loss_dice_3: 0.5057  loss_ce_4: 0  loss_mask_4: 0.03021  loss_dice_4: 0.5007  loss_ce_5: 0  loss_mask_5: 0.02922  loss_dice_5: 0.5278  loss_ce_6: 0  loss_mask_6: 0.02942  loss_dice_6: 0.4759  loss_ce_7: 0  loss_mask_7: 0.02997  loss_dice_7: 0.5369  loss_ce_8: 0  loss_mask_8: 0.03111  loss_dice_8: 0.5537  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:29:08 d2.utils.events]: [0m eta: 0:26:28  iter: 6119  total_loss: 5.699  loss_ce: 1.608e-08  loss_mask: 0.03127  loss_dice: 0.5574  loss_ce_0: 4.773e-05  loss_mask_0: 0.0314  loss_dice_0: 0.5227  loss_ce_1: 4.222e-08  loss_mask_1: 0.02921  loss_dice_1: 0.5449  loss_ce_2: 2.187e-09  loss_mask_2: 0.03153  loss_dice_2: 0.5609  loss_ce_3: 0  loss_mask_3: 0.03301  loss_dice_3: 0.5498  loss_ce_4: 0  loss_mask_4: 0.02815  loss_dice_4: 0.4832  loss_ce_5: 0  loss_mask_5: 0.03064  loss_dice_5: 0.5262  loss_ce_6: 0  loss_mask_6: 0.03106  loss_dice_6: 0.5374  loss_ce_7: 0  loss_mask_7: 0.02858  loss_dice_7: 0.5379  loss_ce_8: 0  loss_mask_8: 0.03266  loss_dice_8: 0.5067  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:29:16 d2.utils.events]: [0m eta: 0:26:20  iter: 6139  total_loss: 5.57  loss_ce: 1.936e-08  loss_mask: 0.03052  loss_dice: 0.5463  loss_ce_0: 4.753e-05  loss_mask_0: 0.0304  loss_dice_0: 0.5116  loss_ce_1: 4.2e-08  loss_mask_1: 0.03465  loss_dice_1: 0.5365  loss_ce_2: 2.187e-09  loss_mask_2: 0.02931  loss_dice_2: 0.531  loss_ce_3: 0  loss_mask_3: 0.03048  loss_dice_3: 0.531  loss_ce_4: 0  loss_mask_4: 0.0338  loss_dice_4: 0.5505  loss_ce_5: 0  loss_mask_5: 0.03178  loss_dice_5: 0.5029  loss_ce_6: 0  loss_mask_6: 0.02959  loss_dice_6: 0.5134  loss_ce_7: 0  loss_mask_7: 0.03161  loss_dice_7: 0.5375  loss_ce_8: 0  loss_mask_8: 0.02884  loss_dice_8: 0.5098  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:29:21 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:29:21 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:29:21 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:29:21 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:29:21 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:29:21 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:29:21 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266558 (0.266558 s / iter per device, on 1 devices)
[32m[06/02 14:29:21 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205130 s / iter per device, on 1 devices)
[32m[06/02 14:29:21 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:29:21 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:29:21 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:29:21 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:29:21 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:29:21 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:29:21 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:29:25 d2.utils.events]: [0m eta: 0:26:12  iter: 6159  total_loss: 5.563  loss_ce: 1.498e-08  loss_mask: 0.03274  loss_dice: 0.5098  loss_ce_0: 4.734e-05  loss_mask_0: 0.03221  loss_dice_0: 0.5261  loss_ce_1: 4.582e-08  loss_mask_1: 0.03406  loss_dice_1: 0.5649  loss_ce_2: 2.187e-09  loss_mask_2: 0.03414  loss_dice_2: 0.5872  loss_ce_3: 0  loss_mask_3: 0.02918  loss_dice_3: 0.513  loss_ce_4: 0  loss_mask_4: 0.03134  loss_dice_4: 0.5296  loss_ce_5: 0  loss_mask_5: 0.03204  loss_dice_5: 0.5513  loss_ce_6: 0  loss_mask_6: 0.03103  loss_dice_6: 0.5402  loss_ce_7: 0  loss_mask_7: 0.03075  loss_dice_7: 0.487  loss_ce_8: 0  loss_mask_8: 0.03444  loss_dice_8: 0.5356  time: 0.4128  data_time: 0.0058  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:29:34 d2.utils.events]: [0m eta: 0:26:04  iter: 6179  total_loss: 5.573  loss_ce: 1.378e-08  loss_mask: 0.03298  loss_dice: 0.5278  loss_ce_0: 4.709e-05  loss_mask_0: 0.0319  loss_dice_0: 0.5718  loss_ce_1: 5.14e-08  loss_mask_1: 0.03143  loss_dice_1: 0.5362  loss_ce_2: 2.187e-09  loss_mask_2: 0.02907  loss_dice_2: 0.4991  loss_ce_3: 0  loss_mask_3: 0.0326  loss_dice_3: 0.5216  loss_ce_4: 0  loss_mask_4: 0.03072  loss_dice_4: 0.4832  loss_ce_5: 0  loss_mask_5: 0.03228  loss_dice_5: 0.5956  loss_ce_6: 0  loss_mask_6: 0.03296  loss_dice_6: 0.5013  loss_ce_7: 0  loss_mask_7: 0.03206  loss_dice_7: 0.4876  loss_ce_8: 0  loss_mask_8: 0.03182  loss_dice_8: 0.5147  time: 0.4128  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:29:42 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:29:42 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:29:42 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:29:42 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:29:42 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:29:42 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:29:43 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271693 (0.271693 s / iter per device, on 1 devices)
[32m[06/02 14:29:43 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210691 s / iter per device, on 1 devices)
[32m[06/02 14:29:43 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:29:43 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:29:43 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:29:43 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:29:43 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:29:43 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:29:43 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:29:43 d2.utils.events]: [0m eta: 0:25:56  iter: 6199  total_loss: 5.721  loss_ce: 1.444e-08  loss_mask: 0.02764  loss_dice: 0.5367  loss_ce_0: 4.689e-05  loss_mask_0: 0.03033  loss_dice_0: 0.5416  loss_ce_1: 3.981e-08  loss_mask_1: 0.03292  loss_dice_1: 0.5244  loss_ce_2: 2.187e-09  loss_mask_2: 0.03386  loss_dice_2: 0.5432  loss_ce_3: 0  loss_mask_3: 0.03135  loss_dice_3: 0.536  loss_ce_4: 0  loss_mask_4: 0.03324  loss_dice_4: 0.5665  loss_ce_5: 0  loss_mask_5: 0.03316  loss_dice_5: 0.5284  loss_ce_6: 0  loss_mask_6: 0.03142  loss_dice_6: 0.5266  loss_ce_7: 0  loss_mask_7: 0.03077  loss_dice_7: 0.5169  loss_ce_8: 0  loss_mask_8: 0.03176  loss_dice_8: 0.5267  time: 0.4128  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:29:51 d2.utils.events]: [0m eta: 0:25:48  iter: 6219  total_loss: 5.67  loss_ce: 1.531e-08  loss_mask: 0.02948  loss_dice: 0.5326  loss_ce_0: 4.672e-05  loss_mask_0: 0.02997  loss_dice_0: 0.5459  loss_ce_1: 3.97e-08  loss_mask_1: 0.02826  loss_dice_1: 0.4896  loss_ce_2: 0  loss_mask_2: 0.03433  loss_dice_2: 0.5599  loss_ce_3: 0  loss_mask_3: 0.03258  loss_dice_3: 0.5337  loss_ce_4: 0  loss_mask_4: 0.03109  loss_dice_4: 0.5042  loss_ce_5: 0  loss_mask_5: 0.03083  loss_dice_5: 0.5649  loss_ce_6: 0  loss_mask_6: 0.03146  loss_dice_6: 0.5632  loss_ce_7: 0  loss_mask_7: 0.02909  loss_dice_7: 0.5219  loss_ce_8: 0  loss_mask_8: 0.03021  loss_dice_8: 0.5231  time: 0.4129  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:00 d2.utils.events]: [0m eta: 0:25:40  iter: 6239  total_loss: 5.786  loss_ce: 1.531e-08  loss_mask: 0.02938  loss_dice: 0.5131  loss_ce_0: 4.656e-05  loss_mask_0: 0.03303  loss_dice_0: 0.5598  loss_ce_1: 4.134e-08  loss_mask_1: 0.03336  loss_dice_1: 0.5593  loss_ce_2: 0  loss_mask_2: 0.03275  loss_dice_2: 0.5505  loss_ce_3: 0  loss_mask_3: 0.03021  loss_dice_3: 0.5294  loss_ce_4: 0  loss_mask_4: 0.03262  loss_dice_4: 0.5511  loss_ce_5: 0  loss_mask_5: 0.03281  loss_dice_5: 0.5355  loss_ce_6: 0  loss_mask_6: 0.02949  loss_dice_6: 0.5542  loss_ce_7: 0  loss_mask_7: 0.03441  loss_dice_7: 0.6118  loss_ce_8: 0  loss_mask_8: 0.03295  loss_dice_8: 0.5758  time: 0.4128  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:04 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:30:04 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:30:04 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:30:04 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:30:04 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:30:04 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:30:04 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.283491 (0.283491 s / iter per device, on 1 devices)
[32m[06/02 14:30:04 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.219769 s / iter per device, on 1 devices)
[32m[06/02 14:30:05 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:30:05 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:30:05 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:30:05 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:30:05 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:30:05 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:30:05 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:30:09 d2.utils.events]: [0m eta: 0:25:32  iter: 6259  total_loss: 5.571  loss_ce: 1.444e-08  loss_mask: 0.03057  loss_dice: 0.5293  loss_ce_0: 4.641e-05  loss_mask_0: 0.02914  loss_dice_0: 0.4898  loss_ce_1: 4.068e-08  loss_mask_1: 0.02862  loss_dice_1: 0.5212  loss_ce_2: 0  loss_mask_2: 0.02634  loss_dice_2: 0.4956  loss_ce_3: 0  loss_mask_3: 0.03148  loss_dice_3: 0.5376  loss_ce_4: 0  loss_mask_4: 0.03099  loss_dice_4: 0.5303  loss_ce_5: 0  loss_mask_5: 0.03251  loss_dice_5: 0.5037  loss_ce_6: 0  loss_mask_6: 0.02981  loss_dice_6: 0.5093  loss_ce_7: 0  loss_mask_7: 0.03019  loss_dice_7: 0.539  loss_ce_8: 0  loss_mask_8: 0.03223  loss_dice_8: 0.5459  time: 0.4129  data_time: 0.0081  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:17 d2.utils.events]: [0m eta: 0:25:24  iter: 6279  total_loss: 5.592  loss_ce: 1.63e-08  loss_mask: 0.03207  loss_dice: 0.5152  loss_ce_0: 4.621e-05  loss_mask_0: 0.02972  loss_dice_0: 0.493  loss_ce_1: 3.926e-08  loss_mask_1: 0.03144  loss_dice_1: 0.5379  loss_ce_2: 0  loss_mask_2: 0.03216  loss_dice_2: 0.5311  loss_ce_3: 0  loss_mask_3: 0.0324  loss_dice_3: 0.5049  loss_ce_4: 0  loss_mask_4: 0.03285  loss_dice_4: 0.565  loss_ce_5: 0  loss_mask_5: 0.02919  loss_dice_5: 0.5298  loss_ce_6: 0  loss_mask_6: 0.02809  loss_dice_6: 0.4886  loss_ce_7: 0  loss_mask_7: 0.03225  loss_dice_7: 0.5578  loss_ce_8: 0  loss_mask_8: 0.03065  loss_dice_8: 0.5313  time: 0.4129  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:25 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:30:25 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:30:25 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:30:25 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:30:25 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:30:25 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:30:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.283803 (0.283803 s / iter per device, on 1 devices)
[32m[06/02 14:30:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.222692 s / iter per device, on 1 devices)
[32m[06/02 14:30:26 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:30:26 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:30:26 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:30:26 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:30:26 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:30:26 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:30:26 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:30:26 d2.utils.events]: [0m eta: 0:25:16  iter: 6299  total_loss: 5.404  loss_ce: 1.531e-08  loss_mask: 0.02685  loss_dice: 0.4734  loss_ce_0: 4.603e-05  loss_mask_0: 0.03299  loss_dice_0: 0.5106  loss_ce_1: 4.254e-08  loss_mask_1: 0.03112  loss_dice_1: 0.5504  loss_ce_2: 0  loss_mask_2: 0.03031  loss_dice_2: 0.5305  loss_ce_3: 0  loss_mask_3: 0.03221  loss_dice_3: 0.5545  loss_ce_4: 0  loss_mask_4: 0.02882  loss_dice_4: 0.5121  loss_ce_5: 0  loss_mask_5: 0.02921  loss_dice_5: 0.4866  loss_ce_6: 0  loss_mask_6: 0.03353  loss_dice_6: 0.5361  loss_ce_7: 0  loss_mask_7: 0.02887  loss_dice_7: 0.4933  loss_ce_8: 0  loss_mask_8: 0.03114  loss_dice_8: 0.4917  time: 0.4129  data_time: 0.0080  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:34 d2.utils.events]: [0m eta: 0:25:08  iter: 6319  total_loss: 5.696  loss_ce: 1.531e-08  loss_mask: 0.03249  loss_dice: 0.5364  loss_ce_0: 4.575e-05  loss_mask_0: 0.03195  loss_dice_0: 0.5299  loss_ce_1: 4.287e-08  loss_mask_1: 0.03223  loss_dice_1: 0.5709  loss_ce_2: 0  loss_mask_2: 0.03134  loss_dice_2: 0.5314  loss_ce_3: 0  loss_mask_3: 0.0303  loss_dice_3: 0.5997  loss_ce_4: 0  loss_mask_4: 0.03177  loss_dice_4: 0.5575  loss_ce_5: 0  loss_mask_5: 0.032  loss_dice_5: 0.5311  loss_ce_6: 0  loss_mask_6: 0.03304  loss_dice_6: 0.5609  loss_ce_7: 0  loss_mask_7: 0.03089  loss_dice_7: 0.5349  loss_ce_8: 0  loss_mask_8: 0.03299  loss_dice_8: 0.5586  time: 0.4129  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:43 d2.utils.events]: [0m eta: 0:25:00  iter: 6339  total_loss: 5.584  loss_ce: 1.444e-08  loss_mask: 0.02994  loss_dice: 0.5272  loss_ce_0: 4.555e-05  loss_mask_0: 0.03121  loss_dice_0: 0.5336  loss_ce_1: 5.414e-08  loss_mask_1: 0.02789  loss_dice_1: 0.4707  loss_ce_2: 0  loss_mask_2: 0.02965  loss_dice_2: 0.4969  loss_ce_3: 0  loss_mask_3: 0.03267  loss_dice_3: 0.5555  loss_ce_4: 0  loss_mask_4: 0.02607  loss_dice_4: 0.537  loss_ce_5: 0  loss_mask_5: 0.0303  loss_dice_5: 0.5552  loss_ce_6: 0  loss_mask_6: 0.03279  loss_dice_6: 0.5904  loss_ce_7: 0  loss_mask_7: 0.03013  loss_dice_7: 0.5376  loss_ce_8: 0  loss_mask_8: 0.02772  loss_dice_8: 0.5097  time: 0.4129  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:30:47 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:30:47 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:30:47 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:30:47 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:30:47 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:30:47 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:30:48 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.248830 (0.248830 s / iter per device, on 1 devices)
[32m[06/02 14:30:48 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.187222 s / iter per device, on 1 devices)
[32m[06/02 14:30:48 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:30:48 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:30:48 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:30:48 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:30:48 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:30:48 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:30:48 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:30:52 d2.utils.events]: [0m eta: 0:24:52  iter: 6359  total_loss: 5.755  loss_ce: 1.531e-08  loss_mask: 0.03633  loss_dice: 0.5311  loss_ce_0: 4.548e-05  loss_mask_0: 0.03286  loss_dice_0: 0.5346  loss_ce_1: 4.068e-08  loss_mask_1: 0.03294  loss_dice_1: 0.4862  loss_ce_2: 0  loss_mask_2: 0.03276  loss_dice_2: 0.5453  loss_ce_3: 0  loss_mask_3: 0.03129  loss_dice_3: 0.5513  loss_ce_4: 0  loss_mask_4: 0.03207  loss_dice_4: 0.5217  loss_ce_5: 0  loss_mask_5: 0.03119  loss_dice_5: 0.5108  loss_ce_6: 0  loss_mask_6: 0.03084  loss_dice_6: 0.5486  loss_ce_7: 0  loss_mask_7: 0.03477  loss_dice_7: 0.5437  loss_ce_8: 0  loss_mask_8: 0.03191  loss_dice_8: 0.5428  time: 0.4129  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:00 d2.utils.events]: [0m eta: 0:24:44  iter: 6379  total_loss: 5.587  loss_ce: 1.794e-08  loss_mask: 0.03229  loss_dice: 0.5468  loss_ce_0: 4.525e-05  loss_mask_0: 0.03254  loss_dice_0: 0.5439  loss_ce_1: 3.981e-08  loss_mask_1: 0.02977  loss_dice_1: 0.5276  loss_ce_2: 0  loss_mask_2: 0.0326  loss_dice_2: 0.5209  loss_ce_3: 0  loss_mask_3: 0.03257  loss_dice_3: 0.5224  loss_ce_4: 0  loss_mask_4: 0.03031  loss_dice_4: 0.5289  loss_ce_5: 0  loss_mask_5: 0.03179  loss_dice_5: 0.5134  loss_ce_6: 0  loss_mask_6: 0.03075  loss_dice_6: 0.5627  loss_ce_7: 0  loss_mask_7: 0.02834  loss_dice_7: 0.5054  loss_ce_8: 0  loss_mask_8: 0.03218  loss_dice_8: 0.5179  time: 0.4129  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:09 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:31:09 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:31:09 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:31:09 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:31:09 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:31:09 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:31:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277149 (0.277149 s / iter per device, on 1 devices)
[32m[06/02 14:31:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.218789 s / iter per device, on 1 devices)
[32m[06/02 14:31:09 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:31:09 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:31:09 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:31:09 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:31:09 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:31:09 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:31:09 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:31:09 d2.utils.events]: [0m eta: 0:24:36  iter: 6399  total_loss: 5.535  loss_ce: 1.236e-08  loss_mask: 0.02844  loss_dice: 0.4727  loss_ce_0: 4.51e-05  loss_mask_0: 0.03165  loss_dice_0: 0.5399  loss_ce_1: 4.418e-08  loss_mask_1: 0.0297  loss_dice_1: 0.4954  loss_ce_2: 0  loss_mask_2: 0.03594  loss_dice_2: 0.573  loss_ce_3: 0  loss_mask_3: 0.03089  loss_dice_3: 0.53  loss_ce_4: 0  loss_mask_4: 0.02924  loss_dice_4: 0.5061  loss_ce_5: 0  loss_mask_5: 0.03162  loss_dice_5: 0.5661  loss_ce_6: 0  loss_mask_6: 0.02908  loss_dice_6: 0.5016  loss_ce_7: 0  loss_mask_7: 0.03071  loss_dice_7: 0.5085  loss_ce_8: 0  loss_mask_8: 0.03094  loss_dice_8: 0.5292  time: 0.4129  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:18 d2.utils.events]: [0m eta: 0:24:28  iter: 6419  total_loss: 5.731  loss_ce: 1.192e-08  loss_mask: 0.03148  loss_dice: 0.5571  loss_ce_0: 4.496e-05  loss_mask_0: 0.02984  loss_dice_0: 0.5658  loss_ce_1: 3.828e-08  loss_mask_1: 0.03037  loss_dice_1: 0.5274  loss_ce_2: 0  loss_mask_2: 0.03119  loss_dice_2: 0.5286  loss_ce_3: 0  loss_mask_3: 0.03142  loss_dice_3: 0.5357  loss_ce_4: 0  loss_mask_4: 0.03147  loss_dice_4: 0.5563  loss_ce_5: 0  loss_mask_5: 0.03332  loss_dice_5: 0.5201  loss_ce_6: 0  loss_mask_6: 0.02942  loss_dice_6: 0.5178  loss_ce_7: 0  loss_mask_7: 0.03073  loss_dice_7: 0.5268  loss_ce_8: 0  loss_mask_8: 0.03267  loss_dice_8: 0.5181  time: 0.4129  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:26 d2.utils.events]: [0m eta: 0:24:20  iter: 6439  total_loss: 5.663  loss_ce: 1.192e-08  loss_mask: 0.0292  loss_dice: 0.513  loss_ce_0: 4.484e-05  loss_mask_0: 0.0287  loss_dice_0: 0.5068  loss_ce_1: 3.937e-08  loss_mask_1: 0.03504  loss_dice_1: 0.5624  loss_ce_2: 0  loss_mask_2: 0.03097  loss_dice_2: 0.549  loss_ce_3: 0  loss_mask_3: 0.03428  loss_dice_3: 0.5637  loss_ce_4: 0  loss_mask_4: 0.03221  loss_dice_4: 0.5248  loss_ce_5: 0  loss_mask_5: 0.03432  loss_dice_5: 0.5418  loss_ce_6: 0  loss_mask_6: 0.03111  loss_dice_6: 0.4973  loss_ce_7: 0  loss_mask_7: 0.03157  loss_dice_7: 0.5067  loss_ce_8: 0  loss_mask_8: 0.03397  loss_dice_8: 0.5009  time: 0.4129  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:30 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:31:30 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:31:30 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:31:30 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:31:30 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:31:30 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:31:31 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274757 (0.274757 s / iter per device, on 1 devices)
[32m[06/02 14:31:31 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.214357 s / iter per device, on 1 devices)
[32m[06/02 14:31:31 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:31:31 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:31:31 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:31:31 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:31:31 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:31:31 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:31:31 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:31:35 d2.utils.events]: [0m eta: 0:24:12  iter: 6459  total_loss: 5.76  loss_ce: 1.269e-08  loss_mask: 0.0296  loss_dice: 0.5258  loss_ce_0: 4.458e-05  loss_mask_0: 0.03166  loss_dice_0: 0.5442  loss_ce_1: 4.451e-08  loss_mask_1: 0.0284  loss_dice_1: 0.5023  loss_ce_2: 0  loss_mask_2: 0.02979  loss_dice_2: 0.5267  loss_ce_3: 0  loss_mask_3: 0.03097  loss_dice_3: 0.5324  loss_ce_4: 0  loss_mask_4: 0.0326  loss_dice_4: 0.5254  loss_ce_5: 0  loss_mask_5: 0.03058  loss_dice_5: 0.5355  loss_ce_6: 0  loss_mask_6: 0.02996  loss_dice_6: 0.5441  loss_ce_7: 0  loss_mask_7: 0.03117  loss_dice_7: 0.5245  loss_ce_8: 0  loss_mask_8: 0.0312  loss_dice_8: 0.5429  time: 0.4129  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:43 d2.utils.events]: [0m eta: 0:24:04  iter: 6479  total_loss: 5.581  loss_ce: 1.4e-08  loss_mask: 0.03202  loss_dice: 0.5205  loss_ce_0: 4.44e-05  loss_mask_0: 0.03098  loss_dice_0: 0.5202  loss_ce_1: 4.998e-08  loss_mask_1: 0.03007  loss_dice_1: 0.5467  loss_ce_2: 0  loss_mask_2: 0.03101  loss_dice_2: 0.5118  loss_ce_3: 0  loss_mask_3: 0.0323  loss_dice_3: 0.512  loss_ce_4: 0  loss_mask_4: 0.02917  loss_dice_4: 0.4568  loss_ce_5: 0  loss_mask_5: 0.03134  loss_dice_5: 0.5312  loss_ce_6: 0  loss_mask_6: 0.02931  loss_dice_6: 0.5107  loss_ce_7: 0  loss_mask_7: 0.03021  loss_dice_7: 0.5639  loss_ce_8: 0  loss_mask_8: 0.02981  loss_dice_8: 0.5338  time: 0.4129  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:31:52 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:31:52 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:31:52 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:31:52 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:31:52 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:31:52 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:31:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264654 (0.264654 s / iter per device, on 1 devices)
[32m[06/02 14:31:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205612 s / iter per device, on 1 devices)
[32m[06/02 14:31:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:31:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:31:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:31:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:31:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:31:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:31:52 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:31:53 d2.utils.events]: [0m eta: 0:23:56  iter: 6499  total_loss: 5.762  loss_ce: 1.684e-08  loss_mask: 0.03295  loss_dice: 0.5421  loss_ce_0: 4.421e-05  loss_mask_0: 0.02946  loss_dice_0: 0.5221  loss_ce_1: 4.801e-08  loss_mask_1: 0.03228  loss_dice_1: 0.5252  loss_ce_2: 0  loss_mask_2: 0.03186  loss_dice_2: 0.5379  loss_ce_3: 0  loss_mask_3: 0.02937  loss_dice_3: 0.5249  loss_ce_4: 0  loss_mask_4: 0.03253  loss_dice_4: 0.5535  loss_ce_5: 0  loss_mask_5: 0.02976  loss_dice_5: 0.5402  loss_ce_6: 0  loss_mask_6: 0.03163  loss_dice_6: 0.5387  loss_ce_7: 0  loss_mask_7: 0.02907  loss_dice_7: 0.5606  loss_ce_8: 0  loss_mask_8: 0.03231  loss_dice_8: 0.4982  time: 0.4129  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:01 d2.utils.events]: [0m eta: 0:23:48  iter: 6519  total_loss: 5.664  loss_ce: 1.586e-08  loss_mask: 0.02858  loss_dice: 0.5224  loss_ce_0: 4.399e-05  loss_mask_0: 0.02982  loss_dice_0: 0.502  loss_ce_1: 3.609e-08  loss_mask_1: 0.03135  loss_dice_1: 0.5229  loss_ce_2: 0  loss_mask_2: 0.03098  loss_dice_2: 0.5204  loss_ce_3: 0  loss_mask_3: 0.03195  loss_dice_3: 0.5534  loss_ce_4: 0  loss_mask_4: 0.03279  loss_dice_4: 0.5331  loss_ce_5: 0  loss_mask_5: 0.03412  loss_dice_5: 0.605  loss_ce_6: 0  loss_mask_6: 0.03051  loss_dice_6: 0.5251  loss_ce_7: 0  loss_mask_7: 0.03075  loss_dice_7: 0.5055  loss_ce_8: 0  loss_mask_8: 0.02836  loss_dice_8: 0.5471  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:09 d2.utils.events]: [0m eta: 0:23:40  iter: 6539  total_loss: 5.595  loss_ce: 1.203e-08  loss_mask: 0.03162  loss_dice: 0.5143  loss_ce_0: 4.379e-05  loss_mask_0: 0.02965  loss_dice_0: 0.5532  loss_ce_1: 4.057e-08  loss_mask_1: 0.02797  loss_dice_1: 0.5391  loss_ce_2: 0  loss_mask_2: 0.02944  loss_dice_2: 0.4975  loss_ce_3: 0  loss_mask_3: 0.03056  loss_dice_3: 0.5547  loss_ce_4: 0  loss_mask_4: 0.03177  loss_dice_4: 0.557  loss_ce_5: 0  loss_mask_5: 0.0312  loss_dice_5: 0.5327  loss_ce_6: 0  loss_mask_6: 0.02802  loss_dice_6: 0.5251  loss_ce_7: 0  loss_mask_7: 0.0285  loss_dice_7: 0.5136  loss_ce_8: 0  loss_mask_8: 0.03053  loss_dice_8: 0.5191  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:13 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:32:13 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:32:13 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:32:13 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:32:13 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:32:13 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:32:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277418 (0.277418 s / iter per device, on 1 devices)
[32m[06/02 14:32:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.220064 s / iter per device, on 1 devices)
[32m[06/02 14:32:14 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:32:14 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:32:14 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:32:14 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:32:14 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:32:14 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:32:14 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:32:18 d2.utils.events]: [0m eta: 0:23:32  iter: 6559  total_loss: 5.729  loss_ce: 1.181e-08  loss_mask: 0.03125  loss_dice: 0.5484  loss_ce_0: 4.367e-05  loss_mask_0: 0.03043  loss_dice_0: 0.5389  loss_ce_1: 3.915e-08  loss_mask_1: 0.03086  loss_dice_1: 0.5109  loss_ce_2: 0  loss_mask_2: 0.03044  loss_dice_2: 0.5269  loss_ce_3: 0  loss_mask_3: 0.02937  loss_dice_3: 0.5462  loss_ce_4: 0  loss_mask_4: 0.02821  loss_dice_4: 0.507  loss_ce_5: 0  loss_mask_5: 0.03064  loss_dice_5: 0.5403  loss_ce_6: 0  loss_mask_6: 0.02878  loss_dice_6: 0.5259  loss_ce_7: 0  loss_mask_7: 0.03102  loss_dice_7: 0.5089  loss_ce_8: 0  loss_mask_8: 0.03291  loss_dice_8: 0.5605  time: 0.4129  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:27 d2.utils.events]: [0m eta: 0:23:24  iter: 6579  total_loss: 5.636  loss_ce: 1.17e-08  loss_mask: 0.03031  loss_dice: 0.5052  loss_ce_0: 4.36e-05  loss_mask_0: 0.03058  loss_dice_0: 0.5421  loss_ce_1: 3.347e-08  loss_mask_1: 0.03128  loss_dice_1: 0.4994  loss_ce_2: 0  loss_mask_2: 0.0335  loss_dice_2: 0.5311  loss_ce_3: 0  loss_mask_3: 0.03221  loss_dice_3: 0.5354  loss_ce_4: 0  loss_mask_4: 0.0324  loss_dice_4: 0.4939  loss_ce_5: 0  loss_mask_5: 0.03181  loss_dice_5: 0.5556  loss_ce_6: 0  loss_mask_6: 0.03311  loss_dice_6: 0.5802  loss_ce_7: 0  loss_mask_7: 0.0329  loss_dice_7: 0.558  loss_ce_8: 0  loss_mask_8: 0.02917  loss_dice_8: 0.568  time: 0.4129  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:35 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:32:35 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:32:35 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:32:35 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:32:35 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:32:35 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:32:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268130 (0.268130 s / iter per device, on 1 devices)
[32m[06/02 14:32:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205029 s / iter per device, on 1 devices)
[32m[06/02 14:32:36 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:32:36 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:32:36 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:32:36 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:32:36 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:32:36 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:32:36 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:32:36 d2.utils.events]: [0m eta: 0:23:16  iter: 6599  total_loss: 5.624  loss_ce: 1.706e-08  loss_mask: 0.03193  loss_dice: 0.5279  loss_ce_0: 4.341e-05  loss_mask_0: 0.03132  loss_dice_0: 0.515  loss_ce_1: 3.281e-08  loss_mask_1: 0.03254  loss_dice_1: 0.5329  loss_ce_2: 0  loss_mask_2: 0.03269  loss_dice_2: 0.5268  loss_ce_3: 0  loss_mask_3: 0.03292  loss_dice_3: 0.5504  loss_ce_4: 0  loss_mask_4: 0.03209  loss_dice_4: 0.4977  loss_ce_5: 0  loss_mask_5: 0.02913  loss_dice_5: 0.519  loss_ce_6: 0  loss_mask_6: 0.03076  loss_dice_6: 0.498  loss_ce_7: 0  loss_mask_7: 0.03183  loss_dice_7: 0.5718  loss_ce_8: 0  loss_mask_8: 0.03142  loss_dice_8: 0.5478  time: 0.4129  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:44 d2.utils.events]: [0m eta: 0:23:08  iter: 6619  total_loss: 5.598  loss_ce: 1.542e-08  loss_mask: 0.03022  loss_dice: 0.5194  loss_ce_0: 4.319e-05  loss_mask_0: 0.03052  loss_dice_0: 0.5362  loss_ce_1: 3.347e-08  loss_mask_1: 0.02896  loss_dice_1: 0.4999  loss_ce_2: 0  loss_mask_2: 0.03068  loss_dice_2: 0.4913  loss_ce_3: 0  loss_mask_3: 0.03486  loss_dice_3: 0.5412  loss_ce_4: 0  loss_mask_4: 0.03125  loss_dice_4: 0.5128  loss_ce_5: 0  loss_mask_5: 0.03302  loss_dice_5: 0.5435  loss_ce_6: 0  loss_mask_6: 0.03209  loss_dice_6: 0.5153  loss_ce_7: 0  loss_mask_7: 0.03126  loss_dice_7: 0.532  loss_ce_8: 0  loss_mask_8: 0.03143  loss_dice_8: 0.5278  time: 0.4129  data_time: 0.0068  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:52 d2.utils.events]: [0m eta: 0:23:00  iter: 6639  total_loss: 5.676  loss_ce: 1.269e-08  loss_mask: 0.03067  loss_dice: 0.549  loss_ce_0: 4.299e-05  loss_mask_0: 0.02991  loss_dice_0: 0.5137  loss_ce_1: 3.828e-08  loss_mask_1: 0.02984  loss_dice_1: 0.5606  loss_ce_2: 0  loss_mask_2: 0.0303  loss_dice_2: 0.5128  loss_ce_3: 0  loss_mask_3: 0.02877  loss_dice_3: 0.5037  loss_ce_4: 0  loss_mask_4: 0.03279  loss_dice_4: 0.5652  loss_ce_5: 0  loss_mask_5: 0.03224  loss_dice_5: 0.5444  loss_ce_6: 0  loss_mask_6: 0.03199  loss_dice_6: 0.5533  loss_ce_7: 0  loss_mask_7: 0.02829  loss_dice_7: 0.4814  loss_ce_8: 0  loss_mask_8: 0.03185  loss_dice_8: 0.559  time: 0.4129  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:32:56 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:32:56 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:32:56 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:32:56 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:32:56 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:32:56 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:32:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.248695 (0.248695 s / iter per device, on 1 devices)
[32m[06/02 14:32:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.185750 s / iter per device, on 1 devices)
[32m[06/02 14:32:57 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:32:57 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:32:57 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:32:57 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:32:57 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:32:57 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:32:57 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:33:01 d2.utils.events]: [0m eta: 0:22:52  iter: 6659  total_loss: 5.753  loss_ce: 1.422e-08  loss_mask: 0.03355  loss_dice: 0.5217  loss_ce_0: 4.288e-05  loss_mask_0: 0.03331  loss_dice_0: 0.5229  loss_ce_1: 3.631e-08  loss_mask_1: 0.03055  loss_dice_1: 0.5419  loss_ce_2: 2.187e-09  loss_mask_2: 0.03092  loss_dice_2: 0.5603  loss_ce_3: 0  loss_mask_3: 0.03224  loss_dice_3: 0.5408  loss_ce_4: 0  loss_mask_4: 0.02957  loss_dice_4: 0.5142  loss_ce_5: 0  loss_mask_5: 0.03524  loss_dice_5: 0.5845  loss_ce_6: 0  loss_mask_6: 0.03312  loss_dice_6: 0.5354  loss_ce_7: 0  loss_mask_7: 0.03048  loss_dice_7: 0.5099  loss_ce_8: 0  loss_mask_8: 0.02977  loss_dice_8: 0.5233  time: 0.4129  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:33:10 d2.utils.events]: [0m eta: 0:22:44  iter: 6679  total_loss: 5.577  loss_ce: 1.411e-08  loss_mask: 0.02945  loss_dice: 0.5239  loss_ce_0: 4.265e-05  loss_mask_0: 0.03161  loss_dice_0: 0.5169  loss_ce_1: 4.681e-08  loss_mask_1: 0.03166  loss_dice_1: 0.5025  loss_ce_2: 2.187e-09  loss_mask_2: 0.03164  loss_dice_2: 0.4841  loss_ce_3: 0  loss_mask_3: 0.03094  loss_dice_3: 0.5173  loss_ce_4: 0  loss_mask_4: 0.0302  loss_dice_4: 0.5093  loss_ce_5: 0  loss_mask_5: 0.03329  loss_dice_5: 0.5486  loss_ce_6: 0  loss_mask_6: 0.03276  loss_dice_6: 0.5498  loss_ce_7: 0  loss_mask_7: 0.0308  loss_dice_7: 0.5566  loss_ce_8: 0  loss_mask_8: 0.03144  loss_dice_8: 0.5157  time: 0.4129  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:33:18 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:33:18 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:33:18 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:33:18 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:33:18 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:33:18 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:33:19 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.247183 (0.247183 s / iter per device, on 1 devices)
[32m[06/02 14:33:19 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.187191 s / iter per device, on 1 devices)
[32m[06/02 14:33:19 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:33:19 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:33:19 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:33:19 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:33:19 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:33:19 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:33:19 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:33:19 d2.utils.events]: [0m eta: 0:22:37  iter: 6699  total_loss: 5.665  loss_ce: 1.301e-08  loss_mask: 0.03337  loss_dice: 0.5053  loss_ce_0: 4.243e-05  loss_mask_0: 0.03284  loss_dice_0: 0.5196  loss_ce_1: 3.576e-08  loss_mask_1: 0.03212  loss_dice_1: 0.5141  loss_ce_2: 0  loss_mask_2: 0.03113  loss_dice_2: 0.5355  loss_ce_3: 0  loss_mask_3: 0.02944  loss_dice_3: 0.5406  loss_ce_4: 0  loss_mask_4: 0.03257  loss_dice_4: 0.5565  loss_ce_5: 0  loss_mask_5: 0.03187  loss_dice_5: 0.5376  loss_ce_6: 0  loss_mask_6: 0.0278  loss_dice_6: 0.5164  loss_ce_7: 0  loss_mask_7: 0.02884  loss_dice_7: 0.4915  loss_ce_8: 0  loss_mask_8: 0.03181  loss_dice_8: 0.5262  time: 0.4129  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:33:27 d2.utils.events]: [0m eta: 0:22:29  iter: 6719  total_loss: 5.609  loss_ce: 1.301e-08  loss_mask: 0.03174  loss_dice: 0.5314  loss_ce_0: 4.229e-05  loss_mask_0: 0.03265  loss_dice_0: 0.5204  loss_ce_1: 3.675e-08  loss_mask_1: 0.03257  loss_dice_1: 0.533  loss_ce_2: 0  loss_mask_2: 0.03147  loss_dice_2: 0.5179  loss_ce_3: 0  loss_mask_3: 0.0291  loss_dice_3: 0.5034  loss_ce_4: 0  loss_mask_4: 0.03091  loss_dice_4: 0.4944  loss_ce_5: 0  loss_mask_5: 0.03322  loss_dice_5: 0.5215  loss_ce_6: 0  loss_mask_6: 0.0322  loss_dice_6: 0.5161  loss_ce_7: 0  loss_mask_7: 0.03323  loss_dice_7: 0.5137  loss_ce_8: 0  loss_mask_8: 0.03132  loss_dice_8: 0.5252  time: 0.4129  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:33:35 d2.utils.events]: [0m eta: 0:22:21  iter: 6739  total_loss: 5.795  loss_ce: 1.28e-08  loss_mask: 0.0317  loss_dice: 0.5267  loss_ce_0: 4.208e-05  loss_mask_0: 0.03193  loss_dice_0: 0.5249  loss_ce_1: 4.681e-08  loss_mask_1: 0.03047  loss_dice_1: 0.5037  loss_ce_2: 0  loss_mask_2: 0.03212  loss_dice_2: 0.5327  loss_ce_3: 0  loss_mask_3: 0.03421  loss_dice_3: 0.5276  loss_ce_4: 0  loss_mask_4: 0.03052  loss_dice_4: 0.5196  loss_ce_5: 0  loss_mask_5: 0.02997  loss_dice_5: 0.5337  loss_ce_6: 0  loss_mask_6: 0.0328  loss_dice_6: 0.5618  loss_ce_7: 0  loss_mask_7: 0.03561  loss_dice_7: 0.5284  loss_ce_8: 0  loss_mask_8: 0.03272  loss_dice_8: 0.5764  time: 0.4129  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:33:40 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:33:40 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:33:40 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:33:40 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:33:40 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:33:40 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:33:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280881 (0.280881 s / iter per device, on 1 devices)
[32m[06/02 14:33:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217416 s / iter per device, on 1 devices)
[32m[06/02 14:33:40 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:33:40 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:33:40 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:33:40 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:33:40 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:33:40 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:33:40 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:33:45 d2.utils.events]: [0m eta: 0:22:13  iter: 6759  total_loss: 5.543  loss_ce: 1.017e-08  loss_mask: 0.03064  loss_dice: 0.5057  loss_ce_0: 4.188e-05  loss_mask_0: 0.02963  loss_dice_0: 0.564  loss_ce_1: 4.331e-08  loss_mask_1: 0.03087  loss_dice_1: 0.565  loss_ce_2: 0  loss_mask_2: 0.03069  loss_dice_2: 0.5186  loss_ce_3: 0  loss_mask_3: 0.03038  loss_dice_3: 0.5006  loss_ce_4: 0  loss_mask_4: 0.03092  loss_dice_4: 0.5195  loss_ce_5: 0  loss_mask_5: 0.0302  loss_dice_5: 0.5375  loss_ce_6: 0  loss_mask_6: 0.03038  loss_dice_6: 0.5357  loss_ce_7: 0  loss_mask_7: 0.03061  loss_dice_7: 0.4866  loss_ce_8: 0  loss_mask_8: 0.02864  loss_dice_8: 0.5076  time: 0.4129  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:33:53 d2.utils.events]: [0m eta: 0:22:05  iter: 6779  total_loss: 5.659  loss_ce: 1.148e-08  loss_mask: 0.03234  loss_dice: 0.5494  loss_ce_0: 4.169e-05  loss_mask_0: 0.03201  loss_dice_0: 0.5307  loss_ce_1: 3.959e-08  loss_mask_1: 0.03013  loss_dice_1: 0.5566  loss_ce_2: 0  loss_mask_2: 0.02921  loss_dice_2: 0.5248  loss_ce_3: 0  loss_mask_3: 0.02999  loss_dice_3: 0.5122  loss_ce_4: 0  loss_mask_4: 0.0318  loss_dice_4: 0.5008  loss_ce_5: 0  loss_mask_5: 0.03229  loss_dice_5: 0.5245  loss_ce_6: 0  loss_mask_6: 0.03292  loss_dice_6: 0.5254  loss_ce_7: 0  loss_mask_7: 0.03107  loss_dice_7: 0.5207  loss_ce_8: 0  loss_mask_8: 0.03477  loss_dice_8: 0.5482  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:01 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:34:01 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:34:01 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:34:01 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:34:01 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:34:01 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:34:02 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.265458 (0.265458 s / iter per device, on 1 devices)
[32m[06/02 14:34:02 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.203771 s / iter per device, on 1 devices)
[32m[06/02 14:34:02 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:34:02 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:34:02 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:34:02 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:34:02 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:34:02 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:34:02 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:34:02 d2.utils.events]: [0m eta: 0:21:57  iter: 6799  total_loss: 5.524  loss_ce: 1.247e-08  loss_mask: 0.02866  loss_dice: 0.5247  loss_ce_0: 4.151e-05  loss_mask_0: 0.03094  loss_dice_0: 0.5098  loss_ce_1: 4.659e-08  loss_mask_1: 0.03053  loss_dice_1: 0.4972  loss_ce_2: 0  loss_mask_2: 0.03113  loss_dice_2: 0.5545  loss_ce_3: 0  loss_mask_3: 0.03026  loss_dice_3: 0.4962  loss_ce_4: 0  loss_mask_4: 0.03082  loss_dice_4: 0.5456  loss_ce_5: 0  loss_mask_5: 0.03  loss_dice_5: 0.549  loss_ce_6: 0  loss_mask_6: 0.02874  loss_dice_6: 0.5453  loss_ce_7: 0  loss_mask_7: 0.03135  loss_dice_7: 0.4964  loss_ce_8: 0  loss_mask_8: 0.03176  loss_dice_8: 0.4943  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:10 d2.utils.events]: [0m eta: 0:21:49  iter: 6819  total_loss: 5.549  loss_ce: 1.509e-08  loss_mask: 0.03246  loss_dice: 0.5603  loss_ce_0: 4.135e-05  loss_mask_0: 0.03092  loss_dice_0: 0.5428  loss_ce_1: 4.429e-08  loss_mask_1: 0.02847  loss_dice_1: 0.5172  loss_ce_2: 0  loss_mask_2: 0.03363  loss_dice_2: 0.5076  loss_ce_3: 0  loss_mask_3: 0.03239  loss_dice_3: 0.5407  loss_ce_4: 0  loss_mask_4: 0.03084  loss_dice_4: 0.5284  loss_ce_5: 0  loss_mask_5: 0.02964  loss_dice_5: 0.515  loss_ce_6: 0  loss_mask_6: 0.03025  loss_dice_6: 0.4979  loss_ce_7: 0  loss_mask_7: 0.02935  loss_dice_7: 0.5459  loss_ce_8: 0  loss_mask_8: 0.03068  loss_dice_8: 0.5016  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:18 d2.utils.events]: [0m eta: 0:21:41  iter: 6839  total_loss: 5.685  loss_ce: 1.509e-08  loss_mask: 0.03266  loss_dice: 0.5718  loss_ce_0: 4.116e-05  loss_mask_0: 0.03189  loss_dice_0: 0.5489  loss_ce_1: 3.653e-08  loss_mask_1: 0.03247  loss_dice_1: 0.5082  loss_ce_2: 0  loss_mask_2: 0.03119  loss_dice_2: 0.5125  loss_ce_3: 0  loss_mask_3: 0.03402  loss_dice_3: 0.5038  loss_ce_4: 0  loss_mask_4: 0.03034  loss_dice_4: 0.5433  loss_ce_5: 0  loss_mask_5: 0.03373  loss_dice_5: 0.5409  loss_ce_6: 0  loss_mask_6: 0.03257  loss_dice_6: 0.5339  loss_ce_7: 0  loss_mask_7: 0.03203  loss_dice_7: 0.5167  loss_ce_8: 0  loss_mask_8: 0.03285  loss_dice_8: 0.5702  time: 0.4129  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:22 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:34:22 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:34:22 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:34:22 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:34:22 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:34:22 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:34:23 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.252589 (0.252589 s / iter per device, on 1 devices)
[32m[06/02 14:34:23 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.192625 s / iter per device, on 1 devices)
[32m[06/02 14:34:23 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:34:23 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:34:23 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:34:23 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:34:23 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:34:23 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:34:23 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:34:27 d2.utils.events]: [0m eta: 0:21:32  iter: 6859  total_loss: 5.773  loss_ce: 1.094e-08  loss_mask: 0.03029  loss_dice: 0.5094  loss_ce_0: 4.097e-05  loss_mask_0: 0.03043  loss_dice_0: 0.5502  loss_ce_1: 4.331e-08  loss_mask_1: 0.03209  loss_dice_1: 0.5121  loss_ce_2: 0  loss_mask_2: 0.03009  loss_dice_2: 0.5209  loss_ce_3: 0  loss_mask_3: 0.02809  loss_dice_3: 0.5642  loss_ce_4: 0  loss_mask_4: 0.0315  loss_dice_4: 0.5519  loss_ce_5: 0  loss_mask_5: 0.02771  loss_dice_5: 0.5219  loss_ce_6: 0  loss_mask_6: 0.02822  loss_dice_6: 0.534  loss_ce_7: 0  loss_mask_7: 0.03077  loss_dice_7: 0.5294  loss_ce_8: 0  loss_mask_8: 0.03197  loss_dice_8: 0.5931  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:35 d2.utils.events]: [0m eta: 0:21:24  iter: 6879  total_loss: 5.635  loss_ce: 1.247e-08  loss_mask: 0.03199  loss_dice: 0.5851  loss_ce_0: 4.091e-05  loss_mask_0: 0.02806  loss_dice_0: 0.5131  loss_ce_1: 3.85e-08  loss_mask_1: 0.03174  loss_dice_1: 0.5113  loss_ce_2: 0  loss_mask_2: 0.0287  loss_dice_2: 0.5039  loss_ce_3: 0  loss_mask_3: 0.03042  loss_dice_3: 0.5331  loss_ce_4: 0  loss_mask_4: 0.03367  loss_dice_4: 0.5426  loss_ce_5: 0  loss_mask_5: 0.03166  loss_dice_5: 0.5207  loss_ce_6: 0  loss_mask_6: 0.03134  loss_dice_6: 0.539  loss_ce_7: 0  loss_mask_7: 0.03448  loss_dice_7: 0.5784  loss_ce_8: 0  loss_mask_8: 0.03253  loss_dice_8: 0.548  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:43 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:34:43 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:34:43 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:34:43 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:34:43 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:34:43 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:34:44 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.277569 (0.277569 s / iter per device, on 1 devices)
[32m[06/02 14:34:44 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215868 s / iter per device, on 1 devices)
[32m[06/02 14:34:44 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:34:44 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:34:44 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:34:44 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:34:44 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:34:44 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:34:44 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:34:44 d2.utils.events]: [0m eta: 0:21:16  iter: 6899  total_loss: 5.777  loss_ce: 9.843e-09  loss_mask: 0.03134  loss_dice: 0.53  loss_ce_0: 4.065e-05  loss_mask_0: 0.032  loss_dice_0: 0.5253  loss_ce_1: 3.707e-08  loss_mask_1: 0.03148  loss_dice_1: 0.5167  loss_ce_2: 0  loss_mask_2: 0.03282  loss_dice_2: 0.5216  loss_ce_3: 0  loss_mask_3: 0.03203  loss_dice_3: 0.5463  loss_ce_4: 0  loss_mask_4: 0.03282  loss_dice_4: 0.5362  loss_ce_5: 0  loss_mask_5: 0.03064  loss_dice_5: 0.5004  loss_ce_6: 0  loss_mask_6: 0.03174  loss_dice_6: 0.5685  loss_ce_7: 0  loss_mask_7: 0.03473  loss_dice_7: 0.5692  loss_ce_8: 0  loss_mask_8: 0.02885  loss_dice_8: 0.4874  time: 0.4128  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:34:53 d2.utils.events]: [0m eta: 0:21:08  iter: 6919  total_loss: 5.566  loss_ce: 7.656e-09  loss_mask: 0.03022  loss_dice: 0.5412  loss_ce_0: 4.05e-05  loss_mask_0: 0.0305  loss_dice_0: 0.5535  loss_ce_1: 4.003e-08  loss_mask_1: 0.03226  loss_dice_1: 0.5269  loss_ce_2: 0  loss_mask_2: 0.02911  loss_dice_2: 0.5566  loss_ce_3: 0  loss_mask_3: 0.02859  loss_dice_3: 0.5157  loss_ce_4: 0  loss_mask_4: 0.0316  loss_dice_4: 0.5562  loss_ce_5: 0  loss_mask_5: 0.0274  loss_dice_5: 0.4854  loss_ce_6: 0  loss_mask_6: 0.03065  loss_dice_6: 0.5009  loss_ce_7: 0  loss_mask_7: 0.02958  loss_dice_7: 0.5288  loss_ce_8: 0  loss_mask_8: 0.02959  loss_dice_8: 0.5249  time: 0.4128  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:01 d2.utils.events]: [0m eta: 0:21:01  iter: 6939  total_loss: 5.557  loss_ce: 9.952e-09  loss_mask: 0.02813  loss_dice: 0.5026  loss_ce_0: 4.039e-05  loss_mask_0: 0.02864  loss_dice_0: 0.5188  loss_ce_1: 3.325e-08  loss_mask_1: 0.03137  loss_dice_1: 0.4872  loss_ce_2: 0  loss_mask_2: 0.02833  loss_dice_2: 0.5328  loss_ce_3: 0  loss_mask_3: 0.03334  loss_dice_3: 0.5595  loss_ce_4: 0  loss_mask_4: 0.03106  loss_dice_4: 0.5232  loss_ce_5: 0  loss_mask_5: 0.03064  loss_dice_5: 0.5502  loss_ce_6: 0  loss_mask_6: 0.03106  loss_dice_6: 0.546  loss_ce_7: 0  loss_mask_7: 0.02829  loss_dice_7: 0.4891  loss_ce_8: 0  loss_mask_8: 0.02993  loss_dice_8: 0.5013  time: 0.4128  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:05 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:35:05 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:35:05 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:35:05 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:35:05 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:35:05 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:35:06 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.279882 (0.279882 s / iter per device, on 1 devices)
[32m[06/02 14:35:06 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.220302 s / iter per device, on 1 devices)
[32m[06/02 14:35:06 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:35:06 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:35:06 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:35:06 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:35:06 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:35:06 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:35:06 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:35:10 d2.utils.events]: [0m eta: 0:20:53  iter: 6959  total_loss: 5.496  loss_ce: 7.546e-09  loss_mask: 0.0311  loss_dice: 0.5475  loss_ce_0: 4.02e-05  loss_mask_0: 0.02978  loss_dice_0: 0.5235  loss_ce_1: 3.554e-08  loss_mask_1: 0.0303  loss_dice_1: 0.5156  loss_ce_2: 0  loss_mask_2: 0.02925  loss_dice_2: 0.5426  loss_ce_3: 0  loss_mask_3: 0.0295  loss_dice_3: 0.5288  loss_ce_4: 0  loss_mask_4: 0.03187  loss_dice_4: 0.5202  loss_ce_5: 0  loss_mask_5: 0.03145  loss_dice_5: 0.4935  loss_ce_6: 0  loss_mask_6: 0.03314  loss_dice_6: 0.5167  loss_ce_7: 0  loss_mask_7: 0.03055  loss_dice_7: 0.5156  loss_ce_8: 0  loss_mask_8: 0.03164  loss_dice_8: 0.5326  time: 0.4128  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:18 d2.utils.events]: [0m eta: 0:20:45  iter: 6979  total_loss: 5.669  loss_ce: 8.312e-09  loss_mask: 0.03545  loss_dice: 0.5301  loss_ce_0: 4.001e-05  loss_mask_0: 0.03184  loss_dice_0: 0.5446  loss_ce_1: 3.762e-08  loss_mask_1: 0.03125  loss_dice_1: 0.5179  loss_ce_2: 0  loss_mask_2: 0.0339  loss_dice_2: 0.5701  loss_ce_3: 0  loss_mask_3: 0.03345  loss_dice_3: 0.548  loss_ce_4: 0  loss_mask_4: 0.03037  loss_dice_4: 0.4807  loss_ce_5: 0  loss_mask_5: 0.0307  loss_dice_5: 0.5406  loss_ce_6: 0  loss_mask_6: 0.03018  loss_dice_6: 0.5404  loss_ce_7: 0  loss_mask_7: 0.03153  loss_dice_7: 0.5454  loss_ce_8: 0  loss_mask_8: 0.03061  loss_dice_8: 0.5187  time: 0.4128  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:26 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0006999.pth
[32m[06/02 14:35:29 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:35:29 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:35:29 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:35:29 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:35:29 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:35:29 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:35:30 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273213 (0.273213 s / iter per device, on 1 devices)
[32m[06/02 14:35:30 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210709 s / iter per device, on 1 devices)
[32m[06/02 14:35:30 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:35:30 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:35:30 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:35:30 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:35:30 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:35:30 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:35:30 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:35:30 d2.utils.events]: [0m eta: 0:20:36  iter: 6999  total_loss: 5.719  loss_ce: 8.531e-09  loss_mask: 0.03001  loss_dice: 0.4868  loss_ce_0: 3.983e-05  loss_mask_0: 0.03162  loss_dice_0: 0.5264  loss_ce_1: 3.128e-08  loss_mask_1: 0.03138  loss_dice_1: 0.5178  loss_ce_2: 0  loss_mask_2: 0.03002  loss_dice_2: 0.5103  loss_ce_3: 0  loss_mask_3: 0.02942  loss_dice_3: 0.5112  loss_ce_4: 0  loss_mask_4: 0.0333  loss_dice_4: 0.5916  loss_ce_5: 0  loss_mask_5: 0.03144  loss_dice_5: 0.5238  loss_ce_6: 0  loss_mask_6: 0.03033  loss_dice_6: 0.5196  loss_ce_7: 0  loss_mask_7: 0.03097  loss_dice_7: 0.5517  loss_ce_8: 0  loss_mask_8: 0.03364  loss_dice_8: 0.5351  time: 0.4128  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:38 d2.utils.events]: [0m eta: 0:20:28  iter: 7019  total_loss: 5.714  loss_ce: 9.187e-09  loss_mask: 0.03283  loss_dice: 0.5474  loss_ce_0: 3.968e-05  loss_mask_0: 0.03085  loss_dice_0: 0.5226  loss_ce_1: 3.062e-08  loss_mask_1: 0.03085  loss_dice_1: 0.5278  loss_ce_2: 0  loss_mask_2: 0.03039  loss_dice_2: 0.49  loss_ce_3: 0  loss_mask_3: 0.02953  loss_dice_3: 0.5719  loss_ce_4: 0  loss_mask_4: 0.02924  loss_dice_4: 0.5273  loss_ce_5: 0  loss_mask_5: 0.03257  loss_dice_5: 0.5304  loss_ce_6: 0  loss_mask_6: 0.03161  loss_dice_6: 0.5078  loss_ce_7: 0  loss_mask_7: 0.03435  loss_dice_7: 0.5392  loss_ce_8: 0  loss_mask_8: 0.03177  loss_dice_8: 0.5412  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:47 d2.utils.events]: [0m eta: 0:20:21  iter: 7039  total_loss: 5.659  loss_ce: 7.218e-09  loss_mask: 0.03016  loss_dice: 0.5215  loss_ce_0: 3.948e-05  loss_mask_0: 0.03472  loss_dice_0: 0.5622  loss_ce_1: 3.511e-08  loss_mask_1: 0.02979  loss_dice_1: 0.531  loss_ce_2: 0  loss_mask_2: 0.03241  loss_dice_2: 0.5243  loss_ce_3: 0  loss_mask_3: 0.03284  loss_dice_3: 0.5086  loss_ce_4: 0  loss_mask_4: 0.03073  loss_dice_4: 0.5535  loss_ce_5: 0  loss_mask_5: 0.0313  loss_dice_5: 0.5033  loss_ce_6: 0  loss_mask_6: 0.03349  loss_dice_6: 0.5463  loss_ce_7: 0  loss_mask_7: 0.03078  loss_dice_7: 0.5209  loss_ce_8: 0  loss_mask_8: 0.03337  loss_dice_8: 0.4931  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:35:51 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:35:51 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:35:51 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:35:51 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:35:51 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:35:51 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:35:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.266369 (0.266369 s / iter per device, on 1 devices)
[32m[06/02 14:35:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.205668 s / iter per device, on 1 devices)
[32m[06/02 14:35:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:35:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:35:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:35:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:35:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:35:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:35:52 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:35:56 d2.utils.events]: [0m eta: 0:20:13  iter: 7059  total_loss: 5.631  loss_ce: 6.89e-09  loss_mask: 0.03081  loss_dice: 0.5107  loss_ce_0: 3.925e-05  loss_mask_0: 0.03121  loss_dice_0: 0.5636  loss_ce_1: 3.806e-08  loss_mask_1: 0.02885  loss_dice_1: 0.5461  loss_ce_2: 0  loss_mask_2: 0.02987  loss_dice_2: 0.574  loss_ce_3: 0  loss_mask_3: 0.03111  loss_dice_3: 0.5235  loss_ce_4: 0  loss_mask_4: 0.03137  loss_dice_4: 0.5129  loss_ce_5: 0  loss_mask_5: 0.03115  loss_dice_5: 0.5132  loss_ce_6: 0  loss_mask_6: 0.03149  loss_dice_6: 0.5211  loss_ce_7: 0  loss_mask_7: 0.03391  loss_dice_7: 0.557  loss_ce_8: 0  loss_mask_8: 0.02913  loss_dice_8: 0.4814  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:04 d2.utils.events]: [0m eta: 0:20:05  iter: 7079  total_loss: 5.71  loss_ce: 5.468e-09  loss_mask: 0.02981  loss_dice: 0.5394  loss_ce_0: 3.911e-05  loss_mask_0: 0.02966  loss_dice_0: 0.5385  loss_ce_1: 3.795e-08  loss_mask_1: 0.03127  loss_dice_1: 0.4893  loss_ce_2: 0  loss_mask_2: 0.02931  loss_dice_2: 0.5285  loss_ce_3: 0  loss_mask_3: 0.03315  loss_dice_3: 0.5247  loss_ce_4: 0  loss_mask_4: 0.03519  loss_dice_4: 0.5614  loss_ce_5: 0  loss_mask_5: 0.03319  loss_dice_5: 0.5428  loss_ce_6: 0  loss_mask_6: 0.02952  loss_dice_6: 0.5376  loss_ce_7: 0  loss_mask_7: 0.03251  loss_dice_7: 0.5336  loss_ce_8: 0  loss_mask_8: 0.03282  loss_dice_8: 0.5342  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:12 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:36:12 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:36:12 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:36:12 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:36:12 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:36:12 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:36:13 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.288115 (0.288115 s / iter per device, on 1 devices)
[32m[06/02 14:36:13 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.226094 s / iter per device, on 1 devices)
[32m[06/02 14:36:13 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:36:13 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:36:13 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:36:13 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:36:13 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:36:13 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:36:13 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:36:13 d2.utils.events]: [0m eta: 0:19:57  iter: 7099  total_loss: 5.591  loss_ce: 6.562e-09  loss_mask: 0.03014  loss_dice: 0.5135  loss_ce_0: 3.891e-05  loss_mask_0: 0.02939  loss_dice_0: 0.5556  loss_ce_1: 3.237e-08  loss_mask_1: 0.02945  loss_dice_1: 0.5182  loss_ce_2: 0  loss_mask_2: 0.0322  loss_dice_2: 0.5499  loss_ce_3: 0  loss_mask_3: 0.02983  loss_dice_3: 0.5356  loss_ce_4: 0  loss_mask_4: 0.02878  loss_dice_4: 0.4843  loss_ce_5: 0  loss_mask_5: 0.03309  loss_dice_5: 0.5752  loss_ce_6: 0  loss_mask_6: 0.03104  loss_dice_6: 0.5174  loss_ce_7: 0  loss_mask_7: 0.03143  loss_dice_7: 0.5348  loss_ce_8: 0  loss_mask_8: 0.0301  loss_dice_8: 0.512  time: 0.4128  data_time: 0.0057  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:21 d2.utils.events]: [0m eta: 0:19:49  iter: 7119  total_loss: 5.561  loss_ce: 6.015e-09  loss_mask: 0.03192  loss_dice: 0.5235  loss_ce_0: 3.882e-05  loss_mask_0: 0.02859  loss_dice_0: 0.5095  loss_ce_1: 3.85e-08  loss_mask_1: 0.03062  loss_dice_1: 0.5416  loss_ce_2: 0  loss_mask_2: 0.03334  loss_dice_2: 0.5758  loss_ce_3: 0  loss_mask_3: 0.03108  loss_dice_3: 0.4898  loss_ce_4: 0  loss_mask_4: 0.03514  loss_dice_4: 0.5098  loss_ce_5: 0  loss_mask_5: 0.03227  loss_dice_5: 0.533  loss_ce_6: 0  loss_mask_6: 0.03102  loss_dice_6: 0.5088  loss_ce_7: 0  loss_mask_7: 0.03207  loss_dice_7: 0.5585  loss_ce_8: 0  loss_mask_8: 0.03127  loss_dice_8: 0.5478  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:30 d2.utils.events]: [0m eta: 0:19:41  iter: 7139  total_loss: 5.567  loss_ce: 4.703e-09  loss_mask: 0.02854  loss_dice: 0.5114  loss_ce_0: 3.865e-05  loss_mask_0: 0.03083  loss_dice_0: 0.5342  loss_ce_1: 3.073e-08  loss_mask_1: 0.03177  loss_dice_1: 0.5031  loss_ce_2: 0  loss_mask_2: 0.03314  loss_dice_2: 0.5789  loss_ce_3: 0  loss_mask_3: 0.02927  loss_dice_3: 0.5342  loss_ce_4: 0  loss_mask_4: 0.02991  loss_dice_4: 0.5017  loss_ce_5: 0  loss_mask_5: 0.03183  loss_dice_5: 0.5532  loss_ce_6: 0  loss_mask_6: 0.03045  loss_dice_6: 0.5463  loss_ce_7: 0  loss_mask_7: 0.03372  loss_dice_7: 0.5193  loss_ce_8: 0  loss_mask_8: 0.02957  loss_dice_8: 0.498  time: 0.4128  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:34 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:36:34 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:36:34 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:36:34 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:36:34 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:36:34 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:36:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.257361 (0.257361 s / iter per device, on 1 devices)
[32m[06/02 14:36:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.196524 s / iter per device, on 1 devices)
[32m[06/02 14:36:35 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:36:35 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:36:35 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:36:35 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:36:35 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:36:35 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:36:35 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:36:39 d2.utils.events]: [0m eta: 0:19:32  iter: 7159  total_loss: 5.812  loss_ce: 4.375e-09  loss_mask: 0.03353  loss_dice: 0.5408  loss_ce_0: 3.843e-05  loss_mask_0: 0.03242  loss_dice_0: 0.5591  loss_ce_1: 3.587e-08  loss_mask_1: 0.03264  loss_dice_1: 0.5632  loss_ce_2: 0  loss_mask_2: 0.03068  loss_dice_2: 0.5324  loss_ce_3: 0  loss_mask_3: 0.0328  loss_dice_3: 0.5294  loss_ce_4: 0  loss_mask_4: 0.03216  loss_dice_4: 0.5302  loss_ce_5: 0  loss_mask_5: 0.03226  loss_dice_5: 0.5683  loss_ce_6: 0  loss_mask_6: 0.032  loss_dice_6: 0.5388  loss_ce_7: 0  loss_mask_7: 0.03102  loss_dice_7: 0.5283  loss_ce_8: 0  loss_mask_8: 0.02937  loss_dice_8: 0.5019  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:47 d2.utils.events]: [0m eta: 0:19:23  iter: 7179  total_loss: 5.481  loss_ce: 4.375e-09  loss_mask: 0.02932  loss_dice: 0.4995  loss_ce_0: 3.83e-05  loss_mask_0: 0.03079  loss_dice_0: 0.5099  loss_ce_1: 3.937e-08  loss_mask_1: 0.03124  loss_dice_1: 0.502  loss_ce_2: 0  loss_mask_2: 0.0292  loss_dice_2: 0.5178  loss_ce_3: 0  loss_mask_3: 0.03138  loss_dice_3: 0.5721  loss_ce_4: 0  loss_mask_4: 0.02944  loss_dice_4: 0.5234  loss_ce_5: 0  loss_mask_5: 0.02944  loss_dice_5: 0.5055  loss_ce_6: 0  loss_mask_6: 0.0309  loss_dice_6: 0.504  loss_ce_7: 0  loss_mask_7: 0.02875  loss_dice_7: 0.5123  loss_ce_8: 0  loss_mask_8: 0.02847  loss_dice_8: 0.5269  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:36:55 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:36:55 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:36:55 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:36:55 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:36:55 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:36:55 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:36:56 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.269090 (0.269090 s / iter per device, on 1 devices)
[32m[06/02 14:36:56 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208935 s / iter per device, on 1 devices)
[32m[06/02 14:36:56 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:36:56 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:36:56 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:36:56 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:36:56 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:36:56 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:36:56 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:36:56 d2.utils.events]: [0m eta: 0:19:15  iter: 7199  total_loss: 5.684  loss_ce: 4.593e-09  loss_mask: 0.03367  loss_dice: 0.5387  loss_ce_0: 3.812e-05  loss_mask_0: 0.03248  loss_dice_0: 0.5142  loss_ce_1: 3.893e-08  loss_mask_1: 0.03249  loss_dice_1: 0.5385  loss_ce_2: 0  loss_mask_2: 0.02908  loss_dice_2: 0.5075  loss_ce_3: 0  loss_mask_3: 0.03284  loss_dice_3: 0.5414  loss_ce_4: 0  loss_mask_4: 0.03287  loss_dice_4: 0.5707  loss_ce_5: 0  loss_mask_5: 0.03032  loss_dice_5: 0.5058  loss_ce_6: 0  loss_mask_6: 0.03306  loss_dice_6: 0.5528  loss_ce_7: 0  loss_mask_7: 0.02988  loss_dice_7: 0.518  loss_ce_8: 0  loss_mask_8: 0.0298  loss_dice_8: 0.4918  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:04 d2.utils.events]: [0m eta: 0:19:07  iter: 7219  total_loss: 5.57  loss_ce: 4.484e-09  loss_mask: 0.0313  loss_dice: 0.5124  loss_ce_0: 3.792e-05  loss_mask_0: 0.03246  loss_dice_0: 0.5342  loss_ce_1: 2.964e-08  loss_mask_1: 0.03271  loss_dice_1: 0.532  loss_ce_2: 0  loss_mask_2: 0.03207  loss_dice_2: 0.502  loss_ce_3: 0  loss_mask_3: 0.03038  loss_dice_3: 0.5044  loss_ce_4: 0  loss_mask_4: 0.02939  loss_dice_4: 0.4856  loss_ce_5: 0  loss_mask_5: 0.0306  loss_dice_5: 0.4997  loss_ce_6: 0  loss_mask_6: 0.03124  loss_dice_6: 0.5337  loss_ce_7: 0  loss_mask_7: 0.03184  loss_dice_7: 0.5383  loss_ce_8: 0  loss_mask_8: 0.03181  loss_dice_8: 0.5537  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:12 d2.utils.events]: [0m eta: 0:18:58  iter: 7239  total_loss: 5.766  loss_ce: 4.812e-09  loss_mask: 0.03288  loss_dice: 0.5565  loss_ce_0: 3.771e-05  loss_mask_0: 0.02853  loss_dice_0: 0.5085  loss_ce_1: 2.964e-08  loss_mask_1: 0.03096  loss_dice_1: 0.5316  loss_ce_2: 0  loss_mask_2: 0.03055  loss_dice_2: 0.5067  loss_ce_3: 0  loss_mask_3: 0.03192  loss_dice_3: 0.5307  loss_ce_4: 0  loss_mask_4: 0.03136  loss_dice_4: 0.5332  loss_ce_5: 0  loss_mask_5: 0.02978  loss_dice_5: 0.541  loss_ce_6: 0  loss_mask_6: 0.03369  loss_dice_6: 0.5777  loss_ce_7: 0  loss_mask_7: 0.03097  loss_dice_7: 0.5394  loss_ce_8: 0  loss_mask_8: 0.03238  loss_dice_8: 0.5132  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:17 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:37:17 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:37:17 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:37:17 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:37:17 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:37:17 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:37:17 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278322 (0.278322 s / iter per device, on 1 devices)
[32m[06/02 14:37:17 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.214690 s / iter per device, on 1 devices)
[32m[06/02 14:37:17 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:37:17 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:37:17 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:37:17 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:37:17 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:37:17 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:37:17 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:37:21 d2.utils.events]: [0m eta: 0:18:50  iter: 7259  total_loss: 5.621  loss_ce: 4.375e-09  loss_mask: 0.03517  loss_dice: 0.5332  loss_ce_0: 3.759e-05  loss_mask_0: 0.02947  loss_dice_0: 0.5354  loss_ce_1: 2.548e-08  loss_mask_1: 0.02881  loss_dice_1: 0.52  loss_ce_2: 0  loss_mask_2: 0.03287  loss_dice_2: 0.5483  loss_ce_3: 0  loss_mask_3: 0.03208  loss_dice_3: 0.5143  loss_ce_4: 0  loss_mask_4: 0.03092  loss_dice_4: 0.5314  loss_ce_5: 0  loss_mask_5: 0.02979  loss_dice_5: 0.5128  loss_ce_6: 0  loss_mask_6: 0.03612  loss_dice_6: 0.6032  loss_ce_7: 0  loss_mask_7: 0.03077  loss_dice_7: 0.5353  loss_ce_8: 0  loss_mask_8: 0.03015  loss_dice_8: 0.4941  time: 0.4128  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:30 d2.utils.events]: [0m eta: 0:18:42  iter: 7279  total_loss: 5.579  loss_ce: 4.484e-09  loss_mask: 0.02981  loss_dice: 0.5279  loss_ce_0: 3.734e-05  loss_mask_0: 0.0275  loss_dice_0: 0.5026  loss_ce_1: 2.603e-08  loss_mask_1: 0.02876  loss_dice_1: 0.5172  loss_ce_2: 0  loss_mask_2: 0.02768  loss_dice_2: 0.505  loss_ce_3: 0  loss_mask_3: 0.03271  loss_dice_3: 0.5423  loss_ce_4: 0  loss_mask_4: 0.03129  loss_dice_4: 0.5651  loss_ce_5: 0  loss_mask_5: 0.03233  loss_dice_5: 0.5385  loss_ce_6: 0  loss_mask_6: 0.02955  loss_dice_6: 0.5256  loss_ce_7: 0  loss_mask_7: 0.02972  loss_dice_7: 0.5426  loss_ce_8: 0  loss_mask_8: 0.03089  loss_dice_8: 0.5233  time: 0.4128  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:38 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:37:38 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:37:38 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:37:38 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:37:38 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:37:38 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:37:39 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278115 (0.278115 s / iter per device, on 1 devices)
[32m[06/02 14:37:39 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217575 s / iter per device, on 1 devices)
[32m[06/02 14:37:39 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:37:39 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:37:39 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:37:39 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:37:39 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:37:39 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:37:39 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:37:39 d2.utils.events]: [0m eta: 0:18:33  iter: 7299  total_loss: 5.616  loss_ce: 4.375e-09  loss_mask: 0.0324  loss_dice: 0.5516  loss_ce_0: 3.728e-05  loss_mask_0: 0.03113  loss_dice_0: 0.5464  loss_ce_1: 2.822e-08  loss_mask_1: 0.02994  loss_dice_1: 0.516  loss_ce_2: 0  loss_mask_2: 0.03107  loss_dice_2: 0.5328  loss_ce_3: 0  loss_mask_3: 0.03368  loss_dice_3: 0.5538  loss_ce_4: 0  loss_mask_4: 0.02962  loss_dice_4: 0.4932  loss_ce_5: 0  loss_mask_5: 0.03163  loss_dice_5: 0.5146  loss_ce_6: 0  loss_mask_6: 0.03092  loss_dice_6: 0.5383  loss_ce_7: 0  loss_mask_7: 0.03332  loss_dice_7: 0.538  loss_ce_8: 0  loss_mask_8: 0.03149  loss_dice_8: 0.4887  time: 0.4128  data_time: 0.0080  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:47 d2.utils.events]: [0m eta: 0:18:25  iter: 7319  total_loss: 5.726  loss_ce: 4.812e-09  loss_mask: 0.03227  loss_dice: 0.5548  loss_ce_0: 3.706e-05  loss_mask_0: 0.03157  loss_dice_0: 0.4991  loss_ce_1: 2.701e-08  loss_mask_1: 0.02973  loss_dice_1: 0.5506  loss_ce_2: 0  loss_mask_2: 0.03271  loss_dice_2: 0.595  loss_ce_3: 0  loss_mask_3: 0.03706  loss_dice_3: 0.5269  loss_ce_4: 0  loss_mask_4: 0.03057  loss_dice_4: 0.4852  loss_ce_5: 0  loss_mask_5: 0.02874  loss_dice_5: 0.5268  loss_ce_6: 0  loss_mask_6: 0.03094  loss_dice_6: 0.5178  loss_ce_7: 0  loss_mask_7: 0.02844  loss_dice_7: 0.5588  loss_ce_8: 0  loss_mask_8: 0.03061  loss_dice_8: 0.5239  time: 0.4128  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:37:56 d2.utils.events]: [0m eta: 0:18:17  iter: 7339  total_loss: 5.612  loss_ce: 4.703e-09  loss_mask: 0.02942  loss_dice: 0.525  loss_ce_0: 3.687e-05  loss_mask_0: 0.02967  loss_dice_0: 0.5617  loss_ce_1: 2.537e-08  loss_mask_1: 0.02635  loss_dice_1: 0.523  loss_ce_2: 0  loss_mask_2: 0.02864  loss_dice_2: 0.5179  loss_ce_3: 0  loss_mask_3: 0.03046  loss_dice_3: 0.535  loss_ce_4: 0  loss_mask_4: 0.03154  loss_dice_4: 0.545  loss_ce_5: 0  loss_mask_5: 0.02864  loss_dice_5: 0.5413  loss_ce_6: 0  loss_mask_6: 0.02876  loss_dice_6: 0.538  loss_ce_7: 0  loss_mask_7: 0.03211  loss_dice_7: 0.5632  loss_ce_8: 0  loss_mask_8: 0.03153  loss_dice_8: 0.523  time: 0.4128  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:00 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:38:00 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:38:00 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:38:00 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:38:00 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:38:00 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:38:00 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.245762 (0.245762 s / iter per device, on 1 devices)
[32m[06/02 14:38:00 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.185925 s / iter per device, on 1 devices)
[32m[06/02 14:38:00 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:38:00 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:38:00 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:38:00 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:38:00 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:38:00 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:38:00 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:38:05 d2.utils.events]: [0m eta: 0:18:08  iter: 7359  total_loss: 5.644  loss_ce: 4.375e-09  loss_mask: 0.03234  loss_dice: 0.4867  loss_ce_0: 3.679e-05  loss_mask_0: 0.03307  loss_dice_0: 0.557  loss_ce_1: 2.362e-08  loss_mask_1: 0.0314  loss_dice_1: 0.55  loss_ce_2: 0  loss_mask_2: 0.03056  loss_dice_2: 0.5114  loss_ce_3: 0  loss_mask_3: 0.03144  loss_dice_3: 0.5332  loss_ce_4: 0  loss_mask_4: 0.02981  loss_dice_4: 0.4963  loss_ce_5: 0  loss_mask_5: 0.03451  loss_dice_5: 0.5617  loss_ce_6: 0  loss_mask_6: 0.03176  loss_dice_6: 0.5343  loss_ce_7: 0  loss_mask_7: 0.0331  loss_dice_7: 0.5189  loss_ce_8: 0  loss_mask_8: 0.03155  loss_dice_8: 0.5032  time: 0.4128  data_time: 0.0062  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:13 d2.utils.events]: [0m eta: 0:18:00  iter: 7379  total_loss: 5.488  loss_ce: 4.375e-09  loss_mask: 0.03146  loss_dice: 0.5366  loss_ce_0: 3.656e-05  loss_mask_0: 0.03109  loss_dice_0: 0.5156  loss_ce_1: 2.406e-08  loss_mask_1: 0.03208  loss_dice_1: 0.5123  loss_ce_2: 0  loss_mask_2: 0.02969  loss_dice_2: 0.4938  loss_ce_3: 0  loss_mask_3: 0.03091  loss_dice_3: 0.5326  loss_ce_4: 0  loss_mask_4: 0.03013  loss_dice_4: 0.5045  loss_ce_5: 0  loss_mask_5: 0.03159  loss_dice_5: 0.4927  loss_ce_6: 0  loss_mask_6: 0.02959  loss_dice_6: 0.5429  loss_ce_7: 0  loss_mask_7: 0.02974  loss_dice_7: 0.4907  loss_ce_8: 0  loss_mask_8: 0.02966  loss_dice_8: 0.5406  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:21 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:38:21 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:38:21 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:38:21 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:38:21 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:38:21 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:38:22 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.281918 (0.281918 s / iter per device, on 1 devices)
[32m[06/02 14:38:22 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.224521 s / iter per device, on 1 devices)
[32m[06/02 14:38:22 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:38:22 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:38:22 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:38:22 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:38:22 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:38:22 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:38:22 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:38:22 d2.utils.events]: [0m eta: 0:17:52  iter: 7399  total_loss: 5.628  loss_ce: 4.375e-09  loss_mask: 0.03168  loss_dice: 0.5448  loss_ce_0: 3.641e-05  loss_mask_0: 0.03557  loss_dice_0: 0.5493  loss_ce_1: 2.537e-08  loss_mask_1: 0.03183  loss_dice_1: 0.5315  loss_ce_2: 0  loss_mask_2: 0.02996  loss_dice_2: 0.483  loss_ce_3: 0  loss_mask_3: 0.02889  loss_dice_3: 0.5075  loss_ce_4: 0  loss_mask_4: 0.03293  loss_dice_4: 0.5327  loss_ce_5: 0  loss_mask_5: 0.03191  loss_dice_5: 0.5616  loss_ce_6: 0  loss_mask_6: 0.03222  loss_dice_6: 0.5534  loss_ce_7: 0  loss_mask_7: 0.03087  loss_dice_7: 0.519  loss_ce_8: 0  loss_mask_8: 0.02831  loss_dice_8: 0.5285  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:30 d2.utils.events]: [0m eta: 0:17:43  iter: 7419  total_loss: 5.484  loss_ce: 4.375e-09  loss_mask: 0.02979  loss_dice: 0.5256  loss_ce_0: 3.623e-05  loss_mask_0: 0.03164  loss_dice_0: 0.5329  loss_ce_1: 2.329e-08  loss_mask_1: 0.02915  loss_dice_1: 0.5175  loss_ce_2: 0  loss_mask_2: 0.03186  loss_dice_2: 0.5364  loss_ce_3: 0  loss_mask_3: 0.03075  loss_dice_3: 0.5304  loss_ce_4: 0  loss_mask_4: 0.02989  loss_dice_4: 0.493  loss_ce_5: 0  loss_mask_5: 0.03085  loss_dice_5: 0.5017  loss_ce_6: 0  loss_mask_6: 0.02949  loss_dice_6: 0.4939  loss_ce_7: 0  loss_mask_7: 0.0313  loss_dice_7: 0.5185  loss_ce_8: 0  loss_mask_8: 0.03215  loss_dice_8: 0.5359  time: 0.4128  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:38 d2.utils.events]: [0m eta: 0:17:35  iter: 7439  total_loss: 5.507  loss_ce: 4.375e-09  loss_mask: 0.03154  loss_dice: 0.4951  loss_ce_0: 3.602e-05  loss_mask_0: 0.0306  loss_dice_0: 0.5411  loss_ce_1: 2.789e-08  loss_mask_1: 0.03001  loss_dice_1: 0.5156  loss_ce_2: 0  loss_mask_2: 0.03137  loss_dice_2: 0.5318  loss_ce_3: 0  loss_mask_3: 0.02871  loss_dice_3: 0.5319  loss_ce_4: 0  loss_mask_4: 0.03037  loss_dice_4: 0.5157  loss_ce_5: 0  loss_mask_5: 0.03032  loss_dice_5: 0.4984  loss_ce_6: 0  loss_mask_6: 0.02808  loss_dice_6: 0.5067  loss_ce_7: 0  loss_mask_7: 0.02872  loss_dice_7: 0.4866  loss_ce_8: 0  loss_mask_8: 0.03349  loss_dice_8: 0.5525  time: 0.4128  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:43 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:38:43 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:38:43 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:38:43 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:38:43 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:38:43 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:38:43 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268084 (0.268084 s / iter per device, on 1 devices)
[32m[06/02 14:38:43 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.207779 s / iter per device, on 1 devices)
[32m[06/02 14:38:43 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:38:43 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:38:43 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:38:43 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:38:43 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:38:43 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:38:43 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:38:48 d2.utils.events]: [0m eta: 0:17:27  iter: 7459  total_loss: 5.669  loss_ce: 4.375e-09  loss_mask: 0.03215  loss_dice: 0.5366  loss_ce_0: 3.592e-05  loss_mask_0: 0.03109  loss_dice_0: 0.52  loss_ce_1: 2.515e-08  loss_mask_1: 0.03359  loss_dice_1: 0.5625  loss_ce_2: 0  loss_mask_2: 0.0322  loss_dice_2: 0.5172  loss_ce_3: 0  loss_mask_3: 0.03324  loss_dice_3: 0.544  loss_ce_4: 0  loss_mask_4: 0.02802  loss_dice_4: 0.5225  loss_ce_5: 0  loss_mask_5: 0.03015  loss_dice_5: 0.549  loss_ce_6: 0  loss_mask_6: 0.03206  loss_dice_6: 0.5141  loss_ce_7: 0  loss_mask_7: 0.03143  loss_dice_7: 0.5127  loss_ce_8: 0  loss_mask_8: 0.03316  loss_dice_8: 0.5189  time: 0.4128  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:38:56 d2.utils.events]: [0m eta: 0:17:18  iter: 7479  total_loss: 5.605  loss_ce: 4.375e-09  loss_mask: 0.03123  loss_dice: 0.5103  loss_ce_0: 3.576e-05  loss_mask_0: 0.03515  loss_dice_0: 0.5439  loss_ce_1: 2.603e-08  loss_mask_1: 0.03365  loss_dice_1: 0.5063  loss_ce_2: 0  loss_mask_2: 0.03306  loss_dice_2: 0.5296  loss_ce_3: 0  loss_mask_3: 0.02948  loss_dice_3: 0.5318  loss_ce_4: 0  loss_mask_4: 0.03171  loss_dice_4: 0.5461  loss_ce_5: 0  loss_mask_5: 0.03392  loss_dice_5: 0.5693  loss_ce_6: 0  loss_mask_6: 0.03039  loss_dice_6: 0.5159  loss_ce_7: 0  loss_mask_7: 0.03029  loss_dice_7: 0.5105  loss_ce_8: 0  loss_mask_8: 0.03099  loss_dice_8: 0.529  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:04 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:39:04 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:39:04 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:39:04 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:39:04 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:39:04 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:39:05 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268811 (0.268811 s / iter per device, on 1 devices)
[32m[06/02 14:39:05 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210602 s / iter per device, on 1 devices)
[32m[06/02 14:39:05 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:39:05 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:39:05 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:39:05 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:39:05 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:39:05 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:39:05 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:39:05 d2.utils.events]: [0m eta: 0:17:10  iter: 7499  total_loss: 5.493  loss_ce: 4.375e-09  loss_mask: 0.03208  loss_dice: 0.5521  loss_ce_0: 3.559e-05  loss_mask_0: 0.03274  loss_dice_0: 0.5256  loss_ce_1: 3.161e-08  loss_mask_1: 0.03258  loss_dice_1: 0.5393  loss_ce_2: 0  loss_mask_2: 0.03086  loss_dice_2: 0.4905  loss_ce_3: 0  loss_mask_3: 0.03138  loss_dice_3: 0.5132  loss_ce_4: 0  loss_mask_4: 0.03323  loss_dice_4: 0.5507  loss_ce_5: 0  loss_mask_5: 0.02906  loss_dice_5: 0.4891  loss_ce_6: 0  loss_mask_6: 0.0304  loss_dice_6: 0.5353  loss_ce_7: 0  loss_mask_7: 0.02814  loss_dice_7: 0.5028  loss_ce_8: 0  loss_mask_8: 0.03068  loss_dice_8: 0.534  time: 0.4128  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:13 d2.utils.events]: [0m eta: 0:17:02  iter: 7519  total_loss: 5.543  loss_ce: 4.375e-09  loss_mask: 0.0304  loss_dice: 0.516  loss_ce_0: 3.541e-05  loss_mask_0: 0.03022  loss_dice_0: 0.5067  loss_ce_1: 2.428e-08  loss_mask_1: 0.03267  loss_dice_1: 0.556  loss_ce_2: 0  loss_mask_2: 0.03162  loss_dice_2: 0.5332  loss_ce_3: 0  loss_mask_3: 0.02798  loss_dice_3: 0.4789  loss_ce_4: 0  loss_mask_4: 0.03427  loss_dice_4: 0.574  loss_ce_5: 0  loss_mask_5: 0.03236  loss_dice_5: 0.5075  loss_ce_6: 0  loss_mask_6: 0.02919  loss_dice_6: 0.5118  loss_ce_7: 0  loss_mask_7: 0.03228  loss_dice_7: 0.5408  loss_ce_8: 0  loss_mask_8: 0.03024  loss_dice_8: 0.5587  time: 0.4127  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:21 d2.utils.events]: [0m eta: 0:16:53  iter: 7539  total_loss: 5.608  loss_ce: 4.375e-09  loss_mask: 0.03181  loss_dice: 0.5337  loss_ce_0: 3.528e-05  loss_mask_0: 0.03064  loss_dice_0: 0.5408  loss_ce_1: 2.734e-08  loss_mask_1: 0.03131  loss_dice_1: 0.5066  loss_ce_2: 0  loss_mask_2: 0.03117  loss_dice_2: 0.5187  loss_ce_3: 0  loss_mask_3: 0.02853  loss_dice_3: 0.5086  loss_ce_4: 0  loss_mask_4: 0.02942  loss_dice_4: 0.5164  loss_ce_5: 0  loss_mask_5: 0.02952  loss_dice_5: 0.4976  loss_ce_6: 0  loss_mask_6: 0.03054  loss_dice_6: 0.5858  loss_ce_7: 0  loss_mask_7: 0.02948  loss_dice_7: 0.5051  loss_ce_8: 0  loss_mask_8: 0.03201  loss_dice_8: 0.554  time: 0.4127  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:25 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:39:25 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:39:25 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:39:25 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:39:25 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:39:25 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:39:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274207 (0.274207 s / iter per device, on 1 devices)
[32m[06/02 14:39:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.211914 s / iter per device, on 1 devices)
[32m[06/02 14:39:26 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:39:26 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:39:26 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:39:26 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:39:26 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:39:26 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:39:26 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:39:30 d2.utils.events]: [0m eta: 0:16:45  iter: 7559  total_loss: 5.686  loss_ce: 4.375e-09  loss_mask: 0.03238  loss_dice: 0.5248  loss_ce_0: 3.51e-05  loss_mask_0: 0.02994  loss_dice_0: 0.5374  loss_ce_1: 2.636e-08  loss_mask_1: 0.02709  loss_dice_1: 0.4867  loss_ce_2: 0  loss_mask_2: 0.03241  loss_dice_2: 0.5452  loss_ce_3: 0  loss_mask_3: 0.03221  loss_dice_3: 0.5333  loss_ce_4: 0  loss_mask_4: 0.03127  loss_dice_4: 0.5523  loss_ce_5: 0  loss_mask_5: 0.03311  loss_dice_5: 0.5756  loss_ce_6: 0  loss_mask_6: 0.03561  loss_dice_6: 0.5708  loss_ce_7: 0  loss_mask_7: 0.03526  loss_dice_7: 0.5829  loss_ce_8: 0  loss_mask_8: 0.03001  loss_dice_8: 0.4939  time: 0.4127  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:38 d2.utils.events]: [0m eta: 0:16:36  iter: 7579  total_loss: 5.565  loss_ce: 4.375e-09  loss_mask: 0.03263  loss_dice: 0.5451  loss_ce_0: 3.495e-05  loss_mask_0: 0.0309  loss_dice_0: 0.5176  loss_ce_1: 2.669e-08  loss_mask_1: 0.03641  loss_dice_1: 0.577  loss_ce_2: 0  loss_mask_2: 0.02791  loss_dice_2: 0.4818  loss_ce_3: 0  loss_mask_3: 0.0297  loss_dice_3: 0.4847  loss_ce_4: 0  loss_mask_4: 0.03025  loss_dice_4: 0.5284  loss_ce_5: 0  loss_mask_5: 0.03052  loss_dice_5: 0.5622  loss_ce_6: 0  loss_mask_6: 0.03092  loss_dice_6: 0.5344  loss_ce_7: 0  loss_mask_7: 0.02882  loss_dice_7: 0.4599  loss_ce_8: 0  loss_mask_8: 0.03347  loss_dice_8: 0.495  time: 0.4127  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:47 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:39:47 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:39:47 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:39:47 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:39:47 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:39:47 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:39:47 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268471 (0.268471 s / iter per device, on 1 devices)
[32m[06/02 14:39:47 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.204733 s / iter per device, on 1 devices)
[32m[06/02 14:39:47 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:39:47 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:39:47 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:39:47 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:39:47 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:39:47 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:39:47 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:39:47 d2.utils.events]: [0m eta: 0:16:28  iter: 7599  total_loss: 5.513  loss_ce: 4.375e-09  loss_mask: 0.02936  loss_dice: 0.55  loss_ce_0: 3.474e-05  loss_mask_0: 0.03232  loss_dice_0: 0.5409  loss_ce_1: 2.581e-08  loss_mask_1: 0.03029  loss_dice_1: 0.5334  loss_ce_2: 0  loss_mask_2: 0.02953  loss_dice_2: 0.5061  loss_ce_3: 0  loss_mask_3: 0.03171  loss_dice_3: 0.5105  loss_ce_4: 0  loss_mask_4: 0.02833  loss_dice_4: 0.4839  loss_ce_5: 0  loss_mask_5: 0.03095  loss_dice_5: 0.5119  loss_ce_6: 0  loss_mask_6: 0.03026  loss_dice_6: 0.5255  loss_ce_7: 0  loss_mask_7: 0.02941  loss_dice_7: 0.5307  loss_ce_8: 0  loss_mask_8: 0.03017  loss_dice_8: 0.5528  time: 0.4127  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:39:56 d2.utils.events]: [0m eta: 0:16:20  iter: 7619  total_loss: 5.702  loss_ce: 2.187e-09  loss_mask: 0.03186  loss_dice: 0.5696  loss_ce_0: 3.465e-05  loss_mask_0: 0.02768  loss_dice_0: 0.4976  loss_ce_1: 3.084e-08  loss_mask_1: 0.03328  loss_dice_1: 0.5533  loss_ce_2: 0  loss_mask_2: 0.03236  loss_dice_2: 0.5401  loss_ce_3: 0  loss_mask_3: 0.02905  loss_dice_3: 0.5247  loss_ce_4: 0  loss_mask_4: 0.0293  loss_dice_4: 0.5246  loss_ce_5: 0  loss_mask_5: 0.03027  loss_dice_5: 0.5111  loss_ce_6: 0  loss_mask_6: 0.02939  loss_dice_6: 0.5244  loss_ce_7: 0  loss_mask_7: 0.03296  loss_dice_7: 0.5735  loss_ce_8: 0  loss_mask_8: 0.03051  loss_dice_8: 0.5527  time: 0.4127  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:04 d2.utils.events]: [0m eta: 0:16:12  iter: 7639  total_loss: 5.726  loss_ce: 2.187e-09  loss_mask: 0.03474  loss_dice: 0.5534  loss_ce_0: 3.453e-05  loss_mask_0: 0.03327  loss_dice_0: 0.5692  loss_ce_1: 3.161e-08  loss_mask_1: 0.03251  loss_dice_1: 0.5538  loss_ce_2: 0  loss_mask_2: 0.03056  loss_dice_2: 0.5253  loss_ce_3: 0  loss_mask_3: 0.03379  loss_dice_3: 0.5356  loss_ce_4: 0  loss_mask_4: 0.03228  loss_dice_4: 0.5181  loss_ce_5: 0  loss_mask_5: 0.03127  loss_dice_5: 0.523  loss_ce_6: 0  loss_mask_6: 0.03319  loss_dice_6: 0.5702  loss_ce_7: 0  loss_mask_7: 0.02962  loss_dice_7: 0.4991  loss_ce_8: 0  loss_mask_8: 0.03206  loss_dice_8: 0.5325  time: 0.4127  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:08 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:40:08 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:40:08 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:40:08 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:40:08 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:40:08 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:40:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.279856 (0.279856 s / iter per device, on 1 devices)
[32m[06/02 14:40:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.219330 s / iter per device, on 1 devices)
[32m[06/02 14:40:09 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:40:09 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:40:09 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:40:09 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:40:09 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:40:09 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:40:09 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:40:13 d2.utils.events]: [0m eta: 0:16:04  iter: 7659  total_loss: 5.553  loss_ce: 2.187e-09  loss_mask: 0.03088  loss_dice: 0.5346  loss_ce_0: 3.435e-05  loss_mask_0: 0.03147  loss_dice_0: 0.5175  loss_ce_1: 2.45e-08  loss_mask_1: 0.03333  loss_dice_1: 0.5354  loss_ce_2: 0  loss_mask_2: 0.03178  loss_dice_2: 0.5283  loss_ce_3: 0  loss_mask_3: 0.03237  loss_dice_3: 0.5453  loss_ce_4: 0  loss_mask_4: 0.03083  loss_dice_4: 0.5201  loss_ce_5: 0  loss_mask_5: 0.03157  loss_dice_5: 0.5344  loss_ce_6: 0  loss_mask_6: 0.03086  loss_dice_6: 0.5364  loss_ce_7: 0  loss_mask_7: 0.03287  loss_dice_7: 0.5197  loss_ce_8: 0  loss_mask_8: 0.03361  loss_dice_8: 0.5129  time: 0.4127  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:21 d2.utils.events]: [0m eta: 0:15:55  iter: 7679  total_loss: 5.715  loss_ce: 2.187e-09  loss_mask: 0.03245  loss_dice: 0.526  loss_ce_0: 3.415e-05  loss_mask_0: 0.03189  loss_dice_0: 0.5133  loss_ce_1: 3.008e-08  loss_mask_1: 0.03292  loss_dice_1: 0.5722  loss_ce_2: 0  loss_mask_2: 0.0348  loss_dice_2: 0.547  loss_ce_3: 0  loss_mask_3: 0.03195  loss_dice_3: 0.4542  loss_ce_4: 0  loss_mask_4: 0.03352  loss_dice_4: 0.5604  loss_ce_5: 0  loss_mask_5: 0.0309  loss_dice_5: 0.5098  loss_ce_6: 0  loss_mask_6: 0.03067  loss_dice_6: 0.5212  loss_ce_7: 0  loss_mask_7: 0.02826  loss_dice_7: 0.5193  loss_ce_8: 0  loss_mask_8: 0.03199  loss_dice_8: 0.5357  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:30 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:40:30 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:40:30 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:40:30 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:40:30 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:40:30 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:40:30 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.261629 (0.261629 s / iter per device, on 1 devices)
[32m[06/02 14:40:30 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.203835 s / iter per device, on 1 devices)
[32m[06/02 14:40:30 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:40:30 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:40:30 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:40:30 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:40:30 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:40:30 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:40:30 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:40:30 d2.utils.events]: [0m eta: 0:15:47  iter: 7699  total_loss: 5.622  loss_ce: 2.187e-09  loss_mask: 0.03049  loss_dice: 0.5412  loss_ce_0: 3.405e-05  loss_mask_0: 0.03175  loss_dice_0: 0.4906  loss_ce_1: 2.406e-08  loss_mask_1: 0.03253  loss_dice_1: 0.5375  loss_ce_2: 0  loss_mask_2: 0.02921  loss_dice_2: 0.4873  loss_ce_3: 0  loss_mask_3: 0.03079  loss_dice_3: 0.5463  loss_ce_4: 0  loss_mask_4: 0.02761  loss_dice_4: 0.4897  loss_ce_5: 0  loss_mask_5: 0.03103  loss_dice_5: 0.5123  loss_ce_6: 0  loss_mask_6: 0.03107  loss_dice_6: 0.5196  loss_ce_7: 0  loss_mask_7: 0.03052  loss_dice_7: 0.5247  loss_ce_8: 0  loss_mask_8: 0.02961  loss_dice_8: 0.5175  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:39 d2.utils.events]: [0m eta: 0:15:39  iter: 7719  total_loss: 5.603  loss_ce: 2.187e-09  loss_mask: 0.03261  loss_dice: 0.539  loss_ce_0: 3.393e-05  loss_mask_0: 0.03418  loss_dice_0: 0.5289  loss_ce_1: 2.625e-08  loss_mask_1: 0.03351  loss_dice_1: 0.5296  loss_ce_2: 0  loss_mask_2: 0.03271  loss_dice_2: 0.5148  loss_ce_3: 0  loss_mask_3: 0.03008  loss_dice_3: 0.5404  loss_ce_4: 0  loss_mask_4: 0.03169  loss_dice_4: 0.5212  loss_ce_5: 0  loss_mask_5: 0.03231  loss_dice_5: 0.5004  loss_ce_6: 0  loss_mask_6: 0.03385  loss_dice_6: 0.5594  loss_ce_7: 0  loss_mask_7: 0.03362  loss_dice_7: 0.5449  loss_ce_8: 0  loss_mask_8: 0.03244  loss_dice_8: 0.5702  time: 0.4127  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:47 d2.utils.events]: [0m eta: 0:15:31  iter: 7739  total_loss: 5.669  loss_ce: 2.187e-09  loss_mask: 0.02973  loss_dice: 0.5143  loss_ce_0: 3.369e-05  loss_mask_0: 0.0323  loss_dice_0: 0.5982  loss_ce_1: 2.745e-08  loss_mask_1: 0.03117  loss_dice_1: 0.5434  loss_ce_2: 0  loss_mask_2: 0.03006  loss_dice_2: 0.5104  loss_ce_3: 0  loss_mask_3: 0.03419  loss_dice_3: 0.5221  loss_ce_4: 0  loss_mask_4: 0.03184  loss_dice_4: 0.54  loss_ce_5: 0  loss_mask_5: 0.03221  loss_dice_5: 0.5095  loss_ce_6: 0  loss_mask_6: 0.02997  loss_dice_6: 0.4955  loss_ce_7: 0  loss_mask_7: 0.03118  loss_dice_7: 0.5078  loss_ce_8: 0  loss_mask_8: 0.03274  loss_dice_8: 0.5277  time: 0.4128  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:40:51 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:40:51 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:40:51 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:40:51 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:40:51 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:40:51 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:40:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273401 (0.273401 s / iter per device, on 1 devices)
[32m[06/02 14:40:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.211637 s / iter per device, on 1 devices)
[32m[06/02 14:40:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:40:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:40:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:40:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:40:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:40:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:40:52 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:40:56 d2.utils.events]: [0m eta: 0:15:22  iter: 7759  total_loss: 5.534  loss_ce: 2.187e-09  loss_mask: 0.03253  loss_dice: 0.5302  loss_ce_0: 3.356e-05  loss_mask_0: 0.02811  loss_dice_0: 0.4992  loss_ce_1: 3.303e-08  loss_mask_1: 0.03267  loss_dice_1: 0.5193  loss_ce_2: 0  loss_mask_2: 0.0323  loss_dice_2: 0.5091  loss_ce_3: 0  loss_mask_3: 0.02923  loss_dice_3: 0.4976  loss_ce_4: 0  loss_mask_4: 0.02908  loss_dice_4: 0.4885  loss_ce_5: 0  loss_mask_5: 0.03277  loss_dice_5: 0.5645  loss_ce_6: 0  loss_mask_6: 0.02879  loss_dice_6: 0.5215  loss_ce_7: 0  loss_mask_7: 0.03338  loss_dice_7: 0.5268  loss_ce_8: 0  loss_mask_8: 0.02996  loss_dice_8: 0.5026  time: 0.4128  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:04 d2.utils.events]: [0m eta: 0:15:14  iter: 7779  total_loss: 5.614  loss_ce: 3.281e-09  loss_mask: 0.0324  loss_dice: 0.518  loss_ce_0: 3.336e-05  loss_mask_0: 0.03185  loss_dice_0: 0.5626  loss_ce_1: 3.915e-08  loss_mask_1: 0.03198  loss_dice_1: 0.4988  loss_ce_2: 0  loss_mask_2: 0.03296  loss_dice_2: 0.5472  loss_ce_3: 0  loss_mask_3: 0.03136  loss_dice_3: 0.5162  loss_ce_4: 0  loss_mask_4: 0.03089  loss_dice_4: 0.5173  loss_ce_5: 0  loss_mask_5: 0.03183  loss_dice_5: 0.5176  loss_ce_6: 0  loss_mask_6: 0.03073  loss_dice_6: 0.5616  loss_ce_7: 0  loss_mask_7: 0.0312  loss_dice_7: 0.5428  loss_ce_8: 0  loss_mask_8: 0.03261  loss_dice_8: 0.5149  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:13 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:41:13 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:41:13 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:41:13 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:41:13 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:41:13 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:41:13 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.271717 (0.271717 s / iter per device, on 1 devices)
[32m[06/02 14:41:13 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.211445 s / iter per device, on 1 devices)
[32m[06/02 14:41:13 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:41:13 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:41:13 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:41:13 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:41:13 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:41:13 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:41:13 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:41:13 d2.utils.events]: [0m eta: 0:15:06  iter: 7799  total_loss: 5.459  loss_ce: 2.406e-09  loss_mask: 0.03003  loss_dice: 0.5014  loss_ce_0: 3.318e-05  loss_mask_0: 0.0309  loss_dice_0: 0.5246  loss_ce_1: 4.003e-08  loss_mask_1: 0.03041  loss_dice_1: 0.4939  loss_ce_2: 0  loss_mask_2: 0.03227  loss_dice_2: 0.5565  loss_ce_3: 0  loss_mask_3: 0.02923  loss_dice_3: 0.5144  loss_ce_4: 0  loss_mask_4: 0.03221  loss_dice_4: 0.5171  loss_ce_5: 0  loss_mask_5: 0.0337  loss_dice_5: 0.5492  loss_ce_6: 0  loss_mask_6: 0.03071  loss_dice_6: 0.5088  loss_ce_7: 0  loss_mask_7: 0.02906  loss_dice_7: 0.51  loss_ce_8: 0  loss_mask_8: 0.02997  loss_dice_8: 0.5069  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:22 d2.utils.events]: [0m eta: 0:14:58  iter: 7819  total_loss: 5.509  loss_ce: 2.515e-09  loss_mask: 0.03078  loss_dice: 0.5598  loss_ce_0: 3.299e-05  loss_mask_0: 0.02895  loss_dice_0: 0.497  loss_ce_1: 3.106e-08  loss_mask_1: 0.02987  loss_dice_1: 0.5057  loss_ce_2: 0  loss_mask_2: 0.0305  loss_dice_2: 0.4933  loss_ce_3: 0  loss_mask_3: 0.03285  loss_dice_3: 0.5494  loss_ce_4: 0  loss_mask_4: 0.02936  loss_dice_4: 0.4784  loss_ce_5: 0  loss_mask_5: 0.03096  loss_dice_5: 0.5323  loss_ce_6: 0  loss_mask_6: 0.03089  loss_dice_6: 0.5021  loss_ce_7: 0  loss_mask_7: 0.03114  loss_dice_7: 0.5066  loss_ce_8: 0  loss_mask_8: 0.0285  loss_dice_8: 0.4963  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:30 d2.utils.events]: [0m eta: 0:14:50  iter: 7839  total_loss: 5.48  loss_ce: 2.297e-09  loss_mask: 0.02709  loss_dice: 0.505  loss_ce_0: 3.288e-05  loss_mask_0: 0.02915  loss_dice_0: 0.4946  loss_ce_1: 3.193e-08  loss_mask_1: 0.03042  loss_dice_1: 0.5025  loss_ce_2: 0  loss_mask_2: 0.03204  loss_dice_2: 0.5185  loss_ce_3: 0  loss_mask_3: 0.02798  loss_dice_3: 0.5174  loss_ce_4: 0  loss_mask_4: 0.03  loss_dice_4: 0.5117  loss_ce_5: 0  loss_mask_5: 0.02905  loss_dice_5: 0.498  loss_ce_6: 0  loss_mask_6: 0.03184  loss_dice_6: 0.5047  loss_ce_7: 0  loss_mask_7: 0.03045  loss_dice_7: 0.5669  loss_ce_8: 0  loss_mask_8: 0.0314  loss_dice_8: 0.5307  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:34 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:41:34 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:41:34 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:41:34 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:41:34 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:41:34 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:41:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282599 (0.282599 s / iter per device, on 1 devices)
[32m[06/02 14:41:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.221179 s / iter per device, on 1 devices)
[32m[06/02 14:41:35 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:41:35 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:41:35 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:41:35 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:41:35 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:41:35 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:41:35 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:41:39 d2.utils.events]: [0m eta: 0:14:42  iter: 7859  total_loss: 5.462  loss_ce: 2.187e-09  loss_mask: 0.0284  loss_dice: 0.5257  loss_ce_0: 3.278e-05  loss_mask_0: 0.02975  loss_dice_0: 0.5002  loss_ce_1: 3.893e-08  loss_mask_1: 0.03063  loss_dice_1: 0.5281  loss_ce_2: 0  loss_mask_2: 0.03084  loss_dice_2: 0.5236  loss_ce_3: 0  loss_mask_3: 0.03268  loss_dice_3: 0.5477  loss_ce_4: 0  loss_mask_4: 0.03039  loss_dice_4: 0.5131  loss_ce_5: 0  loss_mask_5: 0.03073  loss_dice_5: 0.536  loss_ce_6: 0  loss_mask_6: 0.03081  loss_dice_6: 0.5099  loss_ce_7: 0  loss_mask_7: 0.02899  loss_dice_7: 0.4825  loss_ce_8: 0  loss_mask_8: 0.0318  loss_dice_8: 0.5416  time: 0.4127  data_time: 0.0059  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:47 d2.utils.events]: [0m eta: 0:14:34  iter: 7879  total_loss: 5.603  loss_ce: 2.515e-09  loss_mask: 0.03088  loss_dice: 0.5215  loss_ce_0: 3.264e-05  loss_mask_0: 0.02847  loss_dice_0: 0.5342  loss_ce_1: 3.609e-08  loss_mask_1: 0.029  loss_dice_1: 0.4999  loss_ce_2: 0  loss_mask_2: 0.03011  loss_dice_2: 0.5022  loss_ce_3: 0  loss_mask_3: 0.0287  loss_dice_3: 0.5282  loss_ce_4: 0  loss_mask_4: 0.02946  loss_dice_4: 0.4894  loss_ce_5: 0  loss_mask_5: 0.03252  loss_dice_5: 0.5522  loss_ce_6: 0  loss_mask_6: 0.03399  loss_dice_6: 0.58  loss_ce_7: 0  loss_mask_7: 0.03325  loss_dice_7: 0.5352  loss_ce_8: 0  loss_mask_8: 0.03208  loss_dice_8: 0.5037  time: 0.4127  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:41:55 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:41:55 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:41:55 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:41:55 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:41:55 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:41:55 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:41:56 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.260030 (0.260030 s / iter per device, on 1 devices)
[32m[06/02 14:41:56 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.199456 s / iter per device, on 1 devices)
[32m[06/02 14:41:56 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:41:56 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:41:56 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:41:56 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:41:56 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:41:56 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:41:56 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:41:56 d2.utils.events]: [0m eta: 0:14:26  iter: 7899  total_loss: 5.533  loss_ce: 2.297e-09  loss_mask: 0.0349  loss_dice: 0.5033  loss_ce_0: 3.245e-05  loss_mask_0: 0.02903  loss_dice_0: 0.5195  loss_ce_1: 3.401e-08  loss_mask_1: 0.03252  loss_dice_1: 0.5307  loss_ce_2: 0  loss_mask_2: 0.03217  loss_dice_2: 0.4972  loss_ce_3: 0  loss_mask_3: 0.03184  loss_dice_3: 0.5431  loss_ce_4: 0  loss_mask_4: 0.03174  loss_dice_4: 0.5138  loss_ce_5: 0  loss_mask_5: 0.03411  loss_dice_5: 0.5522  loss_ce_6: 0  loss_mask_6: 0.03233  loss_dice_6: 0.5425  loss_ce_7: 0  loss_mask_7: 0.03126  loss_dice_7: 0.5342  loss_ce_8: 0  loss_mask_8: 0.02829  loss_dice_8: 0.5052  time: 0.4127  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:04 d2.utils.events]: [0m eta: 0:14:18  iter: 7919  total_loss: 5.574  loss_ce: 2.625e-09  loss_mask: 0.0333  loss_dice: 0.5356  loss_ce_0: 3.222e-05  loss_mask_0: 0.02889  loss_dice_0: 0.554  loss_ce_1: 2.559e-08  loss_mask_1: 0.03358  loss_dice_1: 0.5473  loss_ce_2: 0  loss_mask_2: 0.03074  loss_dice_2: 0.5114  loss_ce_3: 0  loss_mask_3: 0.02676  loss_dice_3: 0.5166  loss_ce_4: 0  loss_mask_4: 0.03166  loss_dice_4: 0.5098  loss_ce_5: 0  loss_mask_5: 0.02895  loss_dice_5: 0.5162  loss_ce_6: 0  loss_mask_6: 0.03056  loss_dice_6: 0.4969  loss_ce_7: 0  loss_mask_7: 0.03122  loss_dice_7: 0.5068  loss_ce_8: 0  loss_mask_8: 0.02867  loss_dice_8: 0.4855  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:13 d2.utils.events]: [0m eta: 0:14:09  iter: 7939  total_loss: 5.506  loss_ce: 2.187e-09  loss_mask: 0.03068  loss_dice: 0.5122  loss_ce_0: 3.209e-05  loss_mask_0: 0.0298  loss_dice_0: 0.5399  loss_ce_1: 3.15e-08  loss_mask_1: 0.03184  loss_dice_1: 0.5174  loss_ce_2: 0  loss_mask_2: 0.02963  loss_dice_2: 0.5099  loss_ce_3: 0  loss_mask_3: 0.03034  loss_dice_3: 0.5214  loss_ce_4: 0  loss_mask_4: 0.02919  loss_dice_4: 0.4991  loss_ce_5: 0  loss_mask_5: 0.03057  loss_dice_5: 0.535  loss_ce_6: 0  loss_mask_6: 0.03116  loss_dice_6: 0.5222  loss_ce_7: 0  loss_mask_7: 0.03071  loss_dice_7: 0.5193  loss_ce_8: 0  loss_mask_8: 0.02811  loss_dice_8: 0.5056  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:17 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:42:17 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:42:17 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:42:17 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:42:17 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:42:17 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:42:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264719 (0.264719 s / iter per device, on 1 devices)
[32m[06/02 14:42:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.207053 s / iter per device, on 1 devices)
[32m[06/02 14:42:18 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:42:18 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:42:18 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:42:18 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:42:18 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:42:18 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:42:18 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:42:22 d2.utils.events]: [0m eta: 0:14:01  iter: 7959  total_loss: 5.576  loss_ce: 2.187e-09  loss_mask: 0.03111  loss_dice: 0.5126  loss_ce_0: 3.194e-05  loss_mask_0: 0.02863  loss_dice_0: 0.5175  loss_ce_1: 2.964e-08  loss_mask_1: 0.03133  loss_dice_1: 0.5111  loss_ce_2: 0  loss_mask_2: 0.02985  loss_dice_2: 0.5499  loss_ce_3: 0  loss_mask_3: 0.03222  loss_dice_3: 0.5255  loss_ce_4: 0  loss_mask_4: 0.03066  loss_dice_4: 0.5319  loss_ce_5: 0  loss_mask_5: 0.02733  loss_dice_5: 0.4838  loss_ce_6: 0  loss_mask_6: 0.02884  loss_dice_6: 0.4991  loss_ce_7: 0  loss_mask_7: 0.03038  loss_dice_7: 0.5342  loss_ce_8: 0  loss_mask_8: 0.03271  loss_dice_8: 0.5066  time: 0.4127  data_time: 0.0045  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:30 d2.utils.events]: [0m eta: 0:13:53  iter: 7979  total_loss: 5.614  loss_ce: 2.187e-09  loss_mask: 0.03168  loss_dice: 0.5324  loss_ce_0: 3.181e-05  loss_mask_0: 0.03278  loss_dice_0: 0.5539  loss_ce_1: 2.275e-08  loss_mask_1: 0.03057  loss_dice_1: 0.5038  loss_ce_2: 0  loss_mask_2: 0.03275  loss_dice_2: 0.5629  loss_ce_3: 0  loss_mask_3: 0.03213  loss_dice_3: 0.5206  loss_ce_4: 0  loss_mask_4: 0.03009  loss_dice_4: 0.5229  loss_ce_5: 0  loss_mask_5: 0.02978  loss_dice_5: 0.543  loss_ce_6: 0  loss_mask_6: 0.02811  loss_dice_6: 0.522  loss_ce_7: 0  loss_mask_7: 0.02982  loss_dice_7: 0.5102  loss_ce_8: 0  loss_mask_8: 0.02752  loss_dice_8: 0.5236  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:38 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0007999.pth
[32m[06/02 14:42:42 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:42:42 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:42:42 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:42:42 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:42:42 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:42:42 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:42:42 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.272407 (0.272407 s / iter per device, on 1 devices)
[32m[06/02 14:42:42 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.212118 s / iter per device, on 1 devices)
[32m[06/02 14:42:42 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:42:42 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:42:42 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:42:42 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:42:42 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:42:42 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:42:42 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:42:42 d2.utils.events]: [0m eta: 0:13:44  iter: 7999  total_loss: 5.51  loss_ce: 2.297e-09  loss_mask: 0.03428  loss_dice: 0.5351  loss_ce_0: 3.173e-05  loss_mask_0: 0.02962  loss_dice_0: 0.4963  loss_ce_1: 2.373e-08  loss_mask_1: 0.03118  loss_dice_1: 0.5032  loss_ce_2: 0  loss_mask_2: 0.02988  loss_dice_2: 0.53  loss_ce_3: 0  loss_mask_3: 0.02938  loss_dice_3: 0.5213  loss_ce_4: 0  loss_mask_4: 0.03007  loss_dice_4: 0.4912  loss_ce_5: 0  loss_mask_5: 0.02955  loss_dice_5: 0.5354  loss_ce_6: 0  loss_mask_6: 0.03011  loss_dice_6: 0.4964  loss_ce_7: 0  loss_mask_7: 0.03064  loss_dice_7: 0.5355  loss_ce_8: 0  loss_mask_8: 0.02896  loss_dice_8: 0.5405  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:51 d2.utils.events]: [0m eta: 0:13:36  iter: 8019  total_loss: 5.627  loss_ce: 2.187e-09  loss_mask: 0.02992  loss_dice: 0.5128  loss_ce_0: 3.16e-05  loss_mask_0: 0.02976  loss_dice_0: 0.5428  loss_ce_1: 2.472e-08  loss_mask_1: 0.0319  loss_dice_1: 0.5596  loss_ce_2: 0  loss_mask_2: 0.03232  loss_dice_2: 0.5324  loss_ce_3: 0  loss_mask_3: 0.03131  loss_dice_3: 0.5265  loss_ce_4: 0  loss_mask_4: 0.03185  loss_dice_4: 0.4965  loss_ce_5: 0  loss_mask_5: 0.03288  loss_dice_5: 0.5042  loss_ce_6: 0  loss_mask_6: 0.03058  loss_dice_6: 0.5206  loss_ce_7: 0  loss_mask_7: 0.03359  loss_dice_7: 0.5228  loss_ce_8: 0  loss_mask_8: 0.0299  loss_dice_8: 0.5108  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:42:59 d2.utils.events]: [0m eta: 0:13:28  iter: 8039  total_loss: 5.704  loss_ce: 2.187e-09  loss_mask: 0.03082  loss_dice: 0.5075  loss_ce_0: 3.141e-05  loss_mask_0: 0.03055  loss_dice_0: 0.5278  loss_ce_1: 2.603e-08  loss_mask_1: 0.03001  loss_dice_1: 0.5448  loss_ce_2: 0  loss_mask_2: 0.03242  loss_dice_2: 0.5548  loss_ce_3: 0  loss_mask_3: 0.03253  loss_dice_3: 0.5201  loss_ce_4: 0  loss_mask_4: 0.03183  loss_dice_4: 0.5118  loss_ce_5: 0  loss_mask_5: 0.03463  loss_dice_5: 0.5634  loss_ce_6: 0  loss_mask_6: 0.03299  loss_dice_6: 0.5742  loss_ce_7: 0  loss_mask_7: 0.03356  loss_dice_7: 0.5794  loss_ce_8: 0  loss_mask_8: 0.02914  loss_dice_8: 0.4835  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:03 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:43:03 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:43:03 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:43:03 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:43:03 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:43:03 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:43:04 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282985 (0.282985 s / iter per device, on 1 devices)
[32m[06/02 14:43:04 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.222273 s / iter per device, on 1 devices)
[32m[06/02 14:43:04 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:43:04 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:43:04 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:43:04 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:43:04 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:43:04 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:43:04 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:43:08 d2.utils.events]: [0m eta: 0:13:19  iter: 8059  total_loss: 5.584  loss_ce: 2.187e-09  loss_mask: 0.02818  loss_dice: 0.5301  loss_ce_0: 3.122e-05  loss_mask_0: 0.03205  loss_dice_0: 0.5293  loss_ce_1: 2.647e-08  loss_mask_1: 0.02709  loss_dice_1: 0.4674  loss_ce_2: 0  loss_mask_2: 0.02993  loss_dice_2: 0.5432  loss_ce_3: 0  loss_mask_3: 0.02936  loss_dice_3: 0.5273  loss_ce_4: 0  loss_mask_4: 0.02951  loss_dice_4: 0.5104  loss_ce_5: 0  loss_mask_5: 0.03377  loss_dice_5: 0.5328  loss_ce_6: 0  loss_mask_6: 0.02958  loss_dice_6: 0.5175  loss_ce_7: 0  loss_mask_7: 0.03291  loss_dice_7: 0.5531  loss_ce_8: 0  loss_mask_8: 0.03357  loss_dice_8: 0.521  time: 0.4127  data_time: 0.0057  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:16 d2.utils.events]: [0m eta: 0:13:11  iter: 8079  total_loss: 5.483  loss_ce: 2.187e-09  loss_mask: 0.0291  loss_dice: 0.5209  loss_ce_0: 3.109e-05  loss_mask_0: 0.0323  loss_dice_0: 0.5488  loss_ce_1: 3.139e-08  loss_mask_1: 0.03182  loss_dice_1: 0.5375  loss_ce_2: 0  loss_mask_2: 0.02676  loss_dice_2: 0.502  loss_ce_3: 0  loss_mask_3: 0.03028  loss_dice_3: 0.512  loss_ce_4: 0  loss_mask_4: 0.028  loss_dice_4: 0.5085  loss_ce_5: 0  loss_mask_5: 0.03079  loss_dice_5: 0.5068  loss_ce_6: 0  loss_mask_6: 0.03004  loss_dice_6: 0.5417  loss_ce_7: 0  loss_mask_7: 0.02884  loss_dice_7: 0.5491  loss_ce_8: 0  loss_mask_8: 0.03062  loss_dice_8: 0.5292  time: 0.4127  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:24 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:43:24 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:43:24 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:43:24 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:43:24 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:43:24 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:43:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.279841 (0.279841 s / iter per device, on 1 devices)
[32m[06/02 14:43:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.218471 s / iter per device, on 1 devices)
[32m[06/02 14:43:25 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:43:25 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:43:25 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:43:25 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:43:25 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:43:25 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:43:25 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:43:25 d2.utils.events]: [0m eta: 0:13:02  iter: 8099  total_loss: 5.558  loss_ce: 2.187e-09  loss_mask: 0.0296  loss_dice: 0.524  loss_ce_0: 3.098e-05  loss_mask_0: 0.02942  loss_dice_0: 0.509  loss_ce_1: 2.909e-08  loss_mask_1: 0.03088  loss_dice_1: 0.503  loss_ce_2: 0  loss_mask_2: 0.03317  loss_dice_2: 0.5461  loss_ce_3: 0  loss_mask_3: 0.03322  loss_dice_3: 0.5271  loss_ce_4: 0  loss_mask_4: 0.0327  loss_dice_4: 0.5633  loss_ce_5: 0  loss_mask_5: 0.03091  loss_dice_5: 0.502  loss_ce_6: 0  loss_mask_6: 0.02854  loss_dice_6: 0.5019  loss_ce_7: 0  loss_mask_7: 0.03083  loss_dice_7: 0.5229  loss_ce_8: 0  loss_mask_8: 0.03191  loss_dice_8: 0.5232  time: 0.4127  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:33 d2.utils.events]: [0m eta: 0:12:54  iter: 8119  total_loss: 5.613  loss_ce: 2.844e-09  loss_mask: 0.03331  loss_dice: 0.5378  loss_ce_0: 3.081e-05  loss_mask_0: 0.02857  loss_dice_0: 0.5239  loss_ce_1: 3.018e-08  loss_mask_1: 0.03148  loss_dice_1: 0.5152  loss_ce_2: 0  loss_mask_2: 0.0301  loss_dice_2: 0.5128  loss_ce_3: 0  loss_mask_3: 0.02964  loss_dice_3: 0.496  loss_ce_4: 0  loss_mask_4: 0.03224  loss_dice_4: 0.5489  loss_ce_5: 0  loss_mask_5: 0.03446  loss_dice_5: 0.54  loss_ce_6: 0  loss_mask_6: 0.03071  loss_dice_6: 0.4994  loss_ce_7: 0  loss_mask_7: 0.03104  loss_dice_7: 0.5608  loss_ce_8: 0  loss_mask_8: 0.03442  loss_dice_8: 0.5354  time: 0.4127  data_time: 0.0077  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:41 d2.utils.events]: [0m eta: 0:12:46  iter: 8139  total_loss: 5.455  loss_ce: 2.297e-09  loss_mask: 0.03041  loss_dice: 0.5065  loss_ce_0: 3.062e-05  loss_mask_0: 0.03324  loss_dice_0: 0.5299  loss_ce_1: 3.937e-08  loss_mask_1: 0.02941  loss_dice_1: 0.5213  loss_ce_2: 0  loss_mask_2: 0.02975  loss_dice_2: 0.5068  loss_ce_3: 0  loss_mask_3: 0.0301  loss_dice_3: 0.5128  loss_ce_4: 0  loss_mask_4: 0.03248  loss_dice_4: 0.524  loss_ce_5: 0  loss_mask_5: 0.03055  loss_dice_5: 0.4816  loss_ce_6: 0  loss_mask_6: 0.03019  loss_dice_6: 0.5432  loss_ce_7: 0  loss_mask_7: 0.02944  loss_dice_7: 0.5246  loss_ce_8: 0  loss_mask_8: 0.02784  loss_dice_8: 0.4816  time: 0.4127  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:46 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:43:46 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:43:46 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:43:46 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:43:46 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:43:46 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:43:46 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.246482 (0.246482 s / iter per device, on 1 devices)
[32m[06/02 14:43:46 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.187441 s / iter per device, on 1 devices)
[32m[06/02 14:43:46 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:43:46 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:43:46 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:43:46 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:43:46 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:43:46 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:43:46 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:43:50 d2.utils.events]: [0m eta: 0:12:37  iter: 8159  total_loss: 5.684  loss_ce: 2.625e-09  loss_mask: 0.02966  loss_dice: 0.5309  loss_ce_0: 3.046e-05  loss_mask_0: 0.03061  loss_dice_0: 0.5152  loss_ce_1: 2.92e-08  loss_mask_1: 0.03043  loss_dice_1: 0.541  loss_ce_2: 0  loss_mask_2: 0.03098  loss_dice_2: 0.5155  loss_ce_3: 0  loss_mask_3: 0.02971  loss_dice_3: 0.5162  loss_ce_4: 0  loss_mask_4: 0.03033  loss_dice_4: 0.5091  loss_ce_5: 0  loss_mask_5: 0.03059  loss_dice_5: 0.5482  loss_ce_6: 0  loss_mask_6: 0.03132  loss_dice_6: 0.5309  loss_ce_7: 0  loss_mask_7: 0.02919  loss_dice_7: 0.5514  loss_ce_8: 0  loss_mask_8: 0.03181  loss_dice_8: 0.5678  time: 0.4127  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:43:59 d2.utils.events]: [0m eta: 0:12:29  iter: 8179  total_loss: 5.555  loss_ce: 2.515e-09  loss_mask: 0.03144  loss_dice: 0.5265  loss_ce_0: 3.032e-05  loss_mask_0: 0.03169  loss_dice_0: 0.5002  loss_ce_1: 2.8e-08  loss_mask_1: 0.02961  loss_dice_1: 0.5048  loss_ce_2: 0  loss_mask_2: 0.02956  loss_dice_2: 0.4985  loss_ce_3: 0  loss_mask_3: 0.03089  loss_dice_3: 0.5714  loss_ce_4: 0  loss_mask_4: 0.03567  loss_dice_4: 0.5659  loss_ce_5: 0  loss_mask_5: 0.03538  loss_dice_5: 0.5539  loss_ce_6: 0  loss_mask_6: 0.03031  loss_dice_6: 0.4935  loss_ce_7: 0  loss_mask_7: 0.03162  loss_dice_7: 0.5302  loss_ce_8: 0  loss_mask_8: 0.03222  loss_dice_8: 0.5431  time: 0.4127  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:07 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:44:07 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:44:07 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:44:07 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:44:07 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:44:07 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:44:08 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.296116 (0.296116 s / iter per device, on 1 devices)
[32m[06/02 14:44:08 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.232580 s / iter per device, on 1 devices)
[32m[06/02 14:44:08 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:44:08 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:44:08 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:44:08 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:44:08 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:44:08 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:44:08 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:44:08 d2.utils.events]: [0m eta: 0:12:21  iter: 8199  total_loss: 5.43  loss_ce: 2.187e-09  loss_mask: 0.03001  loss_dice: 0.5287  loss_ce_0: 3.018e-05  loss_mask_0: 0.02982  loss_dice_0: 0.5216  loss_ce_1: 3.707e-08  loss_mask_1: 0.03173  loss_dice_1: 0.5144  loss_ce_2: 0  loss_mask_2: 0.03183  loss_dice_2: 0.5415  loss_ce_3: 0  loss_mask_3: 0.03017  loss_dice_3: 0.4992  loss_ce_4: 0  loss_mask_4: 0.03179  loss_dice_4: 0.5065  loss_ce_5: 0  loss_mask_5: 0.03308  loss_dice_5: 0.4987  loss_ce_6: 0  loss_mask_6: 0.02869  loss_dice_6: 0.4915  loss_ce_7: 0  loss_mask_7: 0.02813  loss_dice_7: 0.5045  loss_ce_8: 0  loss_mask_8: 0.032  loss_dice_8: 0.5017  time: 0.4127  data_time: 0.0078  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:16 d2.utils.events]: [0m eta: 0:12:13  iter: 8219  total_loss: 5.463  loss_ce: 2.734e-09  loss_mask: 0.02894  loss_dice: 0.5059  loss_ce_0: 3.002e-05  loss_mask_0: 0.02941  loss_dice_0: 0.4882  loss_ce_1: 3.106e-08  loss_mask_1: 0.03068  loss_dice_1: 0.5037  loss_ce_2: 0  loss_mask_2: 0.03027  loss_dice_2: 0.5166  loss_ce_3: 0  loss_mask_3: 0.03177  loss_dice_3: 0.5021  loss_ce_4: 0  loss_mask_4: 0.03366  loss_dice_4: 0.5493  loss_ce_5: 0  loss_mask_5: 0.03363  loss_dice_5: 0.5493  loss_ce_6: 0  loss_mask_6: 0.0331  loss_dice_6: 0.5531  loss_ce_7: 0  loss_mask_7: 0.02869  loss_dice_7: 0.5277  loss_ce_8: 0  loss_mask_8: 0.03325  loss_dice_8: 0.5382  time: 0.4127  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:25 d2.utils.events]: [0m eta: 0:12:05  iter: 8239  total_loss: 5.63  loss_ce: 2.844e-09  loss_mask: 0.03182  loss_dice: 0.5472  loss_ce_0: 2.984e-05  loss_mask_0: 0.03009  loss_dice_0: 0.5185  loss_ce_1: 3.412e-08  loss_mask_1: 0.03029  loss_dice_1: 0.5564  loss_ce_2: 0  loss_mask_2: 0.02845  loss_dice_2: 0.4941  loss_ce_3: 0  loss_mask_3: 0.03333  loss_dice_3: 0.5336  loss_ce_4: 0  loss_mask_4: 0.03054  loss_dice_4: 0.5525  loss_ce_5: 0  loss_mask_5: 0.03102  loss_dice_5: 0.5071  loss_ce_6: 0  loss_mask_6: 0.03014  loss_dice_6: 0.529  loss_ce_7: 0  loss_mask_7: 0.02721  loss_dice_7: 0.486  loss_ce_8: 0  loss_mask_8: 0.03109  loss_dice_8: 0.5537  time: 0.4127  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:29 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:44:29 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:44:29 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:44:29 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:44:29 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:44:29 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:44:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.279528 (0.279528 s / iter per device, on 1 devices)
[32m[06/02 14:44:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217423 s / iter per device, on 1 devices)
[32m[06/02 14:44:30 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:44:30 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:44:30 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:44:30 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:44:30 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:44:30 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:44:30 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:44:34 d2.utils.events]: [0m eta: 0:11:57  iter: 8259  total_loss: 5.669  loss_ce: 4.921e-09  loss_mask: 0.03238  loss_dice: 0.5354  loss_ce_0: 2.971e-05  loss_mask_0: 0.03087  loss_dice_0: 0.5687  loss_ce_1: 2.636e-08  loss_mask_1: 0.02976  loss_dice_1: 0.5292  loss_ce_2: 0  loss_mask_2: 0.02985  loss_dice_2: 0.4897  loss_ce_3: 0  loss_mask_3: 0.03043  loss_dice_3: 0.543  loss_ce_4: 0  loss_mask_4: 0.03413  loss_dice_4: 0.5644  loss_ce_5: 0  loss_mask_5: 0.03203  loss_dice_5: 0.5369  loss_ce_6: 0  loss_mask_6: 0.03169  loss_dice_6: 0.4943  loss_ce_7: 0  loss_mask_7: 0.03294  loss_dice_7: 0.5249  loss_ce_8: 0  loss_mask_8: 0.03186  loss_dice_8: 0.5176  time: 0.4127  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:42 d2.utils.events]: [0m eta: 0:11:49  iter: 8279  total_loss: 5.584  loss_ce: 4.375e-09  loss_mask: 0.02951  loss_dice: 0.5309  loss_ce_0: 2.959e-05  loss_mask_0: 0.02927  loss_dice_0: 0.5375  loss_ce_1: 2.789e-08  loss_mask_1: 0.03025  loss_dice_1: 0.5256  loss_ce_2: 0  loss_mask_2: 0.03173  loss_dice_2: 0.5388  loss_ce_3: 0  loss_mask_3: 0.02865  loss_dice_3: 0.5166  loss_ce_4: 0  loss_mask_4: 0.03158  loss_dice_4: 0.5115  loss_ce_5: 0  loss_mask_5: 0.02819  loss_dice_5: 0.5126  loss_ce_6: 0  loss_mask_6: 0.03307  loss_dice_6: 0.5671  loss_ce_7: 0  loss_mask_7: 0.02808  loss_dice_7: 0.474  loss_ce_8: 0  loss_mask_8: 0.02862  loss_dice_8: 0.512  time: 0.4127  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:50 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:44:50 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:44:50 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:44:50 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:44:50 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:44:50 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:44:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280225 (0.280225 s / iter per device, on 1 devices)
[32m[06/02 14:44:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217905 s / iter per device, on 1 devices)
[32m[06/02 14:44:51 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:44:51 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:44:51 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:44:51 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:44:51 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:44:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:44:51 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:44:51 d2.utils.events]: [0m eta: 0:11:41  iter: 8299  total_loss: 5.622  loss_ce: 4.156e-09  loss_mask: 0.03046  loss_dice: 0.5671  loss_ce_0: 2.947e-05  loss_mask_0: 0.02997  loss_dice_0: 0.5442  loss_ce_1: 3.292e-08  loss_mask_1: 0.02827  loss_dice_1: 0.5125  loss_ce_2: 0  loss_mask_2: 0.03094  loss_dice_2: 0.4942  loss_ce_3: 0  loss_mask_3: 0.03106  loss_dice_3: 0.5095  loss_ce_4: 0  loss_mask_4: 0.03021  loss_dice_4: 0.5491  loss_ce_5: 0  loss_mask_5: 0.03085  loss_dice_5: 0.5319  loss_ce_6: 0  loss_mask_6: 0.03201  loss_dice_6: 0.5056  loss_ce_7: 0  loss_mask_7: 0.02875  loss_dice_7: 0.5299  loss_ce_8: 0  loss_mask_8: 0.03374  loss_dice_8: 0.5412  time: 0.4127  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:44:59 d2.utils.events]: [0m eta: 0:11:33  iter: 8319  total_loss: 5.502  loss_ce: 6.343e-09  loss_mask: 0.02969  loss_dice: 0.4978  loss_ce_0: 2.929e-05  loss_mask_0: 0.03185  loss_dice_0: 0.5002  loss_ce_1: 2.844e-08  loss_mask_1: 0.03161  loss_dice_1: 0.5324  loss_ce_2: 0  loss_mask_2: 0.03043  loss_dice_2: 0.4872  loss_ce_3: 0  loss_mask_3: 0.03128  loss_dice_3: 0.5646  loss_ce_4: 0  loss_mask_4: 0.03128  loss_dice_4: 0.5301  loss_ce_5: 0  loss_mask_5: 0.02782  loss_dice_5: 0.5162  loss_ce_6: 0  loss_mask_6: 0.03044  loss_dice_6: 0.5103  loss_ce_7: 0  loss_mask_7: 0.03143  loss_dice_7: 0.5444  loss_ce_8: 0  loss_mask_8: 0.03022  loss_dice_8: 0.5049  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:08 d2.utils.events]: [0m eta: 0:11:24  iter: 8339  total_loss: 5.727  loss_ce: 2.844e-09  loss_mask: 0.03037  loss_dice: 0.5149  loss_ce_0: 2.911e-05  loss_mask_0: 0.02822  loss_dice_0: 0.4849  loss_ce_1: 3.697e-08  loss_mask_1: 0.0327  loss_dice_1: 0.5254  loss_ce_2: 0  loss_mask_2: 0.03252  loss_dice_2: 0.5673  loss_ce_3: 0  loss_mask_3: 0.02924  loss_dice_3: 0.5136  loss_ce_4: 0  loss_mask_4: 0.03195  loss_dice_4: 0.5432  loss_ce_5: 0  loss_mask_5: 0.03  loss_dice_5: 0.4853  loss_ce_6: 0  loss_mask_6: 0.03177  loss_dice_6: 0.5188  loss_ce_7: 0  loss_mask_7: 0.03047  loss_dice_7: 0.5236  loss_ce_8: 0  loss_mask_8: 0.02998  loss_dice_8: 0.5475  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:12 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:45:12 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:45:12 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:45:12 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:45:12 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:45:12 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:45:12 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.260945 (0.260945 s / iter per device, on 1 devices)
[32m[06/02 14:45:12 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.200876 s / iter per device, on 1 devices)
[32m[06/02 14:45:12 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:45:12 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:45:12 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:45:12 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:45:12 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:45:12 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:45:12 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:45:16 d2.utils.events]: [0m eta: 0:11:16  iter: 8359  total_loss: 5.416  loss_ce: 2.734e-09  loss_mask: 0.03027  loss_dice: 0.514  loss_ce_0: 2.904e-05  loss_mask_0: 0.0296  loss_dice_0: 0.5096  loss_ce_1: 2.778e-08  loss_mask_1: 0.03094  loss_dice_1: 0.589  loss_ce_2: 0  loss_mask_2: 0.03004  loss_dice_2: 0.5292  loss_ce_3: 0  loss_mask_3: 0.02815  loss_dice_3: 0.4968  loss_ce_4: 0  loss_mask_4: 0.02842  loss_dice_4: 0.5339  loss_ce_5: 0  loss_mask_5: 0.02915  loss_dice_5: 0.5022  loss_ce_6: 0  loss_mask_6: 0.02981  loss_dice_6: 0.5394  loss_ce_7: 0  loss_mask_7: 0.03263  loss_dice_7: 0.5205  loss_ce_8: 0  loss_mask_8: 0.03171  loss_dice_8: 0.5073  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:25 d2.utils.events]: [0m eta: 0:11:07  iter: 8379  total_loss: 5.404  loss_ce: 2.734e-09  loss_mask: 0.03169  loss_dice: 0.5235  loss_ce_0: 2.892e-05  loss_mask_0: 0.0292  loss_dice_0: 0.4891  loss_ce_1: 2.8e-08  loss_mask_1: 0.03133  loss_dice_1: 0.4935  loss_ce_2: 0  loss_mask_2: 0.03082  loss_dice_2: 0.5274  loss_ce_3: 0  loss_mask_3: 0.03176  loss_dice_3: 0.5334  loss_ce_4: 0  loss_mask_4: 0.03125  loss_dice_4: 0.5113  loss_ce_5: 0  loss_mask_5: 0.03101  loss_dice_5: 0.4895  loss_ce_6: 0  loss_mask_6: 0.03073  loss_dice_6: 0.5354  loss_ce_7: 0  loss_mask_7: 0.02925  loss_dice_7: 0.492  loss_ce_8: 0  loss_mask_8: 0.03172  loss_dice_8: 0.5195  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:33 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:45:33 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:45:33 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:45:33 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:45:33 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:45:33 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:45:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.268892 (0.268892 s / iter per device, on 1 devices)
[32m[06/02 14:45:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210597 s / iter per device, on 1 devices)
[32m[06/02 14:45:34 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:45:34 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:45:34 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:45:34 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:45:34 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:45:34 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:45:34 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:45:34 d2.utils.events]: [0m eta: 0:10:59  iter: 8399  total_loss: 5.667  loss_ce: 2.187e-09  loss_mask: 0.03317  loss_dice: 0.5331  loss_ce_0: 2.872e-05  loss_mask_0: 0.03126  loss_dice_0: 0.5349  loss_ce_1: 3.073e-08  loss_mask_1: 0.031  loss_dice_1: 0.5205  loss_ce_2: 0  loss_mask_2: 0.0305  loss_dice_2: 0.5021  loss_ce_3: 0  loss_mask_3: 0.03084  loss_dice_3: 0.5244  loss_ce_4: 0  loss_mask_4: 0.03172  loss_dice_4: 0.529  loss_ce_5: 0  loss_mask_5: 0.03292  loss_dice_5: 0.5282  loss_ce_6: 0  loss_mask_6: 0.03291  loss_dice_6: 0.5292  loss_ce_7: 0  loss_mask_7: 0.02979  loss_dice_7: 0.5204  loss_ce_8: 0  loss_mask_8: 0.03205  loss_dice_8: 0.4659  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:42 d2.utils.events]: [0m eta: 0:10:51  iter: 8419  total_loss: 5.776  loss_ce: 2.406e-09  loss_mask: 0.03146  loss_dice: 0.5715  loss_ce_0: 2.856e-05  loss_mask_0: 0.02945  loss_dice_0: 0.5036  loss_ce_1: 2.581e-08  loss_mask_1: 0.03256  loss_dice_1: 0.5264  loss_ce_2: 0  loss_mask_2: 0.03428  loss_dice_2: 0.5668  loss_ce_3: 0  loss_mask_3: 0.03178  loss_dice_3: 0.5091  loss_ce_4: 0  loss_mask_4: 0.0311  loss_dice_4: 0.4908  loss_ce_5: 0  loss_mask_5: 0.028  loss_dice_5: 0.515  loss_ce_6: 0  loss_mask_6: 0.03289  loss_dice_6: 0.5437  loss_ce_7: 0  loss_mask_7: 0.03287  loss_dice_7: 0.5583  loss_ce_8: 0  loss_mask_8: 0.03055  loss_dice_8: 0.5076  time: 0.4127  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:50 d2.utils.events]: [0m eta: 0:10:42  iter: 8439  total_loss: 5.384  loss_ce: 2.187e-09  loss_mask: 0.03057  loss_dice: 0.4849  loss_ce_0: 2.844e-05  loss_mask_0: 0.02994  loss_dice_0: 0.5061  loss_ce_1: 3.193e-08  loss_mask_1: 0.03061  loss_dice_1: 0.5246  loss_ce_2: 0  loss_mask_2: 0.0286  loss_dice_2: 0.5037  loss_ce_3: 0  loss_mask_3: 0.02708  loss_dice_3: 0.4292  loss_ce_4: 0  loss_mask_4: 0.02991  loss_dice_4: 0.5122  loss_ce_5: 0  loss_mask_5: 0.03235  loss_dice_5: 0.5467  loss_ce_6: 0  loss_mask_6: 0.03179  loss_dice_6: 0.4989  loss_ce_7: 0  loss_mask_7: 0.0278  loss_dice_7: 0.5183  loss_ce_8: 0  loss_mask_8: 0.03048  loss_dice_8: 0.5049  time: 0.4127  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:45:54 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:45:54 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:45:54 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:45:54 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:45:54 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:45:54 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:45:55 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.281022 (0.281022 s / iter per device, on 1 devices)
[32m[06/02 14:45:55 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.220218 s / iter per device, on 1 devices)
[32m[06/02 14:45:55 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:45:55 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:45:55 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:45:55 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:45:55 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:45:55 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:45:55 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:45:59 d2.utils.events]: [0m eta: 0:10:34  iter: 8459  total_loss: 5.635  loss_ce: 2.297e-09  loss_mask: 0.03214  loss_dice: 0.5966  loss_ce_0: 2.836e-05  loss_mask_0: 0.03165  loss_dice_0: 0.461  loss_ce_1: 3.008e-08  loss_mask_1: 0.03065  loss_dice_1: 0.4981  loss_ce_2: 0  loss_mask_2: 0.03346  loss_dice_2: 0.5158  loss_ce_3: 0  loss_mask_3: 0.03332  loss_dice_3: 0.518  loss_ce_4: 0  loss_mask_4: 0.034  loss_dice_4: 0.5238  loss_ce_5: 0  loss_mask_5: 0.03013  loss_dice_5: 0.508  loss_ce_6: 0  loss_mask_6: 0.03152  loss_dice_6: 0.4991  loss_ce_7: 0  loss_mask_7: 0.03047  loss_dice_7: 0.5089  loss_ce_8: 0  loss_mask_8: 0.03128  loss_dice_8: 0.5241  time: 0.4127  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:08 d2.utils.events]: [0m eta: 0:10:26  iter: 8479  total_loss: 5.495  loss_ce: 2.187e-09  loss_mask: 0.035  loss_dice: 0.552  loss_ce_0: 2.822e-05  loss_mask_0: 0.02824  loss_dice_0: 0.5211  loss_ce_1: 3.15e-08  loss_mask_1: 0.03116  loss_dice_1: 0.5405  loss_ce_2: 0  loss_mask_2: 0.02896  loss_dice_2: 0.5241  loss_ce_3: 0  loss_mask_3: 0.03109  loss_dice_3: 0.4768  loss_ce_4: 0  loss_mask_4: 0.03263  loss_dice_4: 0.5596  loss_ce_5: 0  loss_mask_5: 0.03081  loss_dice_5: 0.4887  loss_ce_6: 0  loss_mask_6: 0.0291  loss_dice_6: 0.5169  loss_ce_7: 0  loss_mask_7: 0.03018  loss_dice_7: 0.519  loss_ce_8: 0  loss_mask_8: 0.0294  loss_dice_8: 0.5177  time: 0.4127  data_time: 0.0062  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:16 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:46:16 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:46:16 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:46:16 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:46:16 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:46:16 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:46:16 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.259022 (0.259022 s / iter per device, on 1 devices)
[32m[06/02 14:46:16 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.199492 s / iter per device, on 1 devices)
[32m[06/02 14:46:16 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:46:16 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:46:16 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:46:16 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:46:16 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:46:16 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:46:16 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:46:16 d2.utils.events]: [0m eta: 0:10:17  iter: 8499  total_loss: 5.445  loss_ce: 2.187e-09  loss_mask: 0.02645  loss_dice: 0.5188  loss_ce_0: 2.809e-05  loss_mask_0: 0.03023  loss_dice_0: 0.5267  loss_ce_1: 2.428e-08  loss_mask_1: 0.02826  loss_dice_1: 0.4731  loss_ce_2: 0  loss_mask_2: 0.0295  loss_dice_2: 0.495  loss_ce_3: 0  loss_mask_3: 0.02899  loss_dice_3: 0.4985  loss_ce_4: 0  loss_mask_4: 0.03044  loss_dice_4: 0.5234  loss_ce_5: 0  loss_mask_5: 0.03014  loss_dice_5: 0.5072  loss_ce_6: 0  loss_mask_6: 0.03292  loss_dice_6: 0.535  loss_ce_7: 0  loss_mask_7: 0.02973  loss_dice_7: 0.4858  loss_ce_8: 0  loss_mask_8: 0.03084  loss_dice_8: 0.5452  time: 0.4127  data_time: 0.0054  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:25 d2.utils.events]: [0m eta: 0:10:09  iter: 8519  total_loss: 5.58  loss_ce: 2.406e-09  loss_mask: 0.03061  loss_dice: 0.474  loss_ce_0: 2.803e-05  loss_mask_0: 0.03331  loss_dice_0: 0.5517  loss_ce_1: 2.778e-08  loss_mask_1: 0.03046  loss_dice_1: 0.4812  loss_ce_2: 0  loss_mask_2: 0.02997  loss_dice_2: 0.5171  loss_ce_3: 0  loss_mask_3: 0.03295  loss_dice_3: 0.5379  loss_ce_4: 0  loss_mask_4: 0.03226  loss_dice_4: 0.5707  loss_ce_5: 0  loss_mask_5: 0.03054  loss_dice_5: 0.5022  loss_ce_6: 0  loss_mask_6: 0.02927  loss_dice_6: 0.5064  loss_ce_7: 0  loss_mask_7: 0.02886  loss_dice_7: 0.5195  loss_ce_8: 0  loss_mask_8: 0.02969  loss_dice_8: 0.4769  time: 0.4127  data_time: 0.0043  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:33 d2.utils.events]: [0m eta: 0:10:01  iter: 8539  total_loss: 5.582  loss_ce: 2.625e-09  loss_mask: 0.02948  loss_dice: 0.4991  loss_ce_0: 2.783e-05  loss_mask_0: 0.0317  loss_dice_0: 0.5214  loss_ce_1: 2.45e-08  loss_mask_1: 0.03191  loss_dice_1: 0.5244  loss_ce_2: 0  loss_mask_2: 0.03043  loss_dice_2: 0.5495  loss_ce_3: 0  loss_mask_3: 0.03113  loss_dice_3: 0.525  loss_ce_4: 0  loss_mask_4: 0.03164  loss_dice_4: 0.5489  loss_ce_5: 0  loss_mask_5: 0.03107  loss_dice_5: 0.5126  loss_ce_6: 0  loss_mask_6: 0.03066  loss_dice_6: 0.5236  loss_ce_7: 0  loss_mask_7: 0.03031  loss_dice_7: 0.5263  loss_ce_8: 0  loss_mask_8: 0.03031  loss_dice_8: 0.5385  time: 0.4127  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:37 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:46:37 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:46:37 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:46:37 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:46:37 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:46:37 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:46:38 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.258698 (0.258698 s / iter per device, on 1 devices)
[32m[06/02 14:46:38 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.198672 s / iter per device, on 1 devices)
[32m[06/02 14:46:38 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:46:38 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:46:38 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:46:38 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:46:38 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:46:38 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:46:38 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:46:42 d2.utils.events]: [0m eta: 0:09:53  iter: 8559  total_loss: 5.537  loss_ce: 2.625e-09  loss_mask: 0.03319  loss_dice: 0.5518  loss_ce_0: 2.762e-05  loss_mask_0: 0.03181  loss_dice_0: 0.5283  loss_ce_1: 2.559e-08  loss_mask_1: 0.03028  loss_dice_1: 0.5061  loss_ce_2: 0  loss_mask_2: 0.03082  loss_dice_2: 0.5227  loss_ce_3: 0  loss_mask_3: 0.02793  loss_dice_3: 0.5182  loss_ce_4: 0  loss_mask_4: 0.0311  loss_dice_4: 0.5394  loss_ce_5: 0  loss_mask_5: 0.03145  loss_dice_5: 0.5133  loss_ce_6: 0  loss_mask_6: 0.03124  loss_dice_6: 0.4717  loss_ce_7: 0  loss_mask_7: 0.02848  loss_dice_7: 0.5117  loss_ce_8: 0  loss_mask_8: 0.03028  loss_dice_8: 0.5307  time: 0.4126  data_time: 0.0058  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:50 d2.utils.events]: [0m eta: 0:09:45  iter: 8579  total_loss: 5.644  loss_ce: 2.187e-09  loss_mask: 0.03031  loss_dice: 0.5074  loss_ce_0: 2.751e-05  loss_mask_0: 0.0313  loss_dice_0: 0.5122  loss_ce_1: 3.39e-08  loss_mask_1: 0.0303  loss_dice_1: 0.5449  loss_ce_2: 0  loss_mask_2: 0.02967  loss_dice_2: 0.5121  loss_ce_3: 0  loss_mask_3: 0.03119  loss_dice_3: 0.5309  loss_ce_4: 0  loss_mask_4: 0.03374  loss_dice_4: 0.5205  loss_ce_5: 0  loss_mask_5: 0.03249  loss_dice_5: 0.5256  loss_ce_6: 0  loss_mask_6: 0.0302  loss_dice_6: 0.4994  loss_ce_7: 0  loss_mask_7: 0.03117  loss_dice_7: 0.5451  loss_ce_8: 0  loss_mask_8: 0.03052  loss_dice_8: 0.5312  time: 0.4126  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:46:59 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:46:59 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:46:59 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:46:59 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:46:59 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:46:59 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:46:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264878 (0.264878 s / iter per device, on 1 devices)
[32m[06/02 14:46:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.201516 s / iter per device, on 1 devices)
[32m[06/02 14:46:59 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:46:59 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:46:59 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:46:59 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:46:59 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:46:59 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:46:59 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:46:59 d2.utils.events]: [0m eta: 0:09:36  iter: 8599  total_loss: 5.55  loss_ce: 2.515e-09  loss_mask: 0.02961  loss_dice: 0.4748  loss_ce_0: 2.735e-05  loss_mask_0: 0.02982  loss_dice_0: 0.5389  loss_ce_1: 2.636e-08  loss_mask_1: 0.03427  loss_dice_1: 0.5345  loss_ce_2: 0  loss_mask_2: 0.02946  loss_dice_2: 0.5106  loss_ce_3: 0  loss_mask_3: 0.03102  loss_dice_3: 0.4982  loss_ce_4: 0  loss_mask_4: 0.03411  loss_dice_4: 0.5466  loss_ce_5: 0  loss_mask_5: 0.02628  loss_dice_5: 0.4735  loss_ce_6: 0  loss_mask_6: 0.03201  loss_dice_6: 0.5302  loss_ce_7: 0  loss_mask_7: 0.03004  loss_dice_7: 0.5395  loss_ce_8: 0  loss_mask_8: 0.03225  loss_dice_8: 0.5587  time: 0.4127  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:08 d2.utils.events]: [0m eta: 0:09:28  iter: 8619  total_loss: 5.678  loss_ce: 2.515e-09  loss_mask: 0.031  loss_dice: 0.5135  loss_ce_0: 2.719e-05  loss_mask_0: 0.02783  loss_dice_0: 0.5344  loss_ce_1: 3.281e-08  loss_mask_1: 0.03111  loss_dice_1: 0.5478  loss_ce_2: 0  loss_mask_2: 0.02946  loss_dice_2: 0.5227  loss_ce_3: 0  loss_mask_3: 0.03163  loss_dice_3: 0.5432  loss_ce_4: 0  loss_mask_4: 0.03196  loss_dice_4: 0.5259  loss_ce_5: 0  loss_mask_5: 0.03155  loss_dice_5: 0.5309  loss_ce_6: 0  loss_mask_6: 0.03114  loss_dice_6: 0.5407  loss_ce_7: 0  loss_mask_7: 0.03197  loss_dice_7: 0.5112  loss_ce_8: 0  loss_mask_8: 0.03264  loss_dice_8: 0.5473  time: 0.4127  data_time: 0.0045  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:16 d2.utils.events]: [0m eta: 0:09:20  iter: 8639  total_loss: 5.56  loss_ce: 2.187e-09  loss_mask: 0.03467  loss_dice: 0.578  loss_ce_0: 2.708e-05  loss_mask_0: 0.03043  loss_dice_0: 0.5219  loss_ce_1: 3.39e-08  loss_mask_1: 0.03061  loss_dice_1: 0.4981  loss_ce_2: 0  loss_mask_2: 0.03057  loss_dice_2: 0.4924  loss_ce_3: 0  loss_mask_3: 0.03249  loss_dice_3: 0.538  loss_ce_4: 0  loss_mask_4: 0.03438  loss_dice_4: 0.5271  loss_ce_5: 0  loss_mask_5: 0.03049  loss_dice_5: 0.5367  loss_ce_6: 0  loss_mask_6: 0.03051  loss_dice_6: 0.5159  loss_ce_7: 0  loss_mask_7: 0.02991  loss_dice_7: 0.4951  loss_ce_8: 0  loss_mask_8: 0.03011  loss_dice_8: 0.4967  time: 0.4127  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:20 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:47:20 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:47:20 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:47:20 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:47:20 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:47:20 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:47:21 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278942 (0.278942 s / iter per device, on 1 devices)
[32m[06/02 14:47:21 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217781 s / iter per device, on 1 devices)
[32m[06/02 14:47:21 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:47:21 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:47:21 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:47:21 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:47:21 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:47:21 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:47:21 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:47:25 d2.utils.events]: [0m eta: 0:09:11  iter: 8659  total_loss: 5.51  loss_ce: 2.187e-09  loss_mask: 0.02966  loss_dice: 0.4889  loss_ce_0: 2.693e-05  loss_mask_0: 0.02874  loss_dice_0: 0.5055  loss_ce_1: 2.975e-08  loss_mask_1: 0.02988  loss_dice_1: 0.5223  loss_ce_2: 0  loss_mask_2: 0.02841  loss_dice_2: 0.5281  loss_ce_3: 0  loss_mask_3: 0.02913  loss_dice_3: 0.5387  loss_ce_4: 0  loss_mask_4: 0.02844  loss_dice_4: 0.5164  loss_ce_5: 0  loss_mask_5: 0.03074  loss_dice_5: 0.5614  loss_ce_6: 0  loss_mask_6: 0.03122  loss_dice_6: 0.5202  loss_ce_7: 0  loss_mask_7: 0.03391  loss_dice_7: 0.5106  loss_ce_8: 0  loss_mask_8: 0.02938  loss_dice_8: 0.5163  time: 0.4127  data_time: 0.0057  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:33 d2.utils.events]: [0m eta: 0:09:03  iter: 8679  total_loss: 5.546  loss_ce: 2.187e-09  loss_mask: 0.03149  loss_dice: 0.5028  loss_ce_0: 2.679e-05  loss_mask_0: 0.02935  loss_dice_0: 0.5138  loss_ce_1: 3.073e-08  loss_mask_1: 0.03105  loss_dice_1: 0.5405  loss_ce_2: 0  loss_mask_2: 0.02942  loss_dice_2: 0.5385  loss_ce_3: 0  loss_mask_3: 0.02977  loss_dice_3: 0.5148  loss_ce_4: 0  loss_mask_4: 0.03123  loss_dice_4: 0.5299  loss_ce_5: 0  loss_mask_5: 0.03306  loss_dice_5: 0.5577  loss_ce_6: 0  loss_mask_6: 0.03149  loss_dice_6: 0.515  loss_ce_7: 0  loss_mask_7: 0.02892  loss_dice_7: 0.519  loss_ce_8: 0  loss_mask_8: 0.02909  loss_dice_8: 0.5103  time: 0.4127  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:42 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:47:42 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:47:42 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:47:42 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:47:42 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:47:42 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:47:42 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.272276 (0.272276 s / iter per device, on 1 devices)
[32m[06/02 14:47:42 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210927 s / iter per device, on 1 devices)
[32m[06/02 14:47:42 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:47:42 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:47:42 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:47:42 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:47:42 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:47:42 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:47:42 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:47:42 d2.utils.events]: [0m eta: 0:08:55  iter: 8699  total_loss: 5.538  loss_ce: 3.5e-09  loss_mask: 0.03196  loss_dice: 0.5404  loss_ce_0: 2.669e-05  loss_mask_0: 0.03021  loss_dice_0: 0.5094  loss_ce_1: 2.844e-08  loss_mask_1: 0.02935  loss_dice_1: 0.5145  loss_ce_2: 0  loss_mask_2: 0.03466  loss_dice_2: 0.5884  loss_ce_3: 0  loss_mask_3: 0.03146  loss_dice_3: 0.5601  loss_ce_4: 0  loss_mask_4: 0.02969  loss_dice_4: 0.513  loss_ce_5: 0  loss_mask_5: 0.03201  loss_dice_5: 0.5257  loss_ce_6: 0  loss_mask_6: 0.0288  loss_dice_6: 0.5114  loss_ce_7: 0  loss_mask_7: 0.03013  loss_dice_7: 0.4824  loss_ce_8: 0  loss_mask_8: 0.02952  loss_dice_8: 0.5233  time: 0.4127  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:51 d2.utils.events]: [0m eta: 0:08:47  iter: 8719  total_loss: 5.684  loss_ce: 2.625e-09  loss_mask: 0.03086  loss_dice: 0.5372  loss_ce_0: 2.654e-05  loss_mask_0: 0.03443  loss_dice_0: 0.5311  loss_ce_1: 3.248e-08  loss_mask_1: 0.03087  loss_dice_1: 0.487  loss_ce_2: 0  loss_mask_2: 0.02799  loss_dice_2: 0.5291  loss_ce_3: 0  loss_mask_3: 0.02948  loss_dice_3: 0.5086  loss_ce_4: 0  loss_mask_4: 0.02925  loss_dice_4: 0.5152  loss_ce_5: 0  loss_mask_5: 0.03187  loss_dice_5: 0.5272  loss_ce_6: 0  loss_mask_6: 0.03098  loss_dice_6: 0.5226  loss_ce_7: 0  loss_mask_7: 0.0326  loss_dice_7: 0.5428  loss_ce_8: 0  loss_mask_8: 0.03093  loss_dice_8: 0.5315  time: 0.4127  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:47:59 d2.utils.events]: [0m eta: 0:08:38  iter: 8739  total_loss: 5.546  loss_ce: 2.187e-09  loss_mask: 0.03239  loss_dice: 0.5472  loss_ce_0: 2.644e-05  loss_mask_0: 0.02976  loss_dice_0: 0.4784  loss_ce_1: 3.128e-08  loss_mask_1: 0.0296  loss_dice_1: 0.5066  loss_ce_2: 0  loss_mask_2: 0.03033  loss_dice_2: 0.5683  loss_ce_3: 0  loss_mask_3: 0.02767  loss_dice_3: 0.4882  loss_ce_4: 0  loss_mask_4: 0.03548  loss_dice_4: 0.5319  loss_ce_5: 0  loss_mask_5: 0.03201  loss_dice_5: 0.5802  loss_ce_6: 0  loss_mask_6: 0.02837  loss_dice_6: 0.4932  loss_ce_7: 0  loss_mask_7: 0.03157  loss_dice_7: 0.4825  loss_ce_8: 0  loss_mask_8: 0.03121  loss_dice_8: 0.4997  time: 0.4127  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:03 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:48:03 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:48:03 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:48:03 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:48:03 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:48:03 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:48:04 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.264218 (0.264218 s / iter per device, on 1 devices)
[32m[06/02 14:48:04 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.201417 s / iter per device, on 1 devices)
[32m[06/02 14:48:04 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:48:04 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:48:04 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:48:04 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:48:04 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:48:04 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:48:04 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:48:08 d2.utils.events]: [0m eta: 0:08:30  iter: 8759  total_loss: 5.526  loss_ce: 2.625e-09  loss_mask: 0.03213  loss_dice: 0.513  loss_ce_0: 2.623e-05  loss_mask_0: 0.03279  loss_dice_0: 0.5475  loss_ce_1: 3.707e-08  loss_mask_1: 0.03011  loss_dice_1: 0.4938  loss_ce_2: 0  loss_mask_2: 0.03379  loss_dice_2: 0.5447  loss_ce_3: 0  loss_mask_3: 0.03312  loss_dice_3: 0.5189  loss_ce_4: 0  loss_mask_4: 0.03026  loss_dice_4: 0.5325  loss_ce_5: 0  loss_mask_5: 0.03176  loss_dice_5: 0.5685  loss_ce_6: 0  loss_mask_6: 0.0325  loss_dice_6: 0.5465  loss_ce_7: 0  loss_mask_7: 0.03071  loss_dice_7: 0.5142  loss_ce_8: 0  loss_mask_8: 0.03076  loss_dice_8: 0.5377  time: 0.4127  data_time: 0.0048  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:16 d2.utils.events]: [0m eta: 0:08:22  iter: 8779  total_loss: 5.446  loss_ce: 2.625e-09  loss_mask: 0.03067  loss_dice: 0.5106  loss_ce_0: 2.612e-05  loss_mask_0: 0.03148  loss_dice_0: 0.5262  loss_ce_1: 3.018e-08  loss_mask_1: 0.02798  loss_dice_1: 0.4788  loss_ce_2: 0  loss_mask_2: 0.02767  loss_dice_2: 0.4848  loss_ce_3: 0  loss_mask_3: 0.03031  loss_dice_3: 0.5473  loss_ce_4: 0  loss_mask_4: 0.03058  loss_dice_4: 0.5125  loss_ce_5: 0  loss_mask_5: 0.02996  loss_dice_5: 0.4908  loss_ce_6: 0  loss_mask_6: 0.02955  loss_dice_6: 0.5455  loss_ce_7: 0  loss_mask_7: 0.02902  loss_dice_7: 0.4997  loss_ce_8: 0  loss_mask_8: 0.02848  loss_dice_8: 0.4915  time: 0.4127  data_time: 0.0045  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:24 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:48:24 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:48:24 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:48:24 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:48:24 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:48:24 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:48:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278916 (0.278916 s / iter per device, on 1 devices)
[32m[06/02 14:48:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.217743 s / iter per device, on 1 devices)
[32m[06/02 14:48:25 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:48:25 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:48:25 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:48:25 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:48:25 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:48:25 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:48:25 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:48:25 d2.utils.events]: [0m eta: 0:08:14  iter: 8799  total_loss: 5.63  loss_ce: 2.187e-09  loss_mask: 0.03053  loss_dice: 0.4957  loss_ce_0: 2.605e-05  loss_mask_0: 0.03161  loss_dice_0: 0.565  loss_ce_1: 3.117e-08  loss_mask_1: 0.03214  loss_dice_1: 0.4979  loss_ce_2: 0  loss_mask_2: 0.03232  loss_dice_2: 0.523  loss_ce_3: 0  loss_mask_3: 0.02995  loss_dice_3: 0.4642  loss_ce_4: 0  loss_mask_4: 0.03345  loss_dice_4: 0.5408  loss_ce_5: 0  loss_mask_5: 0.03213  loss_dice_5: 0.5519  loss_ce_6: 0  loss_mask_6: 0.03028  loss_dice_6: 0.5424  loss_ce_7: 0  loss_mask_7: 0.03304  loss_dice_7: 0.5573  loss_ce_8: 0  loss_mask_8: 0.03195  loss_dice_8: 0.5029  time: 0.4127  data_time: 0.0045  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:33 d2.utils.events]: [0m eta: 0:08:05  iter: 8819  total_loss: 5.457  loss_ce: 2.187e-09  loss_mask: 0.03053  loss_dice: 0.4855  loss_ce_0: 2.591e-05  loss_mask_0: 0.03207  loss_dice_0: 0.5605  loss_ce_1: 4.057e-08  loss_mask_1: 0.02936  loss_dice_1: 0.4912  loss_ce_2: 0  loss_mask_2: 0.03099  loss_dice_2: 0.5121  loss_ce_3: 0  loss_mask_3: 0.02592  loss_dice_3: 0.4851  loss_ce_4: 0  loss_mask_4: 0.02945  loss_dice_4: 0.4793  loss_ce_5: 0  loss_mask_5: 0.02812  loss_dice_5: 0.4812  loss_ce_6: 0  loss_mask_6: 0.03174  loss_dice_6: 0.5188  loss_ce_7: 0  loss_mask_7: 0.03196  loss_dice_7: 0.51  loss_ce_8: 0  loss_mask_8: 0.03377  loss_dice_8: 0.5089  time: 0.4126  data_time: 0.0067  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:41 d2.utils.events]: [0m eta: 0:07:57  iter: 8839  total_loss: 5.708  loss_ce: 2.187e-09  loss_mask: 0.03022  loss_dice: 0.5435  loss_ce_0: 2.58e-05  loss_mask_0: 0.03135  loss_dice_0: 0.5001  loss_ce_1: 3.773e-08  loss_mask_1: 0.0327  loss_dice_1: 0.5236  loss_ce_2: 0  loss_mask_2: 0.03147  loss_dice_2: 0.5075  loss_ce_3: 0  loss_mask_3: 0.03018  loss_dice_3: 0.5179  loss_ce_4: 0  loss_mask_4: 0.03301  loss_dice_4: 0.575  loss_ce_5: 0  loss_mask_5: 0.03139  loss_dice_5: 0.5595  loss_ce_6: 0  loss_mask_6: 0.03115  loss_dice_6: 0.5126  loss_ce_7: 0  loss_mask_7: 0.03166  loss_dice_7: 0.5255  loss_ce_8: 0  loss_mask_8: 0.0316  loss_dice_8: 0.5569  time: 0.4126  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:45 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:48:45 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:48:45 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:48:45 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:48:45 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:48:45 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:48:46 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.260258 (0.260258 s / iter per device, on 1 devices)
[32m[06/02 14:48:46 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.200078 s / iter per device, on 1 devices)
[32m[06/02 14:48:46 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:48:46 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:48:46 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:48:46 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:48:46 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:48:46 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:48:46 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:48:50 d2.utils.events]: [0m eta: 0:07:49  iter: 8859  total_loss: 5.514  loss_ce: 2.187e-09  loss_mask: 0.03291  loss_dice: 0.5704  loss_ce_0: 2.563e-05  loss_mask_0: 0.0307  loss_dice_0: 0.5586  loss_ce_1: 3.368e-08  loss_mask_1: 0.03005  loss_dice_1: 0.4796  loss_ce_2: 0  loss_mask_2: 0.02996  loss_dice_2: 0.4966  loss_ce_3: 0  loss_mask_3: 0.03197  loss_dice_3: 0.4926  loss_ce_4: 0  loss_mask_4: 0.03051  loss_dice_4: 0.5196  loss_ce_5: 0  loss_mask_5: 0.03103  loss_dice_5: 0.5165  loss_ce_6: 0  loss_mask_6: 0.03188  loss_dice_6: 0.5558  loss_ce_7: 0  loss_mask_7: 0.03126  loss_dice_7: 0.5119  loss_ce_8: 0  loss_mask_8: 0.02893  loss_dice_8: 0.5057  time: 0.4126  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:48:58 d2.utils.events]: [0m eta: 0:07:40  iter: 8879  total_loss: 5.59  loss_ce: 2.187e-09  loss_mask: 0.03317  loss_dice: 0.495  loss_ce_0: 2.553e-05  loss_mask_0: 0.03198  loss_dice_0: 0.5292  loss_ce_1: 3.773e-08  loss_mask_1: 0.03151  loss_dice_1: 0.5167  loss_ce_2: 0  loss_mask_2: 0.03303  loss_dice_2: 0.5215  loss_ce_3: 0  loss_mask_3: 0.03274  loss_dice_3: 0.5479  loss_ce_4: 0  loss_mask_4: 0.0299  loss_dice_4: 0.5191  loss_ce_5: 0  loss_mask_5: 0.03147  loss_dice_5: 0.5061  loss_ce_6: 0  loss_mask_6: 0.03186  loss_dice_6: 0.5237  loss_ce_7: 0  loss_mask_7: 0.03115  loss_dice_7: 0.5612  loss_ce_8: 0  loss_mask_8: 0.03124  loss_dice_8: 0.542  time: 0.4126  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:49:06 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:49:06 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:49:06 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:49:06 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:49:06 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:49:06 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:49:07 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.243591 (0.243591 s / iter per device, on 1 devices)
[32m[06/02 14:49:07 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.182281 s / iter per device, on 1 devices)
[32m[06/02 14:49:07 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:49:07 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:49:07 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:49:07 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:49:07 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:49:07 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:49:07 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:49:07 d2.utils.events]: [0m eta: 0:07:32  iter: 8899  total_loss: 5.448  loss_ce: 2.625e-09  loss_mask: 0.02838  loss_dice: 0.5458  loss_ce_0: 2.541e-05  loss_mask_0: 0.03065  loss_dice_0: 0.4906  loss_ce_1: 3.161e-08  loss_mask_1: 0.02936  loss_dice_1: 0.5195  loss_ce_2: 0  loss_mask_2: 0.03012  loss_dice_2: 0.5088  loss_ce_3: 0  loss_mask_3: 0.03471  loss_dice_3: 0.5003  loss_ce_4: 0  loss_mask_4: 0.02873  loss_dice_4: 0.4679  loss_ce_5: 0  loss_mask_5: 0.03149  loss_dice_5: 0.5312  loss_ce_6: 0  loss_mask_6: 0.03253  loss_dice_6: 0.5317  loss_ce_7: 0  loss_mask_7: 0.02691  loss_dice_7: 0.5003  loss_ce_8: 0  loss_mask_8: 0.02845  loss_dice_8: 0.5067  time: 0.4125  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:49:15 d2.utils.events]: [0m eta: 0:07:23  iter: 8919  total_loss: 5.516  loss_ce: 2.625e-09  loss_mask: 0.02994  loss_dice: 0.5134  loss_ce_0: 2.534e-05  loss_mask_0: 0.03364  loss_dice_0: 0.527  loss_ce_1: 3.139e-08  loss_mask_1: 0.03215  loss_dice_1: 0.54  loss_ce_2: 0  loss_mask_2: 0.03099  loss_dice_2: 0.5409  loss_ce_3: 0  loss_mask_3: 0.0312  loss_dice_3: 0.5197  loss_ce_4: 0  loss_mask_4: 0.03044  loss_dice_4: 0.5002  loss_ce_5: 0  loss_mask_5: 0.02967  loss_dice_5: 0.4808  loss_ce_6: 0  loss_mask_6: 0.03086  loss_dice_6: 0.5096  loss_ce_7: 0  loss_mask_7: 0.03157  loss_dice_7: 0.55  loss_ce_8: 0  loss_mask_8: 0.03036  loss_dice_8: 0.5029  time: 0.4125  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:49:23 d2.utils.events]: [0m eta: 0:07:15  iter: 8939  total_loss: 5.471  loss_ce: 2.625e-09  loss_mask: 0.03059  loss_dice: 0.5206  loss_ce_0: 2.517e-05  loss_mask_0: 0.03305  loss_dice_0: 0.5412  loss_ce_1: 3.861e-08  loss_mask_1: 0.02883  loss_dice_1: 0.4867  loss_ce_2: 0  loss_mask_2: 0.03306  loss_dice_2: 0.5519  loss_ce_3: 0  loss_mask_3: 0.03119  loss_dice_3: 0.4911  loss_ce_4: 0  loss_mask_4: 0.02924  loss_dice_4: 0.5142  loss_ce_5: 0  loss_mask_5: 0.02949  loss_dice_5: 0.4696  loss_ce_6: 0  loss_mask_6: 0.02967  loss_dice_6: 0.5094  loss_ce_7: 0  loss_mask_7: 0.03387  loss_dice_7: 0.5285  loss_ce_8: 0  loss_mask_8: 0.0306  loss_dice_8: 0.5463  time: 0.4125  data_time: 0.0067  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:49:27 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:49:27 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:49:27 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:49:27 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:49:27 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:49:27 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:49:28 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.252705 (0.252705 s / iter per device, on 1 devices)
[32m[06/02 14:49:28 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.191752 s / iter per device, on 1 devices)
[32m[06/02 14:49:28 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:49:28 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:49:28 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:49:28 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:49:28 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:49:28 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:49:28 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:49:32 d2.utils.events]: [0m eta: 0:07:06  iter: 8959  total_loss: 5.628  loss_ce: 2.734e-09  loss_mask: 0.03178  loss_dice: 0.5147  loss_ce_0: 2.503e-05  loss_mask_0: 0.03177  loss_dice_0: 0.5211  loss_ce_1: 3.39e-08  loss_mask_1: 0.03074  loss_dice_1: 0.5597  loss_ce_2: 0  loss_mask_2: 0.02924  loss_dice_2: 0.5319  loss_ce_3: 0  loss_mask_3: 0.02758  loss_dice_3: 0.4951  loss_ce_4: 0  loss_mask_4: 0.02959  loss_dice_4: 0.5027  loss_ce_5: 0  loss_mask_5: 0.0339  loss_dice_5: 0.5645  loss_ce_6: 0  loss_mask_6: 0.03194  loss_dice_6: 0.5344  loss_ce_7: 0  loss_mask_7: 0.03118  loss_dice_7: 0.5447  loss_ce_8: 0  loss_mask_8: 0.0291  loss_dice_8: 0.5249  time: 0.4125  data_time: 0.0068  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:49:40 d2.utils.events]: [0m eta: 0:06:58  iter: 8979  total_loss: 5.51  loss_ce: 5.14e-09  loss_mask: 0.02947  loss_dice: 0.4958  loss_ce_0: 2.495e-05  loss_mask_0: 0.03062  loss_dice_0: 0.547  loss_ce_1: 3.423e-08  loss_mask_1: 0.03149  loss_dice_1: 0.5269  loss_ce_2: 0  loss_mask_2: 0.03132  loss_dice_2: 0.4847  loss_ce_3: 0  loss_mask_3: 0.0298  loss_dice_3: 0.5135  loss_ce_4: 0  loss_mask_4: 0.02877  loss_dice_4: 0.5053  loss_ce_5: 0  loss_mask_5: 0.03255  loss_dice_5: 0.5347  loss_ce_6: 0  loss_mask_6: 0.02768  loss_dice_6: 0.5056  loss_ce_7: 0  loss_mask_7: 0.0299  loss_dice_7: 0.4827  loss_ce_8: 0  loss_mask_8: 0.03032  loss_dice_8: 0.5189  time: 0.4125  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:49:48 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0008999.pth
[32m[06/02 14:49:51 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:49:51 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:49:51 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:49:51 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:49:51 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:49:51 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:49:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282920 (0.282920 s / iter per device, on 1 devices)
[32m[06/02 14:49:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.222830 s / iter per device, on 1 devices)
[32m[06/02 14:49:52 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:49:52 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:49:52 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:49:52 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:49:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:49:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:49:52 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:49:52 d2.utils.events]: [0m eta: 0:06:50  iter: 8999  total_loss: 5.586  loss_ce: 2.953e-09  loss_mask: 0.03234  loss_dice: 0.5096  loss_ce_0: 2.48e-05  loss_mask_0: 0.03123  loss_dice_0: 0.5232  loss_ce_1: 3.642e-08  loss_mask_1: 0.03033  loss_dice_1: 0.5109  loss_ce_2: 0  loss_mask_2: 0.03297  loss_dice_2: 0.527  loss_ce_3: 0  loss_mask_3: 0.03137  loss_dice_3: 0.535  loss_ce_4: 0  loss_mask_4: 0.03202  loss_dice_4: 0.5704  loss_ce_5: 0  loss_mask_5: 0.03239  loss_dice_5: 0.5629  loss_ce_6: 0  loss_mask_6: 0.03087  loss_dice_6: 0.5242  loss_ce_7: 0  loss_mask_7: 0.02955  loss_dice_7: 0.5057  loss_ce_8: 0  loss_mask_8: 0.03177  loss_dice_8: 0.4888  time: 0.4125  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:01 d2.utils.events]: [0m eta: 0:06:42  iter: 9019  total_loss: 5.487  loss_ce: 3.5e-09  loss_mask: 0.02991  loss_dice: 0.4887  loss_ce_0: 2.463e-05  loss_mask_0: 0.02834  loss_dice_0: 0.5178  loss_ce_1: 4.823e-08  loss_mask_1: 0.03233  loss_dice_1: 0.5102  loss_ce_2: 0  loss_mask_2: 0.03163  loss_dice_2: 0.5087  loss_ce_3: 0  loss_mask_3: 0.03052  loss_dice_3: 0.512  loss_ce_4: 0  loss_mask_4: 0.0293  loss_dice_4: 0.5459  loss_ce_5: 0  loss_mask_5: 0.03042  loss_dice_5: 0.5419  loss_ce_6: 0  loss_mask_6: 0.03111  loss_dice_6: 0.5193  loss_ce_7: 0  loss_mask_7: 0.0334  loss_dice_7: 0.5485  loss_ce_8: 0  loss_mask_8: 0.03043  loss_dice_8: 0.552  time: 0.4125  data_time: 0.0064  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:09 d2.utils.events]: [0m eta: 0:06:34  iter: 9039  total_loss: 5.571  loss_ce: 2.844e-09  loss_mask: 0.03203  loss_dice: 0.5257  loss_ce_0: 2.454e-05  loss_mask_0: 0.03266  loss_dice_0: 0.523  loss_ce_1: 4.036e-08  loss_mask_1: 0.02988  loss_dice_1: 0.509  loss_ce_2: 0  loss_mask_2: 0.03154  loss_dice_2: 0.52  loss_ce_3: 0  loss_mask_3: 0.02965  loss_dice_3: 0.4781  loss_ce_4: 0  loss_mask_4: 0.03107  loss_dice_4: 0.4783  loss_ce_5: 0  loss_mask_5: 0.03437  loss_dice_5: 0.5561  loss_ce_6: 0  loss_mask_6: 0.03159  loss_dice_6: 0.5265  loss_ce_7: 0  loss_mask_7: 0.03386  loss_dice_7: 0.5507  loss_ce_8: 0  loss_mask_8: 0.03123  loss_dice_8: 0.4934  time: 0.4125  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:13 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:50:13 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:50:13 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:50:13 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:50:13 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:50:13 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:50:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.260697 (0.260697 s / iter per device, on 1 devices)
[32m[06/02 14:50:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.200379 s / iter per device, on 1 devices)
[32m[06/02 14:50:14 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:50:14 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:50:14 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:50:14 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:50:14 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:50:14 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:50:14 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:50:18 d2.utils.events]: [0m eta: 0:06:26  iter: 9059  total_loss: 5.612  loss_ce: 2.187e-09  loss_mask: 0.03047  loss_dice: 0.5513  loss_ce_0: 2.439e-05  loss_mask_0: 0.03345  loss_dice_0: 0.5304  loss_ce_1: 5.282e-08  loss_mask_1: 0.03073  loss_dice_1: 0.5022  loss_ce_2: 0  loss_mask_2: 0.03062  loss_dice_2: 0.5427  loss_ce_3: 0  loss_mask_3: 0.03299  loss_dice_3: 0.5524  loss_ce_4: 0  loss_mask_4: 0.02707  loss_dice_4: 0.5299  loss_ce_5: 0  loss_mask_5: 0.02947  loss_dice_5: 0.533  loss_ce_6: 0  loss_mask_6: 0.03044  loss_dice_6: 0.5512  loss_ce_7: 0  loss_mask_7: 0.03307  loss_dice_7: 0.5328  loss_ce_8: 0  loss_mask_8: 0.02951  loss_dice_8: 0.5199  time: 0.4125  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:26 d2.utils.events]: [0m eta: 0:06:18  iter: 9079  total_loss: 5.636  loss_ce: 2.187e-09  loss_mask: 0.03056  loss_dice: 0.5389  loss_ce_0: 2.428e-05  loss_mask_0: 0.02947  loss_dice_0: 0.5213  loss_ce_1: 5.326e-08  loss_mask_1: 0.02862  loss_dice_1: 0.5536  loss_ce_2: 0  loss_mask_2: 0.03197  loss_dice_2: 0.5253  loss_ce_3: 0  loss_mask_3: 0.0303  loss_dice_3: 0.5868  loss_ce_4: 0  loss_mask_4: 0.03163  loss_dice_4: 0.5195  loss_ce_5: 0  loss_mask_5: 0.0285  loss_dice_5: 0.5225  loss_ce_6: 0  loss_mask_6: 0.02671  loss_dice_6: 0.5129  loss_ce_7: 0  loss_mask_7: 0.03199  loss_dice_7: 0.549  loss_ce_8: 0  loss_mask_8: 0.02884  loss_dice_8: 0.4901  time: 0.4125  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:34 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:50:34 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:50:34 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:50:34 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:50:34 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:50:34 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:50:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.258914 (0.258914 s / iter per device, on 1 devices)
[32m[06/02 14:50:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.195792 s / iter per device, on 1 devices)
[32m[06/02 14:50:35 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:50:35 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:50:35 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:50:35 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:50:35 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:50:35 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:50:35 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:50:35 d2.utils.events]: [0m eta: 0:06:10  iter: 9099  total_loss: 5.535  loss_ce: 2.187e-09  loss_mask: 0.0333  loss_dice: 0.5664  loss_ce_0: 2.422e-05  loss_mask_0: 0.02799  loss_dice_0: 0.476  loss_ce_1: 3.653e-08  loss_mask_1: 0.02978  loss_dice_1: 0.499  loss_ce_2: 0  loss_mask_2: 0.0297  loss_dice_2: 0.4816  loss_ce_3: 0  loss_mask_3: 0.03021  loss_dice_3: 0.5369  loss_ce_4: 0  loss_mask_4: 0.03391  loss_dice_4: 0.5307  loss_ce_5: 0  loss_mask_5: 0.02983  loss_dice_5: 0.5199  loss_ce_6: 0  loss_mask_6: 0.02892  loss_dice_6: 0.5081  loss_ce_7: 0  loss_mask_7: 0.03289  loss_dice_7: 0.5272  loss_ce_8: 0  loss_mask_8: 0.03398  loss_dice_8: 0.5634  time: 0.4125  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:43 d2.utils.events]: [0m eta: 0:06:01  iter: 9119  total_loss: 5.63  loss_ce: 2.187e-09  loss_mask: 0.03156  loss_dice: 0.5168  loss_ce_0: 2.412e-05  loss_mask_0: 0.03076  loss_dice_0: 0.5063  loss_ce_1: 4.025e-08  loss_mask_1: 0.03114  loss_dice_1: 0.5003  loss_ce_2: 0  loss_mask_2: 0.03538  loss_dice_2: 0.5424  loss_ce_3: 0  loss_mask_3: 0.03367  loss_dice_3: 0.5348  loss_ce_4: 0  loss_mask_4: 0.0334  loss_dice_4: 0.521  loss_ce_5: 0  loss_mask_5: 0.03063  loss_dice_5: 0.5368  loss_ce_6: 0  loss_mask_6: 0.03148  loss_dice_6: 0.517  loss_ce_7: 0  loss_mask_7: 0.03354  loss_dice_7: 0.5454  loss_ce_8: 0  loss_mask_8: 0.03055  loss_dice_8: 0.4976  time: 0.4125  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:52 d2.utils.events]: [0m eta: 0:05:53  iter: 9139  total_loss: 5.625  loss_ce: 2.187e-09  loss_mask: 0.0317  loss_dice: 0.5642  loss_ce_0: 2.4e-05  loss_mask_0: 0.0303  loss_dice_0: 0.5287  loss_ce_1: 4.386e-08  loss_mask_1: 0.02747  loss_dice_1: 0.5196  loss_ce_2: 0  loss_mask_2: 0.03048  loss_dice_2: 0.5571  loss_ce_3: 0  loss_mask_3: 0.02931  loss_dice_3: 0.5036  loss_ce_4: 0  loss_mask_4: 0.02959  loss_dice_4: 0.4856  loss_ce_5: 0  loss_mask_5: 0.02823  loss_dice_5: 0.5341  loss_ce_6: 0  loss_mask_6: 0.03242  loss_dice_6: 0.5473  loss_ce_7: 0  loss_mask_7: 0.03068  loss_dice_7: 0.5059  loss_ce_8: 0  loss_mask_8: 0.03031  loss_dice_8: 0.5172  time: 0.4125  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:50:56 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:50:56 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:50:56 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:50:56 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:50:56 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:50:56 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:50:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.280218 (0.280218 s / iter per device, on 1 devices)
[32m[06/02 14:50:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.219512 s / iter per device, on 1 devices)
[32m[06/02 14:50:57 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:50:57 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:50:57 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:50:57 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:50:57 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:50:57 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:50:57 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:51:01 d2.utils.events]: [0m eta: 0:05:45  iter: 9159  total_loss: 5.478  loss_ce: 2.187e-09  loss_mask: 0.02909  loss_dice: 0.5222  loss_ce_0: 2.387e-05  loss_mask_0: 0.02937  loss_dice_0: 0.5079  loss_ce_1: 3.325e-08  loss_mask_1: 0.02818  loss_dice_1: 0.5168  loss_ce_2: 0  loss_mask_2: 0.0276  loss_dice_2: 0.492  loss_ce_3: 0  loss_mask_3: 0.03482  loss_dice_3: 0.5628  loss_ce_4: 0  loss_mask_4: 0.03117  loss_dice_4: 0.5218  loss_ce_5: 0  loss_mask_5: 0.02908  loss_dice_5: 0.4968  loss_ce_6: 0  loss_mask_6: 0.03055  loss_dice_6: 0.5318  loss_ce_7: 0  loss_mask_7: 0.02912  loss_dice_7: 0.4872  loss_ce_8: 0  loss_mask_8: 0.03303  loss_dice_8: 0.571  time: 0.4125  data_time: 0.0056  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:51:09 d2.utils.events]: [0m eta: 0:05:37  iter: 9179  total_loss: 5.519  loss_ce: 2.187e-09  loss_mask: 0.0304  loss_dice: 0.4985  loss_ce_0: 2.369e-05  loss_mask_0: 0.02904  loss_dice_0: 0.5157  loss_ce_1: 4.287e-08  loss_mask_1: 0.03074  loss_dice_1: 0.497  loss_ce_2: 0  loss_mask_2: 0.03263  loss_dice_2: 0.5213  loss_ce_3: 0  loss_mask_3: 0.03191  loss_dice_3: 0.5588  loss_ce_4: 0  loss_mask_4: 0.02874  loss_dice_4: 0.5097  loss_ce_5: 0  loss_mask_5: 0.03075  loss_dice_5: 0.496  loss_ce_6: 0  loss_mask_6: 0.03001  loss_dice_6: 0.5051  loss_ce_7: 0  loss_mask_7: 0.03188  loss_dice_7: 0.5286  loss_ce_8: 0  loss_mask_8: 0.03154  loss_dice_8: 0.5611  time: 0.4125  data_time: 0.0060  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:51:17 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:51:17 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:51:17 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:51:17 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:51:17 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:51:17 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:51:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.282138 (0.282138 s / iter per device, on 1 devices)
[32m[06/02 14:51:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.216396 s / iter per device, on 1 devices)
[32m[06/02 14:51:18 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:51:18 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:51:18 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:51:18 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:51:18 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:51:18 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:51:18 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:51:18 d2.utils.events]: [0m eta: 0:05:29  iter: 9199  total_loss: 5.673  loss_ce: 2.187e-09  loss_mask: 0.03248  loss_dice: 0.5419  loss_ce_0: 2.359e-05  loss_mask_0: 0.03131  loss_dice_0: 0.5656  loss_ce_1: 3.511e-08  loss_mask_1: 0.03464  loss_dice_1: 0.5591  loss_ce_2: 0  loss_mask_2: 0.02968  loss_dice_2: 0.5561  loss_ce_3: 0  loss_mask_3: 0.03212  loss_dice_3: 0.4941  loss_ce_4: 0  loss_mask_4: 0.0294  loss_dice_4: 0.5128  loss_ce_5: 0  loss_mask_5: 0.02985  loss_dice_5: 0.5049  loss_ce_6: 0  loss_mask_6: 0.03421  loss_dice_6: 0.5817  loss_ce_7: 0  loss_mask_7: 0.02903  loss_dice_7: 0.5026  loss_ce_8: 0  loss_mask_8: 0.03017  loss_dice_8: 0.5242  time: 0.4125  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:51:26 d2.utils.events]: [0m eta: 0:05:20  iter: 9219  total_loss: 5.502  loss_ce: 2.406e-09  loss_mask: 0.02824  loss_dice: 0.5018  loss_ce_0: 2.348e-05  loss_mask_0: 0.029  loss_dice_0: 0.5506  loss_ce_1: 3.609e-08  loss_mask_1: 0.03289  loss_dice_1: 0.5058  loss_ce_2: 0  loss_mask_2: 0.03034  loss_dice_2: 0.5263  loss_ce_3: 0  loss_mask_3: 0.03145  loss_dice_3: 0.5187  loss_ce_4: 0  loss_mask_4: 0.03065  loss_dice_4: 0.522  loss_ce_5: 0  loss_mask_5: 0.02932  loss_dice_5: 0.4783  loss_ce_6: 0  loss_mask_6: 0.02971  loss_dice_6: 0.5226  loss_ce_7: 0  loss_mask_7: 0.02963  loss_dice_7: 0.5069  loss_ce_8: 0  loss_mask_8: 0.03087  loss_dice_8: 0.5274  time: 0.4125  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:51:35 d2.utils.events]: [0m eta: 0:05:12  iter: 9239  total_loss: 5.676  loss_ce: 2.187e-09  loss_mask: 0.02923  loss_dice: 0.4849  loss_ce_0: 2.335e-05  loss_mask_0: 0.02838  loss_dice_0: 0.5183  loss_ce_1: 3.543e-08  loss_mask_1: 0.02743  loss_dice_1: 0.5179  loss_ce_2: 0  loss_mask_2: 0.03107  loss_dice_2: 0.5158  loss_ce_3: 0  loss_mask_3: 0.03035  loss_dice_3: 0.5529  loss_ce_4: 0  loss_mask_4: 0.03096  loss_dice_4: 0.5237  loss_ce_5: 0  loss_mask_5: 0.031  loss_dice_5: 0.5717  loss_ce_6: 0  loss_mask_6: 0.0323  loss_dice_6: 0.5417  loss_ce_7: 0  loss_mask_7: 0.02809  loss_dice_7: 0.5164  loss_ce_8: 0  loss_mask_8: 0.02915  loss_dice_8: 0.5449  time: 0.4125  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:51:39 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:51:39 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:51:39 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:51:39 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:51:39 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:51:39 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:51:39 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273287 (0.273287 s / iter per device, on 1 devices)
[32m[06/02 14:51:39 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.209973 s / iter per device, on 1 devices)
[32m[06/02 14:51:39 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:51:39 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:51:39 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:51:39 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:51:39 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:51:39 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:51:39 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:51:43 d2.utils.events]: [0m eta: 0:05:04  iter: 9259  total_loss: 5.494  loss_ce: 2.187e-09  loss_mask: 0.03068  loss_dice: 0.5245  loss_ce_0: 2.326e-05  loss_mask_0: 0.03032  loss_dice_0: 0.5483  loss_ce_1: 3.948e-08  loss_mask_1: 0.03026  loss_dice_1: 0.5379  loss_ce_2: 0  loss_mask_2: 0.03107  loss_dice_2: 0.5228  loss_ce_3: 0  loss_mask_3: 0.02971  loss_dice_3: 0.5131  loss_ce_4: 0  loss_mask_4: 0.02867  loss_dice_4: 0.5136  loss_ce_5: 0  loss_mask_5: 0.03097  loss_dice_5: 0.5123  loss_ce_6: 0  loss_mask_6: 0.0315  loss_dice_6: 0.5047  loss_ce_7: 0  loss_mask_7: 0.03047  loss_dice_7: 0.4991  loss_ce_8: 0  loss_mask_8: 0.03193  loss_dice_8: 0.5003  time: 0.4125  data_time: 0.0059  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:51:52 d2.utils.events]: [0m eta: 0:04:55  iter: 9279  total_loss: 5.48  loss_ce: 2.515e-09  loss_mask: 0.02838  loss_dice: 0.4944  loss_ce_0: 2.31e-05  loss_mask_0: 0.02739  loss_dice_0: 0.4671  loss_ce_1: 3.281e-08  loss_mask_1: 0.02601  loss_dice_1: 0.4714  loss_ce_2: 0  loss_mask_2: 0.03074  loss_dice_2: 0.5065  loss_ce_3: 0  loss_mask_3: 0.02997  loss_dice_3: 0.5175  loss_ce_4: 0  loss_mask_4: 0.03032  loss_dice_4: 0.5228  loss_ce_5: 0  loss_mask_5: 0.03129  loss_dice_5: 0.548  loss_ce_6: 0  loss_mask_6: 0.02762  loss_dice_6: 0.486  loss_ce_7: 0  loss_mask_7: 0.0303  loss_dice_7: 0.4819  loss_ce_8: 0  loss_mask_8: 0.0302  loss_dice_8: 0.5535  time: 0.4125  data_time: 0.0044  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:00 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:52:00 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:52:00 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:52:00 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:52:00 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:52:00 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:52:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.298127 (0.298127 s / iter per device, on 1 devices)
[32m[06/02 14:52:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.239275 s / iter per device, on 1 devices)
[32m[06/02 14:52:01 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:52:01 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:52:01 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:52:01 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:52:01 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:52:01 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:52:01 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:52:01 d2.utils.events]: [0m eta: 0:04:47  iter: 9299  total_loss: 5.581  loss_ce: 2.625e-09  loss_mask: 0.02988  loss_dice: 0.4835  loss_ce_0: 2.299e-05  loss_mask_0: 0.0304  loss_dice_0: 0.524  loss_ce_1: 3.193e-08  loss_mask_1: 0.02996  loss_dice_1: 0.4959  loss_ce_2: 0  loss_mask_2: 0.03216  loss_dice_2: 0.5619  loss_ce_3: 0  loss_mask_3: 0.03216  loss_dice_3: 0.5495  loss_ce_4: 0  loss_mask_4: 0.02959  loss_dice_4: 0.506  loss_ce_5: 0  loss_mask_5: 0.03154  loss_dice_5: 0.5398  loss_ce_6: 0  loss_mask_6: 0.03143  loss_dice_6: 0.527  loss_ce_7: 0  loss_mask_7: 0.02816  loss_dice_7: 0.5152  loss_ce_8: 0  loss_mask_8: 0.02884  loss_dice_8: 0.5033  time: 0.4125  data_time: 0.0067  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:09 d2.utils.events]: [0m eta: 0:04:39  iter: 9319  total_loss: 5.448  loss_ce: 2.187e-09  loss_mask: 0.03239  loss_dice: 0.522  loss_ce_0: 2.29e-05  loss_mask_0: 0.03128  loss_dice_0: 0.5074  loss_ce_1: 3.281e-08  loss_mask_1: 0.02969  loss_dice_1: 0.5614  loss_ce_2: 0  loss_mask_2: 0.03225  loss_dice_2: 0.5128  loss_ce_3: 0  loss_mask_3: 0.03033  loss_dice_3: 0.4931  loss_ce_4: 0  loss_mask_4: 0.02913  loss_dice_4: 0.502  loss_ce_5: 0  loss_mask_5: 0.03108  loss_dice_5: 0.4934  loss_ce_6: 0  loss_mask_6: 0.02827  loss_dice_6: 0.4743  loss_ce_7: 0  loss_mask_7: 0.03174  loss_dice_7: 0.5183  loss_ce_8: 0  loss_mask_8: 0.0339  loss_dice_8: 0.4978  time: 0.4125  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:17 d2.utils.events]: [0m eta: 0:04:30  iter: 9339  total_loss: 5.424  loss_ce: 2.625e-09  loss_mask: 0.03017  loss_dice: 0.4995  loss_ce_0: 2.276e-05  loss_mask_0: 0.03064  loss_dice_0: 0.4958  loss_ce_1: 3.325e-08  loss_mask_1: 0.03138  loss_dice_1: 0.5417  loss_ce_2: 0  loss_mask_2: 0.03318  loss_dice_2: 0.5089  loss_ce_3: 0  loss_mask_3: 0.02884  loss_dice_3: 0.5123  loss_ce_4: 0  loss_mask_4: 0.03111  loss_dice_4: 0.5461  loss_ce_5: 0  loss_mask_5: 0.02981  loss_dice_5: 0.4822  loss_ce_6: 0  loss_mask_6: 0.02765  loss_dice_6: 0.492  loss_ce_7: 0  loss_mask_7: 0.031  loss_dice_7: 0.5179  loss_ce_8: 0  loss_mask_8: 0.03117  loss_dice_8: 0.5478  time: 0.4125  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:21 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:52:21 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:52:21 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:52:21 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:52:21 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:52:21 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:52:22 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278280 (0.278280 s / iter per device, on 1 devices)
[32m[06/02 14:52:22 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215642 s / iter per device, on 1 devices)
[32m[06/02 14:52:22 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:52:22 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:52:22 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:52:22 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:52:22 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:52:22 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:52:22 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:52:26 d2.utils.events]: [0m eta: 0:04:22  iter: 9359  total_loss: 5.537  loss_ce: 2.844e-09  loss_mask: 0.03256  loss_dice: 0.4886  loss_ce_0: 2.267e-05  loss_mask_0: 0.02997  loss_dice_0: 0.49  loss_ce_1: 4.484e-08  loss_mask_1: 0.03114  loss_dice_1: 0.5303  loss_ce_2: 0  loss_mask_2: 0.03032  loss_dice_2: 0.532  loss_ce_3: 0  loss_mask_3: 0.03162  loss_dice_3: 0.5726  loss_ce_4: 0  loss_mask_4: 0.03212  loss_dice_4: 0.5392  loss_ce_5: 0  loss_mask_5: 0.03227  loss_dice_5: 0.533  loss_ce_6: 0  loss_mask_6: 0.03215  loss_dice_6: 0.557  loss_ce_7: 0  loss_mask_7: 0.03044  loss_dice_7: 0.5068  loss_ce_8: 0  loss_mask_8: 0.03221  loss_dice_8: 0.5087  time: 0.4125  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:35 d2.utils.events]: [0m eta: 0:04:14  iter: 9379  total_loss: 5.447  loss_ce: 3.281e-09  loss_mask: 0.02874  loss_dice: 0.5267  loss_ce_0: 2.255e-05  loss_mask_0: 0.03191  loss_dice_0: 0.5452  loss_ce_1: 4.156e-08  loss_mask_1: 0.03337  loss_dice_1: 0.5125  loss_ce_2: 0  loss_mask_2: 0.02956  loss_dice_2: 0.5303  loss_ce_3: 0  loss_mask_3: 0.02924  loss_dice_3: 0.4727  loss_ce_4: 0  loss_mask_4: 0.02947  loss_dice_4: 0.514  loss_ce_5: 0  loss_mask_5: 0.02713  loss_dice_5: 0.494  loss_ce_6: 0  loss_mask_6: 0.03188  loss_dice_6: 0.513  loss_ce_7: 0  loss_mask_7: 0.02769  loss_dice_7: 0.5259  loss_ce_8: 0  loss_mask_8: 0.03147  loss_dice_8: 0.4906  time: 0.4125  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:43 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:52:43 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:52:43 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:52:43 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:52:43 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:52:43 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:52:43 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.249653 (0.249653 s / iter per device, on 1 devices)
[32m[06/02 14:52:43 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.189767 s / iter per device, on 1 devices)
[32m[06/02 14:52:43 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:52:43 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:52:43 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:52:43 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:52:43 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:52:43 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:52:43 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:52:43 d2.utils.events]: [0m eta: 0:04:06  iter: 9399  total_loss: 5.226  loss_ce: 3.5e-09  loss_mask: 0.03063  loss_dice: 0.5019  loss_ce_0: 2.246e-05  loss_mask_0: 0.03078  loss_dice_0: 0.5128  loss_ce_1: 3.565e-08  loss_mask_1: 0.0302  loss_dice_1: 0.5054  loss_ce_2: 0  loss_mask_2: 0.02976  loss_dice_2: 0.4899  loss_ce_3: 0  loss_mask_3: 0.03176  loss_dice_3: 0.5194  loss_ce_4: 0  loss_mask_4: 0.0292  loss_dice_4: 0.5026  loss_ce_5: 0  loss_mask_5: 0.03129  loss_dice_5: 0.5249  loss_ce_6: 0  loss_mask_6: 0.02862  loss_dice_6: 0.4906  loss_ce_7: 0  loss_mask_7: 0.02705  loss_dice_7: 0.4356  loss_ce_8: 0  loss_mask_8: 0.02678  loss_dice_8: 0.5265  time: 0.4125  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:52:52 d2.utils.events]: [0m eta: 0:03:58  iter: 9419  total_loss: 5.376  loss_ce: 2.625e-09  loss_mask: 0.03136  loss_dice: 0.4994  loss_ce_0: 2.238e-05  loss_mask_0: 0.03161  loss_dice_0: 0.5037  loss_ce_1: 3.489e-08  loss_mask_1: 0.03006  loss_dice_1: 0.4913  loss_ce_2: 0  loss_mask_2: 0.03224  loss_dice_2: 0.5188  loss_ce_3: 0  loss_mask_3: 0.02998  loss_dice_3: 0.5298  loss_ce_4: 0  loss_mask_4: 0.03105  loss_dice_4: 0.4757  loss_ce_5: 0  loss_mask_5: 0.02968  loss_dice_5: 0.5107  loss_ce_6: 0  loss_mask_6: 0.03081  loss_dice_6: 0.537  loss_ce_7: 0  loss_mask_7: 0.03127  loss_dice_7: 0.5255  loss_ce_8: 0  loss_mask_8: 0.03145  loss_dice_8: 0.5529  time: 0.4125  data_time: 0.0064  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:00 d2.utils.events]: [0m eta: 0:03:50  iter: 9439  total_loss: 5.578  loss_ce: 2.187e-09  loss_mask: 0.03085  loss_dice: 0.5093  loss_ce_0: 2.228e-05  loss_mask_0: 0.02919  loss_dice_0: 0.5206  loss_ce_1: 5.096e-08  loss_mask_1: 0.03234  loss_dice_1: 0.5424  loss_ce_2: 0  loss_mask_2: 0.03024  loss_dice_2: 0.4897  loss_ce_3: 0  loss_mask_3: 0.03307  loss_dice_3: 0.5259  loss_ce_4: 0  loss_mask_4: 0.03007  loss_dice_4: 0.5303  loss_ce_5: 0  loss_mask_5: 0.02947  loss_dice_5: 0.5098  loss_ce_6: 0  loss_mask_6: 0.02922  loss_dice_6: 0.5152  loss_ce_7: 0  loss_mask_7: 0.03038  loss_dice_7: 0.5316  loss_ce_8: 0  loss_mask_8: 0.02999  loss_dice_8: 0.4945  time: 0.4125  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:04 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:53:04 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:53:04 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:53:04 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:53:04 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:53:04 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:53:05 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.258230 (0.258230 s / iter per device, on 1 devices)
[32m[06/02 14:53:05 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.196288 s / iter per device, on 1 devices)
[32m[06/02 14:53:05 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:53:05 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:53:05 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:53:05 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:53:05 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:53:05 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:53:05 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:53:09 d2.utils.events]: [0m eta: 0:03:41  iter: 9459  total_loss: 5.569  loss_ce: 1.094e-09  loss_mask: 0.03168  loss_dice: 0.4989  loss_ce_0: 2.22e-05  loss_mask_0: 0.03187  loss_dice_0: 0.5444  loss_ce_1: 4.659e-08  loss_mask_1: 0.03151  loss_dice_1: 0.5233  loss_ce_2: 0  loss_mask_2: 0.03119  loss_dice_2: 0.5215  loss_ce_3: 0  loss_mask_3: 0.03149  loss_dice_3: 0.5301  loss_ce_4: 0  loss_mask_4: 0.02865  loss_dice_4: 0.514  loss_ce_5: 0  loss_mask_5: 0.03028  loss_dice_5: 0.5407  loss_ce_6: 0  loss_mask_6: 0.03216  loss_dice_6: 0.5231  loss_ce_7: 0  loss_mask_7: 0.03053  loss_dice_7: 0.4914  loss_ce_8: 0  loss_mask_8: 0.03151  loss_dice_8: 0.5655  time: 0.4124  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:17 d2.utils.events]: [0m eta: 0:03:33  iter: 9479  total_loss: 5.519  loss_ce: 2.187e-09  loss_mask: 0.03249  loss_dice: 0.5102  loss_ce_0: 2.206e-05  loss_mask_0: 0.03011  loss_dice_0: 0.5167  loss_ce_1: 4.637e-08  loss_mask_1: 0.0307  loss_dice_1: 0.5141  loss_ce_2: 0  loss_mask_2: 0.03208  loss_dice_2: 0.5327  loss_ce_3: 0  loss_mask_3: 0.03242  loss_dice_3: 0.52  loss_ce_4: 0  loss_mask_4: 0.02971  loss_dice_4: 0.5026  loss_ce_5: 0  loss_mask_5: 0.03033  loss_dice_5: 0.5116  loss_ce_6: 0  loss_mask_6: 0.03506  loss_dice_6: 0.5213  loss_ce_7: 0  loss_mask_7: 0.02947  loss_dice_7: 0.5194  loss_ce_8: 0  loss_mask_8: 0.02957  loss_dice_8: 0.4929  time: 0.4124  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:25 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:53:25 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:53:25 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:53:25 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:53:25 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:53:25 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:53:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273981 (0.273981 s / iter per device, on 1 devices)
[32m[06/02 14:53:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.210831 s / iter per device, on 1 devices)
[32m[06/02 14:53:26 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:53:26 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:53:26 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:53:26 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:53:26 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:53:26 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:53:26 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:53:26 d2.utils.events]: [0m eta: 0:03:25  iter: 9499  total_loss: 5.641  loss_ce: 2.187e-09  loss_mask: 0.03179  loss_dice: 0.5447  loss_ce_0: 2.196e-05  loss_mask_0: 0.03039  loss_dice_0: 0.5159  loss_ce_1: 3.554e-08  loss_mask_1: 0.0316  loss_dice_1: 0.5165  loss_ce_2: 0  loss_mask_2: 0.02792  loss_dice_2: 0.5018  loss_ce_3: 0  loss_mask_3: 0.03092  loss_dice_3: 0.509  loss_ce_4: 0  loss_mask_4: 0.03121  loss_dice_4: 0.5011  loss_ce_5: 0  loss_mask_5: 0.02966  loss_dice_5: 0.5807  loss_ce_6: 0  loss_mask_6: 0.03138  loss_dice_6: 0.53  loss_ce_7: 0  loss_mask_7: 0.02957  loss_dice_7: 0.531  loss_ce_8: 0  loss_mask_8: 0.03182  loss_dice_8: 0.5474  time: 0.4124  data_time: 0.0075  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:34 d2.utils.events]: [0m eta: 0:03:17  iter: 9519  total_loss: 5.636  loss_ce: 0  loss_mask: 0.03125  loss_dice: 0.4892  loss_ce_0: 2.18e-05  loss_mask_0: 0.02947  loss_dice_0: 0.5733  loss_ce_1: 3.117e-08  loss_mask_1: 0.03167  loss_dice_1: 0.5059  loss_ce_2: 0  loss_mask_2: 0.03127  loss_dice_2: 0.4961  loss_ce_3: 0  loss_mask_3: 0.02989  loss_dice_3: 0.5143  loss_ce_4: 0  loss_mask_4: 0.03159  loss_dice_4: 0.5449  loss_ce_5: 0  loss_mask_5: 0.0334  loss_dice_5: 0.6028  loss_ce_6: 0  loss_mask_6: 0.02925  loss_dice_6: 0.5373  loss_ce_7: 0  loss_mask_7: 0.02874  loss_dice_7: 0.5136  loss_ce_8: 0  loss_mask_8: 0.02831  loss_dice_8: 0.4938  time: 0.4124  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:42 d2.utils.events]: [0m eta: 0:03:08  iter: 9539  total_loss: 5.447  loss_ce: 0  loss_mask: 0.0317  loss_dice: 0.5305  loss_ce_0: 2.174e-05  loss_mask_0: 0.03239  loss_dice_0: 0.5028  loss_ce_1: 3.095e-08  loss_mask_1: 0.03057  loss_dice_1: 0.4589  loss_ce_2: 0  loss_mask_2: 0.03318  loss_dice_2: 0.5132  loss_ce_3: 0  loss_mask_3: 0.02879  loss_dice_3: 0.5013  loss_ce_4: 0  loss_mask_4: 0.02918  loss_dice_4: 0.5011  loss_ce_5: 0  loss_mask_5: 0.03092  loss_dice_5: 0.5553  loss_ce_6: 0  loss_mask_6: 0.03029  loss_dice_6: 0.4941  loss_ce_7: 0  loss_mask_7: 0.03313  loss_dice_7: 0.5252  loss_ce_8: 0  loss_mask_8: 0.02778  loss_dice_8: 0.4837  time: 0.4124  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:53:47 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:53:47 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:53:47 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:53:47 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:53:47 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:53:47 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:53:47 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.270734 (0.270734 s / iter per device, on 1 devices)
[32m[06/02 14:53:47 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.208715 s / iter per device, on 1 devices)
[32m[06/02 14:53:47 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:53:47 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:53:47 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:53:47 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:53:47 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:53:47 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:53:47 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:53:52 d2.utils.events]: [0m eta: 0:03:00  iter: 9559  total_loss: 5.457  loss_ce: 0  loss_mask: 0.03242  loss_dice: 0.5175  loss_ce_0: 2.17e-05  loss_mask_0: 0.03344  loss_dice_0: 0.521  loss_ce_1: 3.15e-08  loss_mask_1: 0.03178  loss_dice_1: 0.5254  loss_ce_2: 0  loss_mask_2: 0.0291  loss_dice_2: 0.5009  loss_ce_3: 0  loss_mask_3: 0.03274  loss_dice_3: 0.5243  loss_ce_4: 0  loss_mask_4: 0.03058  loss_dice_4: 0.517  loss_ce_5: 0  loss_mask_5: 0.03369  loss_dice_5: 0.5367  loss_ce_6: 0  loss_mask_6: 0.03331  loss_dice_6: 0.5175  loss_ce_7: 0  loss_mask_7: 0.03124  loss_dice_7: 0.4676  loss_ce_8: 0  loss_mask_8: 0.03267  loss_dice_8: 0.5165  time: 0.4124  data_time: 0.0061  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:00 d2.utils.events]: [0m eta: 0:02:52  iter: 9579  total_loss: 5.476  loss_ce: 0  loss_mask: 0.03047  loss_dice: 0.519  loss_ce_0: 2.16e-05  loss_mask_0: 0.02845  loss_dice_0: 0.4893  loss_ce_1: 2.734e-08  loss_mask_1: 0.03179  loss_dice_1: 0.5314  loss_ce_2: 0  loss_mask_2: 0.02887  loss_dice_2: 0.5175  loss_ce_3: 0  loss_mask_3: 0.0325  loss_dice_3: 0.5478  loss_ce_4: 0  loss_mask_4: 0.0314  loss_dice_4: 0.4869  loss_ce_5: 0  loss_mask_5: 0.0302  loss_dice_5: 0.5153  loss_ce_6: 0  loss_mask_6: 0.0299  loss_dice_6: 0.4955  loss_ce_7: 0  loss_mask_7: 0.02831  loss_dice_7: 0.4659  loss_ce_8: 0  loss_mask_8: 0.02948  loss_dice_8: 0.5201  time: 0.4124  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:08 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:54:08 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:54:08 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:54:08 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:54:08 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:54:08 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:54:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.272448 (0.272448 s / iter per device, on 1 devices)
[32m[06/02 14:54:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213770 s / iter per device, on 1 devices)
[32m[06/02 14:54:09 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:54:09 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:54:09 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:54:09 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:54:09 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:54:09 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:54:09 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:54:09 d2.utils.events]: [0m eta: 0:02:44  iter: 9599  total_loss: 5.383  loss_ce: 0  loss_mask: 0.03177  loss_dice: 0.5071  loss_ce_0: 2.141e-05  loss_mask_0: 0.03113  loss_dice_0: 0.5087  loss_ce_1: 2.329e-08  loss_mask_1: 0.02832  loss_dice_1: 0.5315  loss_ce_2: 0  loss_mask_2: 0.03053  loss_dice_2: 0.5041  loss_ce_3: 0  loss_mask_3: 0.02909  loss_dice_3: 0.5397  loss_ce_4: 0  loss_mask_4: 0.02955  loss_dice_4: 0.5336  loss_ce_5: 0  loss_mask_5: 0.02753  loss_dice_5: 0.4649  loss_ce_6: 0  loss_mask_6: 0.02893  loss_dice_6: 0.4848  loss_ce_7: 0  loss_mask_7: 0.031  loss_dice_7: 0.4955  loss_ce_8: 0  loss_mask_8: 0.02756  loss_dice_8: 0.491  time: 0.4124  data_time: 0.0066  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:17 d2.utils.events]: [0m eta: 0:02:35  iter: 9619  total_loss: 5.526  loss_ce: 0  loss_mask: 0.03034  loss_dice: 0.4914  loss_ce_0: 2.133e-05  loss_mask_0: 0.03061  loss_dice_0: 0.4676  loss_ce_1: 2.559e-08  loss_mask_1: 0.03138  loss_dice_1: 0.5002  loss_ce_2: 0  loss_mask_2: 0.03308  loss_dice_2: 0.5553  loss_ce_3: 0  loss_mask_3: 0.03166  loss_dice_3: 0.537  loss_ce_4: 0  loss_mask_4: 0.03217  loss_dice_4: 0.5263  loss_ce_5: 0  loss_mask_5: 0.03344  loss_dice_5: 0.5717  loss_ce_6: 0  loss_mask_6: 0.03366  loss_dice_6: 0.508  loss_ce_7: 0  loss_mask_7: 0.03234  loss_dice_7: 0.5385  loss_ce_8: 0  loss_mask_8: 0.03023  loss_dice_8: 0.4826  time: 0.4124  data_time: 0.0068  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:25 d2.utils.events]: [0m eta: 0:02:27  iter: 9639  total_loss: 5.578  loss_ce: 0  loss_mask: 0.03103  loss_dice: 0.5296  loss_ce_0: 2.12e-05  loss_mask_0: 0.03196  loss_dice_0: 0.5129  loss_ce_1: 3.554e-08  loss_mask_1: 0.0323  loss_dice_1: 0.5168  loss_ce_2: 0  loss_mask_2: 0.03305  loss_dice_2: 0.5524  loss_ce_3: 0  loss_mask_3: 0.03255  loss_dice_3: 0.4951  loss_ce_4: 0  loss_mask_4: 0.02993  loss_dice_4: 0.5301  loss_ce_5: 0  loss_mask_5: 0.03382  loss_dice_5: 0.5454  loss_ce_6: 0  loss_mask_6: 0.03239  loss_dice_6: 0.5129  loss_ce_7: 0  loss_mask_7: 0.0318  loss_dice_7: 0.5392  loss_ce_8: 0  loss_mask_8: 0.03119  loss_dice_8: 0.5037  time: 0.4124  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:29 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:54:29 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:54:29 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:54:29 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:54:29 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:54:29 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:54:30 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.278199 (0.278199 s / iter per device, on 1 devices)
[32m[06/02 14:54:30 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215988 s / iter per device, on 1 devices)
[32m[06/02 14:54:30 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:54:30 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:54:30 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:54:30 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:54:30 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:54:30 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:54:30 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:54:34 d2.utils.events]: [0m eta: 0:02:19  iter: 9659  total_loss: 5.427  loss_ce: 0  loss_mask: 0.03154  loss_dice: 0.5337  loss_ce_0: 2.111e-05  loss_mask_0: 0.02852  loss_dice_0: 0.515  loss_ce_1: 3.434e-08  loss_mask_1: 0.03186  loss_dice_1: 0.523  loss_ce_2: 0  loss_mask_2: 0.03247  loss_dice_2: 0.5229  loss_ce_3: 0  loss_mask_3: 0.03011  loss_dice_3: 0.4973  loss_ce_4: 0  loss_mask_4: 0.03115  loss_dice_4: 0.5048  loss_ce_5: 0  loss_mask_5: 0.02828  loss_dice_5: 0.492  loss_ce_6: 0  loss_mask_6: 0.03057  loss_dice_6: 0.5304  loss_ce_7: 0  loss_mask_7: 0.02908  loss_dice_7: 0.5011  loss_ce_8: 0  loss_mask_8: 0.03165  loss_dice_8: 0.488  time: 0.4124  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:42 d2.utils.events]: [0m eta: 0:02:11  iter: 9679  total_loss: 5.562  loss_ce: 0  loss_mask: 0.02989  loss_dice: 0.524  loss_ce_0: 2.097e-05  loss_mask_0: 0.02816  loss_dice_0: 0.5074  loss_ce_1: 2.92e-08  loss_mask_1: 0.02982  loss_dice_1: 0.532  loss_ce_2: 0  loss_mask_2: 0.03004  loss_dice_2: 0.5319  loss_ce_3: 0  loss_mask_3: 0.02923  loss_dice_3: 0.5586  loss_ce_4: 0  loss_mask_4: 0.02955  loss_dice_4: 0.4878  loss_ce_5: 0  loss_mask_5: 0.02892  loss_dice_5: 0.4887  loss_ce_6: 0  loss_mask_6: 0.03462  loss_dice_6: 0.551  loss_ce_7: 0  loss_mask_7: 0.03123  loss_dice_7: 0.508  loss_ce_8: 0  loss_mask_8: 0.03073  loss_dice_8: 0.5079  time: 0.4124  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:50 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:54:50 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:54:50 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:54:50 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:54:50 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:54:50 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:54:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.284145 (0.284145 s / iter per device, on 1 devices)
[32m[06/02 14:54:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.226369 s / iter per device, on 1 devices)
[32m[06/02 14:54:51 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:54:51 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:54:51 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:54:51 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:54:51 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:54:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:54:51 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:54:51 d2.utils.events]: [0m eta: 0:02:03  iter: 9699  total_loss: 5.608  loss_ce: 0  loss_mask: 0.02851  loss_dice: 0.5066  loss_ce_0: 2.087e-05  loss_mask_0: 0.03054  loss_dice_0: 0.5075  loss_ce_1: 2.909e-08  loss_mask_1: 0.02926  loss_dice_1: 0.4984  loss_ce_2: 0  loss_mask_2: 0.02977  loss_dice_2: 0.5349  loss_ce_3: 0  loss_mask_3: 0.03393  loss_dice_3: 0.5292  loss_ce_4: 0  loss_mask_4: 0.03497  loss_dice_4: 0.5114  loss_ce_5: 0  loss_mask_5: 0.0314  loss_dice_5: 0.5469  loss_ce_6: 0  loss_mask_6: 0.03268  loss_dice_6: 0.5263  loss_ce_7: 0  loss_mask_7: 0.03175  loss_dice_7: 0.5374  loss_ce_8: 0  loss_mask_8: 0.03212  loss_dice_8: 0.5268  time: 0.4124  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:54:59 d2.utils.events]: [0m eta: 0:01:54  iter: 9719  total_loss: 5.547  loss_ce: 3.281e-10  loss_mask: 0.03194  loss_dice: 0.5294  loss_ce_0: 2.076e-05  loss_mask_0: 0.03068  loss_dice_0: 0.537  loss_ce_1: 2.559e-08  loss_mask_1: 0.03032  loss_dice_1: 0.4986  loss_ce_2: 0  loss_mask_2: 0.02879  loss_dice_2: 0.5448  loss_ce_3: 0  loss_mask_3: 0.02977  loss_dice_3: 0.4847  loss_ce_4: 0  loss_mask_4: 0.02918  loss_dice_4: 0.5173  loss_ce_5: 0  loss_mask_5: 0.03021  loss_dice_5: 0.5225  loss_ce_6: 0  loss_mask_6: 0.03248  loss_dice_6: 0.5447  loss_ce_7: 0  loss_mask_7: 0.02846  loss_dice_7: 0.5098  loss_ce_8: 0  loss_mask_8: 0.03071  loss_dice_8: 0.5108  time: 0.4124  data_time: 0.0067  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:07 d2.utils.events]: [0m eta: 0:01:46  iter: 9739  total_loss: 5.438  loss_ce: 4.375e-10  loss_mask: 0.03213  loss_dice: 0.5476  loss_ce_0: 2.071e-05  loss_mask_0: 0.03158  loss_dice_0: 0.5285  loss_ce_1: 2.515e-08  loss_mask_1: 0.03132  loss_dice_1: 0.4983  loss_ce_2: 0  loss_mask_2: 0.02683  loss_dice_2: 0.5051  loss_ce_3: 0  loss_mask_3: 0.03095  loss_dice_3: 0.512  loss_ce_4: 0  loss_mask_4: 0.03087  loss_dice_4: 0.4984  loss_ce_5: 0  loss_mask_5: 0.0297  loss_dice_5: 0.5168  loss_ce_6: 0  loss_mask_6: 0.02999  loss_dice_6: 0.5162  loss_ce_7: 0  loss_mask_7: 0.02839  loss_dice_7: 0.4999  loss_ce_8: 0  loss_mask_8: 0.0345  loss_dice_8: 0.5537  time: 0.4123  data_time: 0.0074  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:12 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:55:12 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:55:12 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:55:12 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:55:12 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:55:12 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:55:12 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.243843 (0.243843 s / iter per device, on 1 devices)
[32m[06/02 14:55:12 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.181467 s / iter per device, on 1 devices)
[32m[06/02 14:55:12 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:55:12 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:55:12 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:55:12 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:55:12 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:55:12 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:55:12 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:55:16 d2.utils.events]: [0m eta: 0:01:38  iter: 9759  total_loss: 5.547  loss_ce: 4.375e-10  loss_mask: 0.02846  loss_dice: 0.5124  loss_ce_0: 2.063e-05  loss_mask_0: 0.02871  loss_dice_0: 0.4919  loss_ce_1: 2.625e-08  loss_mask_1: 0.02862  loss_dice_1: 0.4896  loss_ce_2: 0  loss_mask_2: 0.0313  loss_dice_2: 0.5219  loss_ce_3: 0  loss_mask_3: 0.02914  loss_dice_3: 0.4917  loss_ce_4: 0  loss_mask_4: 0.0294  loss_dice_4: 0.5213  loss_ce_5: 0  loss_mask_5: 0.0351  loss_dice_5: 0.5353  loss_ce_6: 0  loss_mask_6: 0.02963  loss_dice_6: 0.5452  loss_ce_7: 0  loss_mask_7: 0.03237  loss_dice_7: 0.524  loss_ce_8: 0  loss_mask_8: 0.03415  loss_dice_8: 0.566  time: 0.4123  data_time: 0.0056  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:24 d2.utils.events]: [0m eta: 0:01:30  iter: 9779  total_loss: 5.553  loss_ce: 3.281e-10  loss_mask: 0.03109  loss_dice: 0.5153  loss_ce_0: 2.051e-05  loss_mask_0: 0.02878  loss_dice_0: 0.4684  loss_ce_1: 2.614e-08  loss_mask_1: 0.0343  loss_dice_1: 0.5363  loss_ce_2: 0  loss_mask_2: 0.03223  loss_dice_2: 0.511  loss_ce_3: 0  loss_mask_3: 0.03051  loss_dice_3: 0.5268  loss_ce_4: 0  loss_mask_4: 0.03284  loss_dice_4: 0.5741  loss_ce_5: 0  loss_mask_5: 0.02985  loss_dice_5: 0.4925  loss_ce_6: 0  loss_mask_6: 0.03104  loss_dice_6: 0.5293  loss_ce_7: 0  loss_mask_7: 0.03123  loss_dice_7: 0.5413  loss_ce_8: 0  loss_mask_8: 0.02951  loss_dice_8: 0.4961  time: 0.4123  data_time: 0.0045  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:33 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:55:33 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:55:33 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:55:33 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:55:33 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:55:33 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:55:33 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.276208 (0.276208 s / iter per device, on 1 devices)
[32m[06/02 14:55:33 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.213295 s / iter per device, on 1 devices)
[32m[06/02 14:55:33 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:55:33 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:55:33 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:55:33 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:55:33 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:55:33 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:55:33 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:55:33 d2.utils.events]: [0m eta: 0:01:21  iter: 9799  total_loss: 5.485  loss_ce: 1.312e-09  loss_mask: 0.03208  loss_dice: 0.5648  loss_ce_0: 2.045e-05  loss_mask_0: 0.02904  loss_dice_0: 0.5116  loss_ce_1: 2.789e-08  loss_mask_1: 0.03314  loss_dice_1: 0.5249  loss_ce_2: 0  loss_mask_2: 0.03267  loss_dice_2: 0.5374  loss_ce_3: 0  loss_mask_3: 0.03164  loss_dice_3: 0.4672  loss_ce_4: 0  loss_mask_4: 0.03066  loss_dice_4: 0.5288  loss_ce_5: 0  loss_mask_5: 0.03154  loss_dice_5: 0.5463  loss_ce_6: 0  loss_mask_6: 0.03298  loss_dice_6: 0.5142  loss_ce_7: 0  loss_mask_7: 0.02757  loss_dice_7: 0.5182  loss_ce_8: 0  loss_mask_8: 0.02982  loss_dice_8: 0.5435  time: 0.4123  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:42 d2.utils.events]: [0m eta: 0:01:13  iter: 9819  total_loss: 5.674  loss_ce: 8.749e-10  loss_mask: 0.03285  loss_dice: 0.5907  loss_ce_0: 2.031e-05  loss_mask_0: 0.03312  loss_dice_0: 0.5233  loss_ce_1: 3.073e-08  loss_mask_1: 0.03373  loss_dice_1: 0.5764  loss_ce_2: 0  loss_mask_2: 0.03244  loss_dice_2: 0.539  loss_ce_3: 0  loss_mask_3: 0.0303  loss_dice_3: 0.53  loss_ce_4: 0  loss_mask_4: 0.02874  loss_dice_4: 0.5012  loss_ce_5: 0  loss_mask_5: 0.03222  loss_dice_5: 0.5227  loss_ce_6: 0  loss_mask_6: 0.03074  loss_dice_6: 0.5074  loss_ce_7: 0  loss_mask_7: 0.03364  loss_dice_7: 0.5459  loss_ce_8: 0  loss_mask_8: 0.02796  loss_dice_8: 0.4973  time: 0.4123  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:50 d2.utils.events]: [0m eta: 0:01:05  iter: 9839  total_loss: 5.533  loss_ce: 1.094e-09  loss_mask: 0.02877  loss_dice: 0.4914  loss_ce_0: 2.017e-05  loss_mask_0: 0.02984  loss_dice_0: 0.5047  loss_ce_1: 2.537e-08  loss_mask_1: 0.03278  loss_dice_1: 0.5659  loss_ce_2: 0  loss_mask_2: 0.02873  loss_dice_2: 0.4955  loss_ce_3: 0  loss_mask_3: 0.02692  loss_dice_3: 0.4917  loss_ce_4: 0  loss_mask_4: 0.03237  loss_dice_4: 0.5578  loss_ce_5: 0  loss_mask_5: 0.03157  loss_dice_5: 0.5415  loss_ce_6: 0  loss_mask_6: 0.03323  loss_dice_6: 0.5434  loss_ce_7: 0  loss_mask_7: 0.03052  loss_dice_7: 0.5495  loss_ce_8: 0  loss_mask_8: 0.02897  loss_dice_8: 0.4716  time: 0.4123  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:55:54 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:55:54 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:55:54 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:55:54 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:55:54 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:55:54 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:55:55 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.274732 (0.274732 s / iter per device, on 1 devices)
[32m[06/02 14:55:55 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.214996 s / iter per device, on 1 devices)
[32m[06/02 14:55:55 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:55:55 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:55:55 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:55:55 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:55:55 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:55:55 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:55:55 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:55:59 d2.utils.events]: [0m eta: 0:00:57  iter: 9859  total_loss: 5.478  loss_ce: 2.078e-09  loss_mask: 0.02922  loss_dice: 0.4997  loss_ce_0: 2.01e-05  loss_mask_0: 0.02812  loss_dice_0: 0.4584  loss_ce_1: 2.165e-08  loss_mask_1: 0.0275  loss_dice_1: 0.4735  loss_ce_2: 0  loss_mask_2: 0.03079  loss_dice_2: 0.5348  loss_ce_3: 0  loss_mask_3: 0.03148  loss_dice_3: 0.5456  loss_ce_4: 0  loss_mask_4: 0.02947  loss_dice_4: 0.4997  loss_ce_5: 0  loss_mask_5: 0.03106  loss_dice_5: 0.4864  loss_ce_6: 0  loss_mask_6: 0.03185  loss_dice_6: 0.5284  loss_ce_7: 0  loss_mask_7: 0.03114  loss_dice_7: 0.5373  loss_ce_8: 0  loss_mask_8: 0.02899  loss_dice_8: 0.4883  time: 0.4123  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:07 d2.utils.events]: [0m eta: 0:00:49  iter: 9879  total_loss: 5.423  loss_ce: 3.937e-09  loss_mask: 0.02809  loss_dice: 0.482  loss_ce_0: 1.997e-05  loss_mask_0: 0.02971  loss_dice_0: 0.5006  loss_ce_1: 2.264e-08  loss_mask_1: 0.03092  loss_dice_1: 0.5013  loss_ce_2: 0  loss_mask_2: 0.03317  loss_dice_2: 0.537  loss_ce_3: 0  loss_mask_3: 0.03076  loss_dice_3: 0.5074  loss_ce_4: 0  loss_mask_4: 0.03109  loss_dice_4: 0.514  loss_ce_5: 0  loss_mask_5: 0.02907  loss_dice_5: 0.5154  loss_ce_6: 0  loss_mask_6: 0.02897  loss_dice_6: 0.4868  loss_ce_7: 0  loss_mask_7: 0.03039  loss_dice_7: 0.5048  loss_ce_8: 0  loss_mask_8: 0.03206  loss_dice_8: 0.5631  time: 0.4123  data_time: 0.0073  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:16 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:56:16 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:56:16 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:56:16 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:56:16 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:56:16 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:56:16 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.256157 (0.256157 s / iter per device, on 1 devices)
[32m[06/02 14:56:16 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.193584 s / iter per device, on 1 devices)
[32m[06/02 14:56:16 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:56:16 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:56:16 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:56:16 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:56:16 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:56:16 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:56:16 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:56:16 d2.utils.events]: [0m eta: 0:00:41  iter: 9899  total_loss: 5.362  loss_ce: 7.218e-09  loss_mask: 0.03004  loss_dice: 0.4959  loss_ce_0: 1.987e-05  loss_mask_0: 0.02895  loss_dice_0: 0.4732  loss_ce_1: 2.319e-08  loss_mask_1: 0.03104  loss_dice_1: 0.5262  loss_ce_2: 0  loss_mask_2: 0.03072  loss_dice_2: 0.5104  loss_ce_3: 0  loss_mask_3: 0.0298  loss_dice_3: 0.521  loss_ce_4: 0  loss_mask_4: 0.02921  loss_dice_4: 0.4931  loss_ce_5: 0  loss_mask_5: 0.03265  loss_dice_5: 0.5193  loss_ce_6: 0  loss_mask_6: 0.03306  loss_dice_6: 0.4929  loss_ce_7: 0  loss_mask_7: 0.03069  loss_dice_7: 0.519  loss_ce_8: 0  loss_mask_8: 0.02998  loss_dice_8: 0.5005  time: 0.4123  data_time: 0.0072  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:25 d2.utils.events]: [0m eta: 0:00:32  iter: 9919  total_loss: 5.52  loss_ce: 1.422e-09  loss_mask: 0.03231  loss_dice: 0.4986  loss_ce_0: 1.975e-05  loss_mask_0: 0.03213  loss_dice_0: 0.5273  loss_ce_1: 2.854e-08  loss_mask_1: 0.03028  loss_dice_1: 0.5279  loss_ce_2: 0  loss_mask_2: 0.03049  loss_dice_2: 0.4869  loss_ce_3: 0  loss_mask_3: 0.03242  loss_dice_3: 0.551  loss_ce_4: 0  loss_mask_4: 0.0307  loss_dice_4: 0.5494  loss_ce_5: 0  loss_mask_5: 0.02839  loss_dice_5: 0.5097  loss_ce_6: 0  loss_mask_6: 0.03222  loss_dice_6: 0.5185  loss_ce_7: 0  loss_mask_7: 0.03093  loss_dice_7: 0.508  loss_ce_8: 0  loss_mask_8: 0.03054  loss_dice_8: 0.5124  time: 0.4123  data_time: 0.0069  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:33 d2.utils.events]: [0m eta: 0:00:24  iter: 9939  total_loss: 5.635  loss_ce: 2.515e-09  loss_mask: 0.03107  loss_dice: 0.522  loss_ce_0: 1.963e-05  loss_mask_0: 0.03161  loss_dice_0: 0.5257  loss_ce_1: 2.209e-08  loss_mask_1: 0.03206  loss_dice_1: 0.5299  loss_ce_2: 0  loss_mask_2: 0.02823  loss_dice_2: 0.4964  loss_ce_3: 0  loss_mask_3: 0.02815  loss_dice_3: 0.5233  loss_ce_4: 0  loss_mask_4: 0.03069  loss_dice_4: 0.5329  loss_ce_5: 0  loss_mask_5: 0.02896  loss_dice_5: 0.4645  loss_ce_6: 0  loss_mask_6: 0.03125  loss_dice_6: 0.5294  loss_ce_7: 0  loss_mask_7: 0.03248  loss_dice_7: 0.5131  loss_ce_8: 0  loss_mask_8: 0.03252  loss_dice_8: 0.5379  time: 0.4123  data_time: 0.0070  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:37 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:56:37 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:56:37 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:56:37 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:56:37 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:56:37 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:56:38 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.273672 (0.273672 s / iter per device, on 1 devices)
[32m[06/02 14:56:38 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.215532 s / iter per device, on 1 devices)
[32m[06/02 14:56:38 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:56:38 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:56:38 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:56:38 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:56:38 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:56:38 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:56:38 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
[32m[06/02 14:56:42 d2.utils.events]: [0m eta: 0:00:16  iter: 9959  total_loss: 5.365  loss_ce: 4.375e-09  loss_mask: 0.02588  loss_dice: 0.4521  loss_ce_0: 1.956e-05  loss_mask_0: 0.02849  loss_dice_0: 0.4718  loss_ce_1: 2.373e-08  loss_mask_1: 0.03238  loss_dice_1: 0.5227  loss_ce_2: 0  loss_mask_2: 0.02991  loss_dice_2: 0.4773  loss_ce_3: 0  loss_mask_3: 0.02634  loss_dice_3: 0.4717  loss_ce_4: 0  loss_mask_4: 0.02721  loss_dice_4: 0.4754  loss_ce_5: 0  loss_mask_5: 0.03149  loss_dice_5: 0.5354  loss_ce_6: 0  loss_mask_6: 0.03003  loss_dice_6: 0.5013  loss_ce_7: 0  loss_mask_7: 0.03073  loss_dice_7: 0.5128  loss_ce_8: 0  loss_mask_8: 0.02859  loss_dice_8: 0.5071  time: 0.4123  data_time: 0.0071  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:50 d2.utils.events]: [0m eta: 0:00:08  iter: 9979  total_loss: 5.596  loss_ce: 2.406e-09  loss_mask: 0.02875  loss_dice: 0.4843  loss_ce_0: 1.948e-05  loss_mask_0: 0.03049  loss_dice_0: 0.5525  loss_ce_1: 2.023e-08  loss_mask_1: 0.03276  loss_dice_1: 0.5115  loss_ce_2: 0  loss_mask_2: 0.03243  loss_dice_2: 0.527  loss_ce_3: 0  loss_mask_3: 0.03054  loss_dice_3: 0.5369  loss_ce_4: 0  loss_mask_4: 0.03105  loss_dice_4: 0.5183  loss_ce_5: 0  loss_mask_5: 0.02977  loss_dice_5: 0.5026  loss_ce_6: 0  loss_mask_6: 0.03401  loss_dice_6: 0.5387  loss_ce_7: 0  loss_mask_7: 0.03256  loss_dice_7: 0.5623  loss_ce_8: 0  loss_mask_8: 0.03154  loss_dice_8: 0.5321  time: 0.4123  data_time: 0.0076  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:56:58 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_0009999.pth
[32m[06/02 14:57:02 fvcore.common.checkpoint]: [0mSaving checkpoint to output_3_train_1_fish_low_class_loss/model_final.pth
[32m[06/02 14:57:04 d2.utils.events]: [0m eta: 0:00:00  iter: 9999  total_loss: 5.436  loss_ce: 1.859e-09  loss_mask: 0.02938  loss_dice: 0.5094  loss_ce_0: 1.936e-05  loss_mask_0: 0.02914  loss_dice_0: 0.5144  loss_ce_1: 1.936e-08  loss_mask_1: 0.02805  loss_dice_1: 0.4862  loss_ce_2: 0  loss_mask_2: 0.02952  loss_dice_2: 0.524  loss_ce_3: 0  loss_mask_3: 0.03327  loss_dice_3: 0.5634  loss_ce_4: 0  loss_mask_4: 0.02935  loss_dice_4: 0.5169  loss_ce_5: 0  loss_mask_5: 0.02955  loss_dice_5: 0.519  loss_ce_6: 0  loss_mask_6: 0.02744  loss_dice_6: 0.4945  loss_ce_7: 0  loss_mask_7: 0.02979  loss_dice_7: 0.5039  loss_ce_8: 0  loss_mask_8: 0.02882  loss_dice_8: 0.5044  time: 0.4123  data_time: 0.0079  lr: 5e-05  max_mem: 5953M
[32m[06/02 14:57:04 d2.engine.hooks]: [0mOverall training speed: 9998 iterations in 1:08:42 (0.4123 s / it)
[32m[06/02 14:57:04 d2.engine.hooks]: [0mTotal training time: 1:12:09 (0:03:26 on hooks)
[32m[06/02 14:57:04 mask2former_video.data_video.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]
[32m[06/02 14:57:04 mask2former_video.data_video.datasets.ytvis]: [0mLoaded 2 videos in YTVIS format from datasets/danio2d_small/val.json
[32m[06/02 14:57:04 d2.data.common]: [0mSerializing 2 elements to byte tensors and concatenating them all ...
[32m[06/02 14:57:04 d2.data.common]: [0mSerialized dataset takes 0.12 MiB
[5m[31mWARNING[0m [32m[06/02 14:57:04 mask2former_video.data_video.ytvis_eval]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/02 14:57:04 d2.evaluation.evaluator]: [0mStart inference on 2 batches
[32m[06/02 14:57:05 d2.evaluation.evaluator]: [0mTotal inference time: 0:00:00.286091 (0.286091 s / iter per device, on 1 devices)
[32m[06/02 14:57:05 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:00 (0.222466 s / iter per device, on 1 devices)
[32m[06/02 14:57:05 mask2former_video.data_video.ytvis_eval]: [0mPreparing results for YTVIS format ...
[32m[06/02 14:57:05 mask2former_video.data_video.ytvis_eval]: [0mSaving results to output_3_train_1_fish_low_class_loss/inference/results.json
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
[32m[06/02 14:57:05 mask2former_video.data_video.ytvis_eval]: [0mEvaluation results for segm: 
|   AP    |  AP50   |  AP75   |   APs   |  APm  |  APl  |   AR1   |  AR10   |
|:-------:|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|:-------:|
| 100.000 | 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 | 100.000 |
[32m[06/02 14:57:05 mask2former_video.data_video.ytvis_eval]: [0mSome metrics cannot be computed and is shown as NaN.
[32m[06/02 14:57:05 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[06/02 14:57:05 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl,AR1,AR10
[32m[06/02 14:57:05 d2.evaluation.testing]: [0mcopypaste: 100.0000,100.0000,100.0000,100.0000,nan,nan,100.0000,100.0000
Done
